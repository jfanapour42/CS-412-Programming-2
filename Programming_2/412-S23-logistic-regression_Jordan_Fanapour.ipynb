{"cells":[{"cell_type":"markdown","metadata":{"id":"va6zv-gKhIu8"},"source":["Name: Write Your Name Here"]},{"cell_type":"code","source":["#Jordan Fanapour"],"metadata":{"id":"tuaYthb0hXnU","executionInfo":{"status":"ok","timestamp":1678162916788,"user_tz":360,"elapsed":321,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"execution_count":204,"outputs":[]},{"cell_type":"markdown","source":["Submission:\n","\n","1- Run all cells (this is important, the results will remain there for us to look)\n","\n","2- Download .ipynb\n","\n","3- Download .py\n","\n","4- Use Save as or Print to create a PDF version of the notebook  \n","\n","5- Create a directory named: firstname_lastname_lr (e.g. pedram_rooshenas_lr)\n","\n","6- Put all three .ipynb, .py, and .pdf into the directory. (*** Don't forget the PDF and .py ***) \n","\n","7- Zip (don't use rar) and Submit on Gradescope"],"metadata":{"id":"QZj93Jcmh8pV"}},{"cell_type":"code","source":["#Mounting Google Drive:\n","#After running this cell a popup window will appear and requesting to select your  Google account and give the access permission.\n","#You can either use your personal Google account or your UIC Google account.\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"jkGXbH9kiQEK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162917864,"user_tz":360,"elapsed":564,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"9623265d-092c-4781-cd96-753daa669179"},"execution_count":205,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["path=\"/content/gdrive/MyDrive/CS_412/Programming_2/\""],"metadata":{"id":"K5ljlocpiRxq","executionInfo":{"status":"ok","timestamp":1678162917865,"user_tz":360,"elapsed":16,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"execution_count":206,"outputs":[]},{"cell_type":"code","execution_count":207,"metadata":{"id":"qB86CgDBhIu9","executionInfo":{"status":"ok","timestamp":1678162917866,"user_tz":360,"elapsed":16,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import style\n","style.use('ggplot')\n","%matplotlib inline\n","import re\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"QhFQTtLWhIu-"},"source":["Numpy is library for scientific computing in Python. It has efficient implementation of n-dimensional array (tensor) manupulations, which is useful for machine learning applications."]},{"cell_type":"code","execution_count":208,"metadata":{"id":"RZceSJk8hIu-","executionInfo":{"status":"ok","timestamp":1678162917867,"user_tz":360,"elapsed":16,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"MLJ1LE0RhIu_"},"source":["We can convert a list into numpy array (tensor)  "]},{"cell_type":"code","execution_count":209,"metadata":{"id":"UIBuEViJhIu_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918051,"user_tz":360,"elapsed":200,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"2d9650b8-e718-4761-86af-f0ff195e422a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 2, 4],\n","       [2, 6, 9]])"]},"metadata":{},"execution_count":209}],"source":["b = [[1, 2, 4], [2, 6, 9]]\n","a = np.array(b)\n","a"]},{"cell_type":"markdown","metadata":{"id":"YpaBPUPEhIu_"},"source":["We can check the dimensions of the array"]},{"cell_type":"code","execution_count":210,"metadata":{"id":"fUrF-jvPhIu_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918053,"user_tz":360,"elapsed":69,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"06e06094-5c70-41d9-cf29-5f82ed2732f7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 3)"]},"metadata":{},"execution_count":210}],"source":["a.shape"]},{"cell_type":"markdown","metadata":{"id":"UjUJI4vRhIu_"},"source":["We can apply simple arithmetic operation on all element of a tensor"]},{"cell_type":"code","execution_count":211,"metadata":{"id":"VlQlxcGzhIu_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918055,"user_tz":360,"elapsed":66,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"f0df621e-f608-46ee-e331-10ccac71a320"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 3,  6, 12],\n","       [ 6, 18, 27]])"]},"metadata":{},"execution_count":211}],"source":["a * 3"]},{"cell_type":"markdown","metadata":{"id":"MHyhHZbghIvA"},"source":["You can transpose a tensor\n","    "]},{"cell_type":"code","execution_count":212,"metadata":{"id":"88ItMKO8hIvA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918056,"user_tz":360,"elapsed":61,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"487400be-3d42-4720-c7bd-900762b7575d"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3, 2)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[1, 2],\n","       [2, 6],\n","       [4, 9]])"]},"metadata":{},"execution_count":212}],"source":["print(a.T.shape)\n","a.T"]},{"cell_type":"markdown","metadata":{"id":"x5TsmIfZhIvA"},"source":["You can apply aggregate functions on the whole tensor"]},{"cell_type":"code","execution_count":213,"metadata":{"id":"0rHaLkUMhIvA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918057,"user_tz":360,"elapsed":56,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"71f05f89-f967-4589-d568-7de689a527cb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["24"]},"metadata":{},"execution_count":213}],"source":["np.sum(a)"]},{"cell_type":"markdown","metadata":{"id":"xYoIqupihIvA"},"source":["or on one dimension of it"]},{"cell_type":"code","execution_count":214,"metadata":{"id":"2t74tXHOhIvA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918057,"user_tz":360,"elapsed":50,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"d6551f03-fe7d-4119-808e-167741a1341d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 3,  8, 13])"]},"metadata":{},"execution_count":214}],"source":["np.sum(a, axis=0)"]},{"cell_type":"code","execution_count":215,"metadata":{"id":"9pLDwLh8hIvA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918058,"user_tz":360,"elapsed":45,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"78243cda-ce58-40ec-ff59-00e3d851e84d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 7, 17])"]},"metadata":{},"execution_count":215}],"source":["np.sum(a, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"1gQUIziphIvA"},"source":["We can do element-wise arithmetic operation on two tensors (of the same size)"]},{"cell_type":"code","execution_count":216,"metadata":{"id":"hLcwq4nZhIvA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918059,"user_tz":360,"elapsed":42,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"1df42089-4f7e-43a7-e95c-8f70da80377b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2,  6, 20],\n","       [ 2, 12,  9]])"]},"metadata":{},"execution_count":216}],"source":["c1 = np.array([[1, 2, 4], [2, 6, 9]])\n","c2 = np.array([[2, 3, 5], [1, 2, 1]])\n","c1 * c2"]},{"cell_type":"markdown","metadata":{"id":"nTbsvWJxhIvA"},"source":["If you want to multiply all columns of a tensor by vector (for example if you want to multiply all data features by their lables) you need a trick. This multiplication shows up in calculating the gradients. "]},{"cell_type":"code","execution_count":217,"metadata":{"id":"VDEbp8nzhIvB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162918060,"user_tz":360,"elapsed":35,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"fed0812b-e805-4933-ffc1-28206b356477"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 2 4]\n"," [2 6 9]]\n","[ 1 -1]\n"]}],"source":["a = np.array([[1, 2, 4], [2, 6, 9]])\n","b = np.array([1,-1])\n","print(a)\n","print(b)\n"]},{"cell_type":"markdown","metadata":{"id":"YHqynmerhIvB"},"source":["Here we want to multiply the first row of a by 1 and the second row of a by -1. Simply multiplying a by b does not work because a and b do not have the same dimension"]},{"cell_type":"code","execution_count":218,"metadata":{"id":"uMALphCUhIvB","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"error","timestamp":1678162918060,"user_tz":360,"elapsed":29,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"2b2d39dd-5354-4f39-e00c-0b5afaee4d87"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-218-9bc1a869709f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (2,) "]}],"source":["a * b"]},{"cell_type":"markdown","metadata":{"id":"_qkH4wCRhIvB"},"source":["To do this multiplication we first have to assume b has one column and then repeat the column of b with the number of columns in a. We use tile function to do that"]},{"cell_type":"code","execution_count":219,"metadata":{"id":"BK6SI4OqhIvB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162924495,"user_tz":360,"elapsed":311,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"24660713-c43b-4e6c-a9f9-78d27852c171"},"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 3)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  1,  1],\n","       [-1, -1, -1]])"]},"metadata":{},"execution_count":219}],"source":["b_repeat = np.tile(b,  (a.shape[1],1)).T\n","print(b_repeat.shape)\n","b_repeat"]},{"cell_type":"markdown","metadata":{"id":"-37pUmo_hIvB"},"source":["Now we can multiply each column of a by b:"]},{"cell_type":"code","execution_count":220,"metadata":{"id":"Dj57Pxv1hIvB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162924631,"user_tz":360,"elapsed":21,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"94ad0b2b-298b-4ffc-a736-a955fd611066"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  2,  4],\n","       [-2, -6, -9]])"]},"metadata":{},"execution_count":220}],"source":["a * b_repeat"]},{"cell_type":"markdown","metadata":{"id":"njjOZYvHhIvB"},"source":["You can create inital random vector using numpy (using N(0,1)):"]},{"cell_type":"code","execution_count":221,"metadata":{"id":"x8O7pRx5hIvB","executionInfo":{"status":"ok","timestamp":1678162924632,"user_tz":360,"elapsed":19,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["mu = 0 #mean\n","sigma = 1 #standard deviation\n","r = np.random.normal(mu,sigma, 1000) #draws 1000 samples from a normal distribution"]},{"cell_type":"markdown","metadata":{"id":"f6kyKWMnhIvB"},"source":["We can apply functions on tensors"]},{"cell_type":"code","execution_count":222,"metadata":{"id":"13oMFM34hIvC","executionInfo":{"status":"ok","timestamp":1678162924633,"user_tz":360,"elapsed":20,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["#implementation of Normal distribution\n","def normal(x, mu, sigma):\n","    return np.exp( -0.5 * ((x-mu)/sigma)**2)/np.sqrt(2.0*np.pi*sigma**2)\n","\n","#probability of samples on the Normal distribution\n","probabilities = normal(r, mu, sigma)"]},{"cell_type":"markdown","metadata":{"id":"Hby6OxM2hIvC"},"source":["Numpy has useful APIs for analysis. Here we plot the histogram of samples and also plot the probabilies to see if the samples follow the normal distribution."]},{"cell_type":"code","execution_count":223,"metadata":{"id":"Ort_wanOhIvC","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1678162924985,"user_tz":360,"elapsed":371,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"a1f38520-e4ed-4782-8727-1a0a187e78a5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7f929ed1f070>"]},"metadata":{},"execution_count":223},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["counts, bins = np.histogram(r,50,density=True)\n","plt.hist(bins[:-1], bins, weights=counts)\n","plt.scatter(r, probabilities, c='b', marker='.')"]},{"cell_type":"code","execution_count":224,"metadata":{"id":"1ixKIArxhIvC","executionInfo":{"status":"ok","timestamp":1678162924986,"user_tz":360,"elapsed":6,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["def read_data(filename):\n","    f = open(filename, 'r')\n","    p = re.compile(',')\n","    xdata = []\n","    ydata = []\n","    header = f.readline().strip()\n","    varnames = p.split(header)\n","    namehash = {}\n","    for l in f:\n","        li = p.split(l.strip())\n","        xdata.append([float(x) for x in li[:-1]])\n","        ydata.append(float(li[-1]))\n","    \n","    return np.array(xdata), np.array(ydata)\n"]},{"cell_type":"markdown","metadata":{"id":"ES-3FSlmhIvC"},"source":["Assuming our data is x is available in numpy we use numpy to implement logistic regression\n"]},{"cell_type":"code","execution_count":225,"metadata":{"id":"SN454nr0hIvC","executionInfo":{"status":"ok","timestamp":1678162925170,"user_tz":360,"elapsed":189,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["(xtrain_whole, ytrain_whole) = read_data(path + 'spambase-train.csv')\n","(xtest, ytest) = read_data(path + 'spambase-test.csv')"]},{"cell_type":"code","execution_count":226,"metadata":{"id":"ciUpv-cGhIvC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162925171,"user_tz":360,"elapsed":23,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"cf71e9e0-2e9c-4fc6-d676-c2d3bf0f10b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["The shape of xtrain: (3601, 54)\n","The shape of ytrain: (3601,)\n","The shape of xtest: (1000, 54)\n","The shape of ytest: (1000,)\n"]}],"source":["print(\"The shape of xtrain:\", xtrain_whole.shape)\n","print(\"The shape of ytrain:\", ytrain_whole.shape)\n","print(\"The shape of xtest:\", xtest.shape)\n","print(\"The shape of ytest:\", ytest.shape)"]},{"cell_type":"markdown","metadata":{"id":"QW3mxkkahIvC"},"source":["before training make we normalize the input data (features)"]},{"cell_type":"code","execution_count":227,"metadata":{"id":"whv7x-gthIvD","executionInfo":{"status":"ok","timestamp":1678162925173,"user_tz":360,"elapsed":23,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["xmean = np.mean(xtrain_whole, axis=0)\n","xstd = np.std(xtrain_whole, axis=0)\n","xtrain_normal_whole = (xtrain_whole-xmean) / xstd\n","xtest_normal = (xtest-xmean) / xstd"]},{"cell_type":"markdown","metadata":{"id":"wu6SLWKFhIvD"},"source":["We need to create a validation set. We create an array of indecies and permute it."]},{"cell_type":"code","execution_count":228,"metadata":{"id":"QcTRxtqohIvD","executionInfo":{"status":"ok","timestamp":1678162925173,"user_tz":360,"elapsed":23,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["premute_indicies = np.random.permutation(np.arange(xtrain_whole.shape[0]))"]},{"cell_type":"markdown","metadata":{"id":"rols6OsxhIvD"},"source":["We keep the first 2600 data points as the training data and rest as the validation data "]},{"cell_type":"code","execution_count":229,"metadata":{"id":"DP87ZxqhhIvD","executionInfo":{"status":"ok","timestamp":1678162925174,"user_tz":360,"elapsed":23,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["xtrain_normal = xtrain_normal_whole[premute_indicies[:2600]]\n","ytrain = ytrain_whole[premute_indicies[:2600]]\n","xval_normal = xtrain_normal_whole[premute_indicies[2600:]]\n","yval = ytrain_whole[premute_indicies[2600:]]"]},{"cell_type":"markdown","metadata":{"id":"OcY4MTtKhIvD"},"source":["Initiallizing the weights and bias with random values from N(0,1)"]},{"cell_type":"code","execution_count":230,"metadata":{"id":"GSZd1VgGhIvD","executionInfo":{"status":"ok","timestamp":1678162925175,"user_tz":360,"elapsed":24,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["weights = np.random.normal(0, 1, xtrain_normal.shape[1]);\n","bias = np.random.normal(0,1,1)"]},{"cell_type":"code","execution_count":231,"metadata":{"id":"2wT420TChIvD","executionInfo":{"status":"ok","timestamp":1678162925176,"user_tz":360,"elapsed":25,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["#the sigmoid function\n","def sigmoid(v):\n","    #return np.exp(-np.logaddexp(0, -v)) #numerically stable implementation of sigmoid function \n","    return 1.0 / (1+np.exp(-v))"]},{"cell_type":"markdown","metadata":{"id":"oe6TnxiBhIvD"},"source":["We can use dot-product from numpy to calculate the margin and pass it to the sigmoid function"]},{"cell_type":"code","execution_count":232,"metadata":{"id":"YzLLEHfihIvD","executionInfo":{"status":"ok","timestamp":1678162925176,"user_tz":360,"elapsed":24,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["#w: weight vector (numpy array of size n)\n","#b: numpy array of size 1\n","#returns p(y=1|x, w, b)\n","def prob(x, w, b):\n","    return sigmoid(np.dot(x,w) + b);"]},{"cell_type":"markdown","metadata":{"id":"6HJuk25jhIvD"},"source":["You can also calculate $l_2$ penalty using linalg library of numpy "]},{"cell_type":"code","execution_count":233,"metadata":{"id":"TC70IRw9hIvD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162925177,"user_tz":360,"elapsed":25,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"af786a8d-6e96-4bcb-b4c2-e61ee3fa9bd7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.429579320921634"]},"metadata":{},"execution_count":233}],"source":["np.linalg.norm(weights)"]},{"cell_type":"markdown","metadata":{"id":"bVsllOx5hIvD"},"source":["$$\\text{Cross Entropy Loss} = -\\frac{1}{|D|}[\\sum_{(y^i,\\mathbf{x}^i)\\in\\mathcal{D}} \n"," y^i \\log p(y=1|\\mathbf{x}^i;\\mathbf{w},b)  +  (1-y^i) \\log (1 - p(y=1|\\mathbf{x}^i;\\mathbf{w},b))]+\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 $$"]},{"cell_type":"code","execution_count":234,"metadata":{"id":"iI64I76vhIvE","executionInfo":{"status":"ok","timestamp":1678162925178,"user_tz":360,"elapsed":19,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["#w: weight vector (numpy array of size n)\n","#x: training data points (only attributes)\n","#y_prob: p(y|x, w, b)\n","#y_true: class variable data\n","#lambda_: l2 penalty coefficient\n","#returns the cross entropy loss\n","def loss(w, x, y_prob, y_true, lambda_):\n","    regularizer = (lambda_*((np.linalg.norm(w))**2))/2    # Calculate regularizer (λ2∥w∥2)\n","    theta = y_true*np.log(y_prob)                       # Calculate yi*log(p(y=1|xi;w,b))\n","    oneMinusTheta = (1-y_true)*np.log(1-y_prob)         # Calculate (1−yi)*log(1−p(y=1|xi;w,b))\n","    sum = np.sum(theta + oneMinusTheta)                 # Calculate sum\n","    return ((-1*sum)/y_prob.shape[0]) + regularizer     # return\n","    "]},{"cell_type":"code","execution_count":235,"metadata":{"id":"hN3S3kGnhIvE","executionInfo":{"status":"ok","timestamp":1678162925179,"user_tz":360,"elapsed":19,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["#x: input variables (data of size m x n with m data point and n features)\n","#w: weight vector (numpy array of size n)\n","#y_prob: p(y|x, w, b)\n","#y_true: class variable data\n","#lambda_: l2 penalty coefficient\n","#returns tuple of gradient w.r.t w and w.r.t to bias\n","\n","def grad_w_b(x, w, y_prob, y_true, lambda_):\n","    expYPred = (1/y_prob) - 1                                                  # Equivalent to exp(-(w*x+b))\n","    deriOfTheta = y_true*y_prob*expYPred                                       # derivative of yi*log(p(y=1|xi;w,b))\n","    deriOfOneMinusTheta = -1*(1-y_true)*y_prob                                 # derivative of (1−yi)*log(1−p(y=1|xi;w,b))\n","    theta_repeat = np.tile(deriOfTheta,  (x.shape[1],1)).T\n","    oneMinusTheta_repeat = np.tile(deriOfOneMinusTheta, (x.shape[1],1)).T\n","    grad_w = (-1*np.sum(x*(theta_repeat + oneMinusTheta_repeat), axis=0))/y_prob.shape[0] + lambda_*w\n","    grad_b = (-1*np.sum(deriOfTheta + deriOfOneMinusTheta))/y_prob.shape[0]\n","\n","    return (grad_w,grad_b)"]},{"cell_type":"code","execution_count":236,"metadata":{"scrolled":false,"id":"4tGaJCDjhIvE","executionInfo":{"status":"ok","timestamp":1678162925727,"user_tz":360,"elapsed":567,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["\n","#lambda_ is the coeffienct of l2 norm penalty\n","#learning_rate is learning rate of gradient descent algorithm\n","#max_iter determines the maximum number of iterations if the gradients descent does not converge.\n","#continue the training while gradient > 0.1 or the number steps is less max_iter\n","\n","#returns model as tuple of (weights,bias)\n","\n","def fit(x, y_true, learning_rate, lambda_, max_iter, verbose=0):\n","    weights = np.random.normal(0, 1, x.shape[1]);\n","    bias = np.random.normal(0,1,1)\n","    #change the condition appropriately\n","    iter = 1\n","    grad_norm = np.inf\n","    while iter < max_iter and grad_norm > 0.01:\n","        y_prob = prob(x, weights, bias)\n","        (grad_weight, grad_b) = grad_w_b(x, weights, y_prob, y_true, lambda_)\n","        if verbose: #verbose is used for debugging purposes\n","            #print iteration number, loss, l2 norm of gradients, l2 norm of weights\n","            print(\"Iteration #\" + str(iter) + \"  Loss: \" + str(loss(weights, x, y_prob, y_true, lambda_)) + \"; l2 norm of gradients: \" + str(np.linalg.norm(grad_weight)) + \"; l2 norm of weights: \" + str(np.linalg.norm(weights)))\n","        # Update weights and bias\n","        weights = weights - learning_rate*grad_weight\n","        bias = bias - learning_rate*grad_b\n","        iter += 1\n","        grad_norm = np.linalg.norm(grad_weight)\n","    return (weights, bias)"]},{"cell_type":"code","execution_count":237,"metadata":{"id":"55TdlFeyhIvE","executionInfo":{"status":"ok","timestamp":1678162925729,"user_tz":360,"elapsed":14,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}}},"outputs":[],"source":["def accuracy(x, y_true, model):\n","    w, b = model\n","    return np.sum((prob(x, w, b)>0.5).astype(np.float) == y_true)  / y_true.shape[0]"]},{"cell_type":"code","execution_count":238,"metadata":{"id":"DZlAxkr2hIvE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162941723,"user_tz":360,"elapsed":16007,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"0ff7b6ca-372e-45a0-eb03-ce0d407053e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Iteration #1119  Loss: 2.6532871883457556; l2 norm of gradients: 2.1538267545784833; l2 norm of weights: 2.093214172624914\n","Iteration #1120  Loss: 2.64864917390042; l2 norm of gradients: 2.1515465243329492; l2 norm of weights: 2.0910952506191496\n","Iteration #1121  Loss: 2.6440209663233105; l2 norm of gradients: 2.149268539440445; l2 norm of weights: 2.0889786419831795\n","Iteration #1122  Loss: 2.639402545566997; l2 norm of gradients: 2.146992797896266; l2 norm of weights: 2.0868643445178563\n","Iteration #1123  Loss: 2.6347938916233526; l2 norm of gradients: 2.144719297697974; l2 norm of weights: 2.084752356026085\n","Iteration #1124  Loss: 2.6301949845234738; l2 norm of gradients: 2.142448036845394; l2 norm of weights: 2.0826426743128192\n","Iteration #1125  Loss: 2.625605804337613; l2 norm of gradients: 2.140179013340609; l2 norm of weights: 2.0805352971850626\n","Iteration #1126  Loss: 2.6210263311750928; l2 norm of gradients: 2.1379122251879528; l2 norm of weights: 2.078430222451862\n","Iteration #1127  Loss: 2.6164565451842368; l2 norm of gradients: 2.135647670394011; l2 norm of weights: 2.0763274479243083\n","Iteration #1128  Loss: 2.61189642655229; l2 norm of gradients: 2.13338534696761; l2 norm of weights: 2.074226971415533\n","Iteration #1129  Loss: 2.607345955505344; l2 norm of gradients: 2.131125252919817; l2 norm of weights: 2.0721287907407047\n","Iteration #1130  Loss: 2.602805112308265; l2 norm of gradients: 2.1288673862639333; l2 norm of weights: 2.0700329037170295\n","Iteration #1131  Loss: 2.598273877264612; l2 norm of gradients: 2.1266117450154893; l2 norm of weights: 2.067939308163746\n","Iteration #1132  Loss: 2.5937522307165692; l2 norm of gradients: 2.1243583271922413; l2 norm of weights: 2.065848001902125\n","Iteration #1133  Loss: 2.589240153044866; l2 norm of gradients: 2.1221071308141646; l2 norm of weights: 2.063758982755466\n","Iteration #1134  Loss: 2.584737624668702; l2 norm of gradients: 2.119858153903451; l2 norm of weights: 2.061672248549094\n","Iteration #1135  Loss: 2.580244626045678; l2 norm of gradients: 2.117611394484504; l2 norm of weights: 2.0595877971103604\n","Iteration #1136  Loss: 2.575761137671715; l2 norm of gradients: 2.11536685058393; l2 norm of weights: 2.0575056262686378\n","Iteration #1137  Loss: 2.5712871400809822; l2 norm of gradients: 2.113124520230541; l2 norm of weights: 2.055425733855318\n","Iteration #1138  Loss: 2.5668226138458263; l2 norm of gradients: 2.1108844014553423; l2 norm of weights: 2.0533481177038113\n","Iteration #1139  Loss: 2.5623675395766905; l2 norm of gradients: 2.108646492291533; l2 norm of weights: 2.0512727756495424\n","Iteration #1140  Loss: 2.557921897922047; l2 norm of gradients: 2.1064107907744987; l2 norm of weights: 2.049199705529949\n","Iteration #1141  Loss: 2.5534856695683197; l2 norm of gradients: 2.1041772949418087; l2 norm of weights: 2.047128905184479\n","Iteration #1142  Loss: 2.5490588352398156; l2 norm of gradients: 2.101946002833208; l2 norm of weights: 2.045060372454591\n","Iteration #1143  Loss: 2.544641375698644; l2 norm of gradients: 2.0997169124906163; l2 norm of weights: 2.042994105183747\n","Iteration #1144  Loss: 2.5402332717446474; l2 norm of gradients: 2.0974900219581203; l2 norm of weights: 2.040930101217413\n","Iteration #1145  Loss: 2.53583450421533; l2 norm of gradients: 2.0952653292819714; l2 norm of weights: 2.038868358403058\n","Iteration #1146  Loss: 2.5314450539857836; l2 norm of gradients: 2.09304283251058; l2 norm of weights: 2.0368088745901494\n","Iteration #1147  Loss: 2.527064901968611; l2 norm of gradients: 2.090822529694509; l2 norm of weights: 2.0347516476301513\n","Iteration #1148  Loss: 2.52269402911386; l2 norm of gradients: 2.088604418886472; l2 norm of weights: 2.032696675376523\n","Iteration #1149  Loss: 2.518332416408948; l2 norm of gradients: 2.0863884981413263; l2 norm of weights: 2.030643955684717\n","Iteration #1150  Loss: 2.5139800448785863; l2 norm of gradients: 2.0841747655160696; l2 norm of weights: 2.028593486412175\n","Iteration #1151  Loss: 2.509636895584712; l2 norm of gradients: 2.0819632190698347; l2 norm of weights: 2.0265452654183256\n","Iteration #1152  Loss: 2.5053029496264196; l2 norm of gradients: 2.0797538568638845; l2 norm of weights: 2.0244992905645867\n","Iteration #1153  Loss: 2.500978188139879; l2 norm of gradients: 2.077546676961606; l2 norm of weights: 2.022455559714356\n","Iteration #1154  Loss: 2.496662592298272; l2 norm of gradients: 2.07534167742851; l2 norm of weights: 2.0204140707330147\n","Iteration #1155  Loss: 2.492356143311718; l2 norm of gradients: 2.073138856332221; l2 norm of weights: 2.0183748214879222\n","Iteration #1156  Loss: 2.4880588224272056; l2 norm of gradients: 2.0709382117424746; l2 norm of weights: 2.0163378098484155\n","Iteration #1157  Loss: 2.483770610928514; l2 norm of gradients: 2.068739741731115; l2 norm of weights: 2.0143030336858048\n","Iteration #1158  Loss: 2.4794914901361493; l2 norm of gradients: 2.0665434443720856; l2 norm of weights: 2.012270490873374\n","Iteration #1159  Loss: 2.475221441407273; l2 norm of gradients: 2.0643493177414287; l2 norm of weights: 2.010240179286377\n","Iteration #1160  Loss: 2.4709604461356274; l2 norm of gradients: 2.0621573599172773; l2 norm of weights: 2.0082120968020356\n","Iteration #1161  Loss: 2.4667084857514636; l2 norm of gradients: 2.059967568979853; l2 norm of weights: 2.0061862412995364\n","Iteration #1162  Loss: 2.4624655417214805; l2 norm of gradients: 2.057779943011459; l2 norm of weights: 2.004162610660031\n","Iteration #1163  Loss: 2.4582315955487433; l2 norm of gradients: 2.0555944800964783; l2 norm of weights: 2.0021412027666314\n","Iteration #1164  Loss: 2.4540066287726217; l2 norm of gradients: 2.053411178321364; l2 norm of weights: 2.00012201550441\n","Iteration #1165  Loss: 2.4497906229687145; l2 norm of gradients: 2.0512300357746405; l2 norm of weights: 1.9981050467603947\n","Iteration #1166  Loss: 2.445583559748781; l2 norm of gradients: 2.0490510505468933; l2 norm of weights: 1.9960902944235686\n","Iteration #1167  Loss: 2.441385420760675; l2 norm of gradients: 2.0468742207307686; l2 norm of weights: 1.9940777563848682\n","Iteration #1168  Loss: 2.43719618768827; l2 norm of gradients: 2.0446995444209657; l2 norm of weights: 1.9920674305371804\n","Iteration #1169  Loss: 2.4330158422513923; l2 norm of gradients: 2.042527019714233; l2 norm of weights: 1.9900593147753394\n","Iteration #1170  Loss: 2.4288443662057504; l2 norm of gradients: 2.0403566447093646; l2 norm of weights: 1.9880534069961255\n","Iteration #1171  Loss: 2.4246817413428676; l2 norm of gradients: 2.038188417507193; l2 norm of weights: 1.986049705098264\n","Iteration #1172  Loss: 2.4205279494900136; l2 norm of gradients: 2.0360223362105865; l2 norm of weights: 1.9840482069824215\n","Iteration #1173  Loss: 2.4163829725101333; l2 norm of gradients: 2.033858398924442; l2 norm of weights: 1.9820489105512042\n","Iteration #1174  Loss: 2.4122467923017776; l2 norm of gradients: 2.0316966037556847; l2 norm of weights: 1.9800518137091556\n","Iteration #1175  Loss: 2.408119390799038; l2 norm of gradients: 2.0295369488132575; l2 norm of weights: 1.9780569143627547\n","Iteration #1176  Loss: 2.404000749971476; l2 norm of gradients: 2.027379432208121; l2 norm of weights: 1.9760642104204134\n","Iteration #1177  Loss: 2.399890851824056; l2 norm of gradients: 2.025224052053247; l2 norm of weights: 1.9740736997924753\n","Iteration #1178  Loss: 2.3957896783970742; l2 norm of gradients: 2.0230708064636125; l2 norm of weights: 1.9720853803912117\n","Iteration #1179  Loss: 2.3916972117660955; l2 norm of gradients: 2.0209196935561966; l2 norm of weights: 1.970099250130822\n","Iteration #1180  Loss: 2.387613434041883; l2 norm of gradients: 2.018770711449976; l2 norm of weights: 1.96811530692743\n","Iteration #1181  Loss: 2.3835383273703266; l2 norm of gradients: 2.0166238582659184; l2 norm of weights: 1.9661335486990803\n","Iteration #1182  Loss: 2.379471873932384; l2 norm of gradients: 2.0144791321269797; l2 norm of weights: 1.9641539733657403\n","Iteration #1183  Loss: 2.3754140559440042; l2 norm of gradients: 2.0123365311580974; l2 norm of weights: 1.9621765788492935\n","Iteration #1184  Loss: 2.3713648556560685; l2 norm of gradients: 2.010196053486187; l2 norm of weights: 1.9602013630735413\n","Iteration #1185  Loss: 2.367324255354316; l2 norm of gradients: 2.0080576972401394; l2 norm of weights: 1.9582283239641982\n","Iteration #1186  Loss: 2.363292237359281; l2 norm of gradients: 2.00592146055081; l2 norm of weights: 1.9562574594488904\n","Iteration #1187  Loss: 2.359268784026226; l2 norm of gradients: 2.0037873415510212; l2 norm of weights: 1.9542887674571547\n","Iteration #1188  Loss: 2.355253877745072; l2 norm of gradients: 2.001655338375552; l2 norm of weights: 1.9523222459204348\n","Iteration #1189  Loss: 2.3512475009403366; l2 norm of gradients: 1.999525449161136; l2 norm of weights: 1.9503578927720806\n","Iteration #1190  Loss: 2.3472496360710626; l2 norm of gradients: 1.9973976720464568; l2 norm of weights: 1.9483957059473453\n","Iteration #1191  Loss: 2.3432602656307546; l2 norm of gradients: 1.9952720051721424; l2 norm of weights: 1.9464356833833831\n","Iteration #1192  Loss: 2.339279372147313; l2 norm of gradients: 1.9931484466807603; l2 norm of weights: 1.9444778230192488\n","Iteration #1193  Loss: 2.3353069381829656; l2 norm of gradients: 1.9910269947168138; l2 norm of weights: 1.9425221227958924\n","Iteration #1194  Loss: 2.331342946334206; l2 norm of gradients: 1.988907647426736; l2 norm of weights: 1.9405685806561614\n","Iteration #1195  Loss: 2.3273873792317223; l2 norm of gradients: 1.9867904029588859; l2 norm of weights: 1.9386171945447948\n","Iteration #1196  Loss: 2.3234402195403367; l2 norm of gradients: 1.9846752594635437; l2 norm of weights: 1.9366679624084235\n","Iteration #1197  Loss: 2.319501449958934; l2 norm of gradients: 1.9825622150929059; l2 norm of weights: 1.9347208821955657\n","Iteration #1198  Loss: 2.3155710532204057; l2 norm of gradients: 1.9804512680010806; l2 norm of weights: 1.9327759518566294\n","Iteration #1199  Loss: 2.311649012091575; l2 norm of gradients: 1.9783424163440826; l2 norm of weights: 1.9308331693439054\n","Iteration #1200  Loss: 2.307735309373135; l2 norm of gradients: 1.9762356582798284; l2 norm of weights: 1.9288925326115671\n","Iteration #1201  Loss: 2.303829927899588; l2 norm of gradients: 1.9741309919681327; l2 norm of weights: 1.9269540396156697\n","Iteration #1202  Loss: 2.299932850539175; l2 norm of gradients: 1.972028415570703; l2 norm of weights: 1.9250176883141468\n","Iteration #1203  Loss: 2.2960440601938137; l2 norm of gradients: 1.969927927251134; l2 norm of weights: 1.9230834766668083\n","Iteration #1204  Loss: 2.2921635397990334; l2 norm of gradients: 1.9678295251749052; l2 norm of weights: 1.921151402635339\n","Iteration #1205  Loss: 2.2882912723239115; l2 norm of gradients: 1.9657332075093734; l2 norm of weights: 1.919221464183296\n","Iteration #1206  Loss: 2.2844272407710067; l2 norm of gradients: 1.96363897242377; l2 norm of weights: 1.9172936592761072\n","Iteration #1207  Loss: 2.2805714281763003; l2 norm of gradients: 1.9615468180891957; l2 norm of weights: 1.9153679858810693\n","Iteration #1208  Loss: 2.2767238176091245; l2 norm of gradients: 1.9594567426786158; l2 norm of weights: 1.9134444419673444\n","Iteration #1209  Loss: 2.272884392172105; l2 norm of gradients: 1.9573687443668557; l2 norm of weights: 1.91152302550596\n","Iteration #1210  Loss: 2.269053135001096; l2 norm of gradients: 1.9552828213305964; l2 norm of weights: 1.9096037344698065\n","Iteration #1211  Loss: 2.2652300292651115; l2 norm of gradients: 1.953198971748369; l2 norm of weights: 1.907686566833633\n","Iteration #1212  Loss: 2.26141505816627; l2 norm of gradients: 1.951117193800551; l2 norm of weights: 1.9057715205740484\n","Iteration #1213  Loss: 2.257608204939724; l2 norm of gradients: 1.9490374856693613; l2 norm of weights: 1.9038585936695176\n","Iteration #1214  Loss: 2.253809452853604; l2 norm of gradients: 1.9469598455388557; l2 norm of weights: 1.9019477841003603\n","Iteration #1215  Loss: 2.2500187852089466; l2 norm of gradients: 1.9448842715949213; l2 norm of weights: 1.9000390898487478\n","Iteration #1216  Loss: 2.246236185339639; l2 norm of gradients: 1.9428107620252735; l2 norm of weights: 1.8981325088987024\n","Iteration #1217  Loss: 2.242461636612352; l2 norm of gradients: 1.940739315019451; l2 norm of weights: 1.8962280392360946\n","Iteration #1218  Loss: 2.2386951224264813; l2 norm of gradients: 1.9386699287688094; l2 norm of weights: 1.8943256788486416\n","Iteration #1219  Loss: 2.2349366262140795; l2 norm of gradients: 1.9366026014665187; l2 norm of weights: 1.8924254257259046\n","Iteration #1220  Loss: 2.231186131439798; l2 norm of gradients: 1.934537331307558; l2 norm of weights: 1.8905272778592874\n","Iteration #1221  Loss: 2.227443621600824; l2 norm of gradients: 1.932474116488711; l2 norm of weights: 1.8886312332420347\n","Iteration #1222  Loss: 2.223709080226817; l2 norm of gradients: 1.93041295520856; l2 norm of weights: 1.886737289869229\n","Iteration #1223  Loss: 2.219982490879848; l2 norm of gradients: 1.9283538456674847; l2 norm of weights: 1.8848454457377901\n","Iteration #1224  Loss: 2.2162638371543344; l2 norm of gradients: 1.9262967860676532; l2 norm of weights: 1.8829556988464713\n","Iteration #1225  Loss: 2.212553102676986; l2 norm of gradients: 1.9242417746130211; l2 norm of weights: 1.8810680471958598\n","Iteration #1226  Loss: 2.2088502711067344; l2 norm of gradients: 1.9221888095093256; l2 norm of weights: 1.8791824887883726\n","Iteration #1227  Loss: 2.205155326134676; l2 norm of gradients: 1.92013788896408; l2 norm of weights: 1.8772990216282557\n","Iteration #1228  Loss: 2.201468251484012; l2 norm of gradients: 1.9180890111865703; l2 norm of weights: 1.8754176437215815\n","Iteration #1229  Loss: 2.197789030909981; l2 norm of gradients: 1.916042174387851; l2 norm of weights: 1.8735383530762468\n","Iteration #1230  Loss: 2.1941176481998075; l2 norm of gradients: 1.9139973767807394; l2 norm of weights: 1.8716611477019731\n","Iteration #1231  Loss: 2.1904540871726317; l2 norm of gradients: 1.911954616579811; l2 norm of weights: 1.869786025610301\n","Iteration #1232  Loss: 2.1867983316794524; l2 norm of gradients: 1.9099138920013972; l2 norm of weights: 1.8679129848145903\n","Iteration #1233  Loss: 2.1831503656030673; l2 norm of gradients: 1.9078752012635773; l2 norm of weights: 1.8660420233300181\n","Iteration #1234  Loss: 2.1795101728580124; l2 norm of gradients: 1.9058385425861772; l2 norm of weights: 1.8641731391735774\n","Iteration #1235  Loss: 2.175877737390497; l2 norm of gradients: 1.9038039141907626; l2 norm of weights: 1.8623063303640723\n","Iteration #1236  Loss: 2.172253043178349; l2 norm of gradients: 1.9017713143006363; l2 norm of weights: 1.86044159492212\n","Iteration #1237  Loss: 2.1686360742309523; l2 norm of gradients: 1.899740741140832; l2 norm of weights: 1.8585789308701466\n","Iteration #1238  Loss: 2.1650268145891864; l2 norm of gradients: 1.8977121929381118; l2 norm of weights: 1.8567183362323851\n","Iteration #1239  Loss: 2.161425248325367; l2 norm of gradients: 1.8956856679209593; l2 norm of weights: 1.8548598090348745\n","Iteration #1240  Loss: 2.157831359543184; l2 norm of gradients: 1.893661164319577; l2 norm of weights: 1.8530033473054568\n","Iteration #1241  Loss: 2.1542451323776453; l2 norm of gradients: 1.8916386803658818; l2 norm of weights: 1.851148949073776\n","Iteration #1242  Loss: 2.150666550995015; l2 norm of gradients: 1.8896182142934992; l2 norm of weights: 1.849296612371276\n","Iteration #1243  Loss: 2.147095599592755; l2 norm of gradients: 1.8875997643377602; l2 norm of weights: 1.8474463352311983\n","Iteration #1244  Loss: 2.143532262399464; l2 norm of gradients: 1.885583328735696; l2 norm of weights: 1.8455981156885806\n","Iteration #1245  Loss: 2.139976523674819; l2 norm of gradients: 1.883568905726034; l2 norm of weights: 1.8437519517802545\n","Iteration #1246  Loss: 2.1364283677095153; l2 norm of gradients: 1.8815564935491944; l2 norm of weights: 1.841907841544843\n","Iteration #1247  Loss: 2.1328877788252116; l2 norm of gradients: 1.8795460904472825; l2 norm of weights: 1.8400657830227616\n","Iteration #1248  Loss: 2.129354741374467; l2 norm of gradients: 1.8775376946640887; l2 norm of weights: 1.8382257742562123\n","Iteration #1249  Loss: 2.1258292397406793; l2 norm of gradients: 1.8755313044450805; l2 norm of weights: 1.8363878132891835\n","Iteration #1250  Loss: 2.1223112583380352; l2 norm of gradients: 1.8735269180374001; l2 norm of weights: 1.8345518981674493\n","Iteration #1251  Loss: 2.1188007816114434; l2 norm of gradients: 1.87152453368986; l2 norm of weights: 1.832718026938566\n","Iteration #1252  Loss: 2.115297794036482; l2 norm of gradients: 1.8695241496529376; l2 norm of weights: 1.8308861976518709\n","Iteration #1253  Loss: 2.111802280119336; l2 norm of gradients: 1.8675257641787704; l2 norm of weights: 1.8290564083584806\n","Iteration #1254  Loss: 2.1083142243967417; l2 norm of gradients: 1.8655293755211546; l2 norm of weights: 1.8272286571112886\n","Iteration #1255  Loss: 2.1048336114359274; l2 norm of gradients: 1.8635349819355373; l2 norm of weights: 1.8254029419649636\n","Iteration #1256  Loss: 2.1013604258345557; l2 norm of gradients: 1.861542581679014; l2 norm of weights: 1.8235792609759478\n","Iteration #1257  Loss: 2.097894652220666; l2 norm of gradients: 1.8595521730103248; l2 norm of weights: 1.8217576122024555\n","Iteration #1258  Loss: 2.0944362752526167; l2 norm of gradients: 1.857563754189848; l2 norm of weights: 1.8199379937044702\n","Iteration #1259  Loss: 2.090985279619027; l2 norm of gradients: 1.8555773234795976; l2 norm of weights: 1.818120403543743\n","Iteration #1260  Loss: 2.087541650038721; l2 norm of gradients: 1.8535928791432184; l2 norm of weights: 1.8163048397837922\n","Iteration #1261  Loss: 2.084105371260668; l2 norm of gradients: 1.851610419445982; l2 norm of weights: 1.8144913004898995\n","Iteration #1262  Loss: 2.0806764280639274; l2 norm of gradients: 1.8496299426547822; l2 norm of weights: 1.812679783729109\n","Iteration #1263  Loss: 2.0772548052575903; l2 norm of gradients: 1.8476514470381311; l2 norm of weights: 1.8108702875702258\n","Iteration #1264  Loss: 2.0738404876807217; l2 norm of gradients: 1.8456749308661538; l2 norm of weights: 1.8090628100838129\n","Iteration #1265  Loss: 2.0704334602023087; l2 norm of gradients: 1.8437003924105855; l2 norm of weights: 1.8072573493421913\n","Iteration #1266  Loss: 2.067033707721194; l2 norm of gradients: 1.841727829944768; l2 norm of weights: 1.805453903419436\n","Iteration #1267  Loss: 2.0636412151660295; l2 norm of gradients: 1.8397572417436414; l2 norm of weights: 1.8036524703913757\n","Iteration #1268  Loss: 2.060255967495215; l2 norm of gradients: 1.8377886260837453; l2 norm of weights: 1.8018530483355912\n","Iteration #1269  Loss: 2.056877949696838; l2 norm of gradients: 1.8358219812432115; l2 norm of weights: 1.8000556353314112\n","Iteration #1270  Loss: 2.0535071467886277; l2 norm of gradients: 1.833857305501759; l2 norm of weights: 1.7982602294599146\n","Iteration #1271  Loss: 2.050143543817888; l2 norm of gradients: 1.831894597140693; l2 norm of weights: 1.7964668288039243\n","Iteration #1272  Loss: 2.0467871258614467; l2 norm of gradients: 1.8299338544428974; l2 norm of weights: 1.7946754314480082\n","Iteration #1273  Loss: 2.043437878025599; l2 norm of gradients: 1.8279750756928335; l2 norm of weights: 1.7928860354784761\n","Iteration #1274  Loss: 2.040095785446053; l2 norm of gradients: 1.8260182591765333; l2 norm of weights: 1.7910986389833796\n","Iteration #1275  Loss: 2.03676083328787; l2 norm of gradients: 1.8240634031815974; l2 norm of weights: 1.7893132400525076\n","Iteration #1276  Loss: 2.0334330067454123; l2 norm of gradients: 1.82211050599719; l2 norm of weights: 1.7875298367773869\n","Iteration #1277  Loss: 2.030112291042287; l2 norm of gradients: 1.8201595659140348; l2 norm of weights: 1.7857484272512794\n","Iteration #1278  Loss: 2.0267986714312904; l2 norm of gradients: 1.8182105812244103; l2 norm of weights: 1.7839690095691807\n","Iteration #1279  Loss: 2.0234921331943503; l2 norm of gradients: 1.8162635502221474; l2 norm of weights: 1.782191581827817\n","Iteration #1280  Loss: 2.020192661642475; l2 norm of gradients: 1.8143184712026235; l2 norm of weights: 1.7804161421256453\n","Iteration #1281  Loss: 2.016900242115696; l2 norm of gradients: 1.8123753424627598; l2 norm of weights: 1.7786426885628506\n","Iteration #1282  Loss: 2.0136148599830124; l2 norm of gradients: 1.8104341623010165; l2 norm of weights: 1.776871219241344\n","Iteration #1283  Loss: 2.010336500642338; l2 norm of gradients: 1.8084949290173888; l2 norm of weights: 1.7751017322647615\n","Iteration #1284  Loss: 2.007065149520445; l2 norm of gradients: 1.8065576409134032; l2 norm of weights: 1.7733342257384614\n","Iteration #1285  Loss: 2.0038007920729095; l2 norm of gradients: 1.8046222962921121; l2 norm of weights: 1.7715686977695237\n","Iteration #1286  Loss: 2.0005434137840563; l2 norm of gradients: 1.8026888934580938; l2 norm of weights: 1.7698051464667466\n","Iteration #1287  Loss: 1.9972930001669087; l2 norm of gradients: 1.8007574307174423; l2 norm of weights: 1.7680435699406467\n","Iteration #1288  Loss: 1.9940495367631277; l2 norm of gradients: 1.7988279063777697; l2 norm of weights: 1.7662839663034562\n","Iteration #1289  Loss: 1.990813009142962; l2 norm of gradients: 1.7969003187481967; l2 norm of weights: 1.7645263336691208\n","Iteration #1290  Loss: 1.9875834029051946; l2 norm of gradients: 1.7949746661393529; l2 norm of weights: 1.7627706701532997\n","Iteration #1291  Loss: 1.9843607036770838; l2 norm of gradients: 1.79305094686337; l2 norm of weights: 1.7610169738733605\n","Iteration #1292  Loss: 1.9811448971143164; l2 norm of gradients: 1.7911291592338796; l2 norm of weights: 1.7592652429483817\n","Iteration #1293  Loss: 1.9779359689009484; l2 norm of gradients: 1.7892093015660087; l2 norm of weights: 1.7575154754991478\n","Iteration #1294  Loss: 1.9747339047493522; l2 norm of gradients: 1.7872913721763748; l2 norm of weights: 1.7557676696481481\n","Iteration #1295  Loss: 1.9715386904001666; l2 norm of gradients: 1.7853753693830845; l2 norm of weights: 1.7540218235195768\n","Iteration #1296  Loss: 1.9683503116222387; l2 norm of gradients: 1.7834612915057266; l2 norm of weights: 1.7522779352393287\n","Iteration #1297  Loss: 1.965168754212574; l2 norm of gradients: 1.78154913686537; l2 norm of weights: 1.7505360029349994\n","Iteration #1298  Loss: 1.9619940039962809; l2 norm of gradients: 1.7796389037845601; l2 norm of weights: 1.7487960247358827\n","Iteration #1299  Loss: 1.9588260468265193; l2 norm of gradients: 1.7777305905873138; l2 norm of weights: 1.7470579987729686\n","Iteration #1300  Loss: 1.9556648685844449; l2 norm of gradients: 1.7758241955991156; l2 norm of weights: 1.7453219231789423\n","Iteration #1301  Loss: 1.952510455179162; l2 norm of gradients: 1.7739197171469154; l2 norm of weights: 1.743587796088183\n","Iteration #1302  Loss: 1.949362792547663; l2 norm of gradients: 1.7720171535591236; l2 norm of weights: 1.74185561563676\n","Iteration #1303  Loss: 1.9462218666547826; l2 norm of gradients: 1.7701165031656063; l2 norm of weights: 1.7401253799624337\n","Iteration #1304  Loss: 1.9430876634931398; l2 norm of gradients: 1.768217764297683; l2 norm of weights: 1.7383970872046515\n","Iteration #1305  Loss: 1.939960169083088; l2 norm of gradients: 1.7663209352881235; l2 norm of weights: 1.7366707355045474\n","Iteration #1306  Loss: 1.9368393694726649; l2 norm of gradients: 1.7644260144711412; l2 norm of weights: 1.734946323004941\n","Iteration #1307  Loss: 1.9337252507375347; l2 norm of gradients: 1.7625330001823918; l2 norm of weights: 1.7332238478503335\n","Iteration #1308  Loss: 1.930617798980942; l2 norm of gradients: 1.7606418907589696; l2 norm of weights: 1.7315033081869087\n","Iteration #1309  Loss: 1.9275170003336535; l2 norm of gradients: 1.7587526845394017; l2 norm of weights: 1.7297847021625288\n","Iteration #1310  Loss: 1.924422840953911; l2 norm of gradients: 1.7568653798636467; l2 norm of weights: 1.7280680279267344\n","Iteration #1311  Loss: 1.9213353070273789; l2 norm of gradients: 1.7549799750730894; l2 norm of weights: 1.7263532836307431\n","Iteration #1312  Loss: 1.9182543847670877; l2 norm of gradients: 1.7530964685105377; l2 norm of weights: 1.7246404674274451\n","Iteration #1313  Loss: 1.915180060413389; l2 norm of gradients: 1.7512148585202192; l2 norm of weights: 1.722929577471405\n","Iteration #1314  Loss: 1.9121123202339014; l2 norm of gradients: 1.7493351434477769; l2 norm of weights: 1.721220611918859\n","Iteration #1315  Loss: 1.9090511505234553; l2 norm of gradients: 1.747457321640265; l2 norm of weights: 1.7195135689277108\n","Iteration #1316  Loss: 1.9059965376040466; l2 norm of gradients: 1.7455813914461473; l2 norm of weights: 1.717808446657533\n","Iteration #1317  Loss: 1.9029484678247852; l2 norm of gradients: 1.743707351215292; l2 norm of weights: 1.7161052432695654\n","Iteration #1318  Loss: 1.8999069275618385; l2 norm of gradients: 1.7418351992989685; l2 norm of weights: 1.71440395692671\n","Iteration #1319  Loss: 1.896871903218388; l2 norm of gradients: 1.7399649340498433; l2 norm of weights: 1.712704585793534\n","Iteration #1320  Loss: 1.893843381224572; l2 norm of gradients: 1.7380965538219766; l2 norm of weights: 1.7110071280362638\n","Iteration #1321  Loss: 1.8908213480374374; l2 norm of gradients: 1.73623005697082; l2 norm of weights: 1.709311581822786\n","Iteration #1322  Loss: 1.8878057901408891; l2 norm of gradients: 1.7343654418532108; l2 norm of weights: 1.7076179453226454\n","Iteration #1323  Loss: 1.8847966940456398; l2 norm of gradients: 1.7325027068273704; l2 norm of weights: 1.7059262167070426\n","Iteration #1324  Loss: 1.8817940462891576; l2 norm of gradients: 1.730641850252899; l2 norm of weights: 1.7042363941488325\n","Iteration #1325  Loss: 1.8787978334356163; l2 norm of gradients: 1.7287828704907737; l2 norm of weights: 1.7025484758225233\n","Iteration #1326  Loss: 1.8758080420758472; l2 norm of gradients: 1.7269257659033435; l2 norm of weights: 1.7008624599042743\n","Iteration #1327  Loss: 1.8728246588272843; l2 norm of gradients: 1.7250705348543272; l2 norm of weights: 1.699178344571894\n","Iteration #1328  Loss: 1.8698476703339186; l2 norm of gradients: 1.7232171757088095; l2 norm of weights: 1.6974961280048393\n","Iteration #1329  Loss: 1.866877063266247; l2 norm of gradients: 1.7213656868332365; l2 norm of weights: 1.6958158083842136\n","Iteration #1330  Loss: 1.8639128243212189; l2 norm of gradients: 1.7195160665954128; l2 norm of weights: 1.6941373838927638\n","Iteration #1331  Loss: 1.8609549402221914; l2 norm of gradients: 1.7176683133644994; l2 norm of weights: 1.692460852714881\n","Iteration #1332  Loss: 1.8580033977188766; l2 norm of gradients: 1.7158224255110077; l2 norm of weights: 1.6907862130365976\n","Iteration #1333  Loss: 1.8550581835872926; l2 norm of gradients: 1.7139784014067994; l2 norm of weights: 1.689113463045585\n","Iteration #1334  Loss: 1.8521192846297128; l2 norm of gradients: 1.712136239425079; l2 norm of weights: 1.6874426009311534\n","Iteration #1335  Loss: 1.8491866876746186; l2 norm of gradients: 1.7102959379403946; l2 norm of weights: 1.6857736248842492\n","Iteration #1336  Loss: 1.8462603795766488; l2 norm of gradients: 1.708457495328631; l2 norm of weights: 1.6841065330974538\n","Iteration #1337  Loss: 1.8433403472165502; l2 norm of gradients: 1.7066209099670084; l2 norm of weights: 1.682441323764982\n","Iteration #1338  Loss: 1.8404265775011286; l2 norm of gradients: 1.704786180234079; l2 norm of weights: 1.6807779950826802\n","Iteration #1339  Loss: 1.8375190573631999; l2 norm of gradients: 1.702953304509722; l2 norm of weights: 1.6791165452480246\n","Iteration #1340  Loss: 1.8346177737615408; l2 norm of gradients: 1.7011222811751419; l2 norm of weights: 1.6774569724601198\n","Iteration #1341  Loss: 1.8317227136808403; l2 norm of gradients: 1.699293108612866; l2 norm of weights: 1.675799274919698\n","Iteration #1342  Loss: 1.828833864131651; l2 norm of gradients: 1.6974657852067374; l2 norm of weights: 1.6741434508291158\n","Iteration #1343  Loss: 1.825951212150339; l2 norm of gradients: 1.6956403093419161; l2 norm of weights: 1.6724894983923533\n","Iteration #1344  Loss: 1.823074744799037; l2 norm of gradients: 1.6938166794048723; l2 norm of weights: 1.6708374158150134\n","Iteration #1345  Loss: 1.8202044491655955; l2 norm of gradients: 1.691994893783386; l2 norm of weights: 1.6691872013043192\n","Iteration #1346  Loss: 1.8173403123635343; l2 norm of gradients: 1.6901749508665405; l2 norm of weights: 1.6675388530691124\n","Iteration #1347  Loss: 1.814482321531992; l2 norm of gradients: 1.688356849044723; l2 norm of weights: 1.6658923693198515\n","Iteration #1348  Loss: 1.811630463835682; l2 norm of gradients: 1.6865405867096175; l2 norm of weights: 1.6642477482686115\n","Iteration #1349  Loss: 1.80878472646484; l2 norm of gradients: 1.6847261622542047; l2 norm of weights: 1.662604988129081\n","Iteration #1350  Loss: 1.8059450966351789; l2 norm of gradients: 1.6829135740727568; l2 norm of weights: 1.6609640871165605\n","Iteration #1351  Loss: 1.8031115615878417; l2 norm of gradients: 1.6811028205608354; l2 norm of weights: 1.6593250434479632\n","Iteration #1352  Loss: 1.8002841085893486; l2 norm of gradients: 1.6792939001152878; l2 norm of weights: 1.6576878553418095\n","Iteration #1353  Loss: 1.7974627249315545; l2 norm of gradients: 1.6774868111342442; l2 norm of weights: 1.6560525210182289\n","Iteration #1354  Loss: 1.7946473979315984; l2 norm of gradients: 1.6756815520171147; l2 norm of weights: 1.654419038698956\n","Iteration #1355  Loss: 1.7918381149318576; l2 norm of gradients: 1.6738781211645848; l2 norm of weights: 1.652787406607331\n","Iteration #1356  Loss: 1.7890348632998991; l2 norm of gradients: 1.6720765169786145; l2 norm of weights: 1.6511576229682965\n","Iteration #1357  Loss: 1.786237630428432; l2 norm of gradients: 1.6702767378624332; l2 norm of weights: 1.6495296860083966\n","Iteration #1358  Loss: 1.7834464037352609; l2 norm of gradients: 1.668478782220538; l2 norm of weights: 1.6479035939557756\n","Iteration #1359  Loss: 1.7806611706632391; l2 norm of gradients: 1.6666826484586894; l2 norm of weights: 1.6462793450401758\n","Iteration #1360  Loss: 1.7778819186802188; l2 norm of gradients: 1.6648883349839092; l2 norm of weights: 1.6446569374929356\n","Iteration #1361  Loss: 1.7751086352790078; l2 norm of gradients: 1.6630958402044784; l2 norm of weights: 1.6430363695469896\n","Iteration #1362  Loss: 1.7723413079773218; l2 norm of gradients: 1.6613051625299302; l2 norm of weights: 1.6414176394368665\n","Iteration #1363  Loss: 1.7695799243177337; l2 norm of gradients: 1.6595163003710522; l2 norm of weights: 1.6398007453986851\n","Iteration #1364  Loss: 1.7668244718676307; l2 norm of gradients: 1.6577292521398796; l2 norm of weights: 1.6381856856701558\n","Iteration #1365  Loss: 1.7640749382191683; l2 norm of gradients: 1.655944016249694; l2 norm of weights: 1.6365724584905785\n","Iteration #1366  Loss: 1.76133131098922; l2 norm of gradients: 1.6541605911150192; l2 norm of weights: 1.63496106210084\n","Iteration #1367  Loss: 1.7585935778193338; l2 norm of gradients: 1.65237897515162; l2 norm of weights: 1.6333514947434125\n","Iteration #1368  Loss: 1.7558617263756846; l2 norm of gradients: 1.650599166776497; l2 norm of weights: 1.6317437546623532\n","Iteration #1369  Loss: 1.753135744349028; l2 norm of gradients: 1.6488211644078854; l2 norm of weights: 1.6301378401033018\n","Iteration #1370  Loss: 1.7504156194546543; l2 norm of gradients: 1.6470449664652524; l2 norm of weights: 1.6285337493134793\n","Iteration #1371  Loss: 1.7477013394323424; l2 norm of gradients: 1.645270571369292; l2 norm of weights: 1.6269314805416863\n","Iteration #1372  Loss: 1.744992892046313; l2 norm of gradients: 1.643497977541923; l2 norm of weights: 1.6253310320383016\n","Iteration #1373  Loss: 1.7422902650851844; l2 norm of gradients: 1.641727183406289; l2 norm of weights: 1.623732402055281\n","Iteration #1374  Loss: 1.7395934463619231; l2 norm of gradients: 1.6399581873867504; l2 norm of weights: 1.6221355888461544\n","Iteration #1375  Loss: 1.7369024237138033; l2 norm of gradients: 1.638190987908886; l2 norm of weights: 1.6205405906660262\n","Iteration #1376  Loss: 1.734217185002356; l2 norm of gradients: 1.6364255833994883; l2 norm of weights: 1.6189474057715727\n","Iteration #1377  Loss: 1.731537718113327; l2 norm of gradients: 1.6346619722865599; l2 norm of weights: 1.6173560324210403\n","Iteration #1378  Loss: 1.7288640109566287; l2 norm of gradients: 1.6329001529993117; l2 norm of weights: 1.615766468874245\n","Iteration #1379  Loss: 1.726196051466298; l2 norm of gradients: 1.6311401239681604; l2 norm of weights: 1.61417871339257\n","Iteration #1380  Loss: 1.7235338276004457; l2 norm of gradients: 1.6293818836247256; l2 norm of weights: 1.6125927642389637\n","Iteration #1381  Loss: 1.720877327341218; l2 norm of gradients: 1.6276254304018247; l2 norm of weights: 1.6110086196779403\n","Iteration #1382  Loss: 1.7182265386947444; l2 norm of gradients: 1.6258707627334748; l2 norm of weights: 1.609426277975576\n","Iteration #1383  Loss: 1.7155814496910988; l2 norm of gradients: 1.6241178790548851; l2 norm of weights: 1.6078457373995085\n","Iteration #1384  Loss: 1.7129420483842501; l2 norm of gradients: 1.6223667778024575; l2 norm of weights: 1.606266996218936\n","Iteration #1385  Loss: 1.7103083228520204; l2 norm of gradients: 1.620617457413782; l2 norm of weights: 1.604690052704615\n","Iteration #1386  Loss: 1.7076802611960349; l2 norm of gradients: 1.6188699163276346; l2 norm of weights: 1.6031149051288576\n","Iteration #1387  Loss: 1.7050578515416848; l2 norm of gradients: 1.6171241529839748; l2 norm of weights: 1.6015415517655327\n","Iteration #1388  Loss: 1.702441082038077; l2 norm of gradients: 1.6153801658239428; l2 norm of weights: 1.5999699908900629\n","Iteration #1389  Loss: 1.6998299408579907; l2 norm of gradients: 1.6136379532898566; l2 norm of weights: 1.5984002207794226\n","Iteration #1390  Loss: 1.6972244161978343; l2 norm of gradients: 1.6118975138252098; l2 norm of weights: 1.5968322397121375\n","Iteration #1391  Loss: 1.6946244962775994; l2 norm of gradients: 1.6101588458746683; l2 norm of weights: 1.5952660459682828\n","Iteration #1392  Loss: 1.692030169340817; l2 norm of gradients: 1.6084219478840682; l2 norm of weights: 1.593701637829481\n","Iteration #1393  Loss: 1.689441423654515; l2 norm of gradients: 1.606686818300413; l2 norm of weights: 1.5921390135789022\n","Iteration #1394  Loss: 1.6868582475091696; l2 norm of gradients: 1.6049534555718703; l2 norm of weights: 1.59057817150126\n","Iteration #1395  Loss: 1.6842806292186654; l2 norm of gradients: 1.6032218581477706; l2 norm of weights: 1.5890191098828121\n","Iteration #1396  Loss: 1.6817085571202515; l2 norm of gradients: 1.601492024478604; l2 norm of weights: 1.5874618270113587\n","Iteration #1397  Loss: 1.6791420195744933; l2 norm of gradients: 1.5997639530160168; l2 norm of weights: 1.5859063211762392\n","Iteration #1398  Loss: 1.6765810049652328; l2 norm of gradients: 1.598037642212811; l2 norm of weights: 1.584352590668333\n","Iteration #1399  Loss: 1.6740255016995442; l2 norm of gradients: 1.5963130905229395; l2 norm of weights: 1.5828006337800566\n","Iteration #1400  Loss: 1.6714754982076885; l2 norm of gradients: 1.5945902964015048; l2 norm of weights: 1.581250448805363\n","Iteration #1401  Loss: 1.6689309829430712; l2 norm of gradients: 1.5928692583047561; l2 norm of weights: 1.579702034039739\n","Iteration #1402  Loss: 1.6663919443821986; l2 norm of gradients: 1.591149974690087; l2 norm of weights: 1.5781553877802048\n","Iteration #1403  Loss: 1.6638583710246335; l2 norm of gradients: 1.5894324440160335; l2 norm of weights: 1.5766105083253124\n","Iteration #1404  Loss: 1.6613302513929544; l2 norm of gradients: 1.5877166647422705; l2 norm of weights: 1.5750673939751436\n","Iteration #1405  Loss: 1.6588075740327095; l2 norm of gradients: 1.5860026353296093; l2 norm of weights: 1.5735260430313096\n","Iteration #1406  Loss: 1.656290327512373; l2 norm of gradients: 1.5842903542399958; l2 norm of weights: 1.5719864537969472\n","Iteration #1407  Loss: 1.6537785004233063; l2 norm of gradients: 1.5825798199365089; l2 norm of weights: 1.5704486245767209\n","Iteration #1408  Loss: 1.6512720813797115; l2 norm of gradients: 1.5808710308833562; l2 norm of weights: 1.5689125536768185\n","Iteration #1409  Loss: 1.6487710590185873; l2 norm of gradients: 1.579163985545872; l2 norm of weights: 1.56737823940495\n","Iteration #1410  Loss: 1.6462754219996905; l2 norm of gradients: 1.5774586823905157; l2 norm of weights: 1.5658456800703486\n","Iteration #1411  Loss: 1.6437851590054882; l2 norm of gradients: 1.57575511988487; l2 norm of weights: 1.5643148739837651\n","Iteration #1412  Loss: 1.6413002587411194; l2 norm of gradients: 1.5740532964976361; l2 norm of weights: 1.5627858194574704\n","Iteration #1413  Loss: 1.6388207099343508; l2 norm of gradients: 1.5723532106986333; l2 norm of weights: 1.561258514805252\n","Iteration #1414  Loss: 1.6363465013355323; l2 norm of gradients: 1.570654860958797; l2 norm of weights: 1.5597329583424122\n","Iteration #1415  Loss: 1.6338776217175581; l2 norm of gradients: 1.5689582457501732; l2 norm of weights: 1.5582091483857687\n","Iteration #1416  Loss: 1.6314140598758222; l2 norm of gradients: 1.5672633635459212; l2 norm of weights: 1.556687083253651\n","Iteration #1417  Loss: 1.6289558046281754; l2 norm of gradients: 1.5655702128203066; l2 norm of weights: 1.5551667612658997\n","Iteration #1418  Loss: 1.6265028448148846; l2 norm of gradients: 1.5638787920487023; l2 norm of weights: 1.5536481807438656\n","Iteration #1419  Loss: 1.6240551692985912; l2 norm of gradients: 1.5621890997075834; l2 norm of weights: 1.5521313400104075\n","Iteration #1420  Loss: 1.6216127669642657; l2 norm of gradients: 1.5605011342745272; l2 norm of weights: 1.550616237389891\n","Iteration #1421  Loss: 1.619175626719171; l2 norm of gradients: 1.5588148942282098; l2 norm of weights: 1.549102871208188\n","Iteration #1422  Loss: 1.6167437374928146; l2 norm of gradients: 1.5571303780484047; l2 norm of weights: 1.547591239792673\n","Iteration #1423  Loss: 1.614317088236911; l2 norm of gradients: 1.555447584215979; l2 norm of weights: 1.5460813414722234\n","Iteration #1424  Loss: 1.61189566792534; l2 norm of gradients: 1.5537665112128933; l2 norm of weights: 1.5445731745772195\n","Iteration #1425  Loss: 1.6094794655540998; l2 norm of gradients: 1.5520871575221977; l2 norm of weights: 1.5430667374395384\n","Iteration #1426  Loss: 1.6070684701412743; l2 norm of gradients: 1.55040952162803; l2 norm of weights: 1.5415620283925582\n","Iteration #1427  Loss: 1.6046626707269822; l2 norm of gradients: 1.5487336020156142; l2 norm of weights: 1.5400590457711518\n","Iteration #1428  Loss: 1.6022620563733418; l2 norm of gradients: 1.5470593971712576; l2 norm of weights: 1.5385577879116885\n","Iteration #1429  Loss: 1.599866616164428; l2 norm of gradients: 1.5453869055823488; l2 norm of weights: 1.5370582531520318\n","Iteration #1430  Loss: 1.5974763392062283; l2 norm of gradients: 1.5437161257373564; l2 norm of weights: 1.5355604398315363\n","Iteration #1431  Loss: 1.5950912146266079; l2 norm of gradients: 1.542047056125825; l2 norm of weights: 1.53406434629105\n","Iteration #1432  Loss: 1.592711231575261; l2 norm of gradients: 1.5403796952383746; l2 norm of weights: 1.5325699708729084\n","Iteration #1433  Loss: 1.5903363792236753; l2 norm of gradients: 1.5387140415666989; l2 norm of weights: 1.5310773119209367\n","Iteration #1434  Loss: 1.5879666467650901; l2 norm of gradients: 1.537050093603561; l2 norm of weights: 1.529586367780447\n","Iteration #1435  Loss: 1.58560202341445; l2 norm of gradients: 1.5353878498427924; l2 norm of weights: 1.5280971367982354\n","Iteration #1436  Loss: 1.5832424984083744; l2 norm of gradients: 1.5337273087792935; l2 norm of weights: 1.5266096173225845\n","Iteration #1437  Loss: 1.5808880610051061; l2 norm of gradients: 1.5320684689090263; l2 norm of weights: 1.5251238077032574\n","Iteration #1438  Loss: 1.5785387004844778; l2 norm of gradients: 1.530411328729017; l2 norm of weights: 1.523639706291499\n","Iteration #1439  Loss: 1.5761944061478679; l2 norm of gradients: 1.5287558867373525; l2 norm of weights: 1.5221573114400349\n","Iteration #1440  Loss: 1.5738551673181613; l2 norm of gradients: 1.5271021414331758; l2 norm of weights: 1.5206766215030678\n","Iteration #1441  Loss: 1.5715209733397086; l2 norm of gradients: 1.5254500913166886; l2 norm of weights: 1.5191976348362783\n","Iteration #1442  Loss: 1.5691918135782863; l2 norm of gradients: 1.5237997348891454; l2 norm of weights: 1.5177203497968226\n","Iteration #1443  Loss: 1.5668676774210553; l2 norm of gradients: 1.5221510706528543; l2 norm of weights: 1.5162447647433304\n","Iteration #1444  Loss: 1.5645485542765214; l2 norm of gradients: 1.5205040971111723; l2 norm of weights: 1.514770878035905\n","Iteration #1445  Loss: 1.562234433574496; l2 norm of gradients: 1.5188588127685054; l2 norm of weights: 1.5132986880361208\n","Iteration #1446  Loss: 1.5599253047660544; l2 norm of gradients: 1.5172152161303063; l2 norm of weights: 1.5118281931070223\n","Iteration #1447  Loss: 1.557621157323496; l2 norm of gradients: 1.5155733057030714; l2 norm of weights: 1.5103593916131224\n","Iteration #1448  Loss: 1.5553219807403051; l2 norm of gradients: 1.5139330799943405; l2 norm of weights: 1.5088922819204011\n","Iteration #1449  Loss: 1.5530277645311121; l2 norm of gradients: 1.512294537512693; l2 norm of weights: 1.507426862396305\n","Iteration #1450  Loss: 1.5507384982316503; l2 norm of gradients: 1.5106576767677473; l2 norm of weights: 1.5059631314097444\n","Iteration #1451  Loss: 1.5484541713987199; l2 norm of gradients: 1.5090224962701586; l2 norm of weights: 1.504501087331093\n","Iteration #1452  Loss: 1.546174773610145; l2 norm of gradients: 1.507388994531617; l2 norm of weights: 1.5030407285321854\n","Iteration #1453  Loss: 1.5439002944647373; l2 norm of gradients: 1.5057571700648453; l2 norm of weights: 1.5015820533863178\n","Iteration #1454  Loss: 1.5416307235822544; l2 norm of gradients: 1.5041270213835978; l2 norm of weights: 1.5001250602682445\n","Iteration #1455  Loss: 1.5393660506033606; l2 norm of gradients: 1.502498547002657; l2 norm of weights: 1.4986697475541775\n","Iteration #1456  Loss: 1.5371062651895882; l2 norm of gradients: 1.500871745437834; l2 norm of weights: 1.4972161136217843\n","Iteration #1457  Loss: 1.5348513570232984; l2 norm of gradients: 1.499246615205965; l2 norm of weights: 1.4957641568501883\n","Iteration #1458  Loss: 1.5326013158076401; l2 norm of gradients: 1.497623154824909; l2 norm of weights: 1.494313875619965\n","Iteration #1459  Loss: 1.5303561312665144; l2 norm of gradients: 1.4960013628135485; l2 norm of weights: 1.4928652683131434\n","Iteration #1460  Loss: 1.528115793144531; l2 norm of gradients: 1.4943812376917844; l2 norm of weights: 1.4914183333132014\n","Iteration #1461  Loss: 1.525880291206973; l2 norm of gradients: 1.4927627779805377; l2 norm of weights: 1.4899730690050672\n","Iteration #1462  Loss: 1.5236496152397558; l2 norm of gradients: 1.491145982201744; l2 norm of weights: 1.4885294737751165\n","Iteration #1463  Loss: 1.5214237550493905; l2 norm of gradients: 1.489530848878355; l2 norm of weights: 1.487087546011172\n","Iteration #1464  Loss: 1.5192027004629423; l2 norm of gradients: 1.487917376534335; l2 norm of weights: 1.4856472841025015\n","Iteration #1465  Loss: 1.5169864413279917; l2 norm of gradients: 1.4863055636946585; l2 norm of weights: 1.4842086864398152\n","Iteration #1466  Loss: 1.5147749675125997; l2 norm of gradients: 1.484695408885312; l2 norm of weights: 1.4827717514152672\n","Iteration #1467  Loss: 1.5125682689052649; l2 norm of gradients: 1.4830869106332865; l2 norm of weights: 1.481336477422452\n","Iteration #1468  Loss: 1.5103663354148884; l2 norm of gradients: 1.4814800674665816; l2 norm of weights: 1.4799028628564042\n","Iteration #1469  Loss: 1.5081691569707316; l2 norm of gradients: 1.4798748779142004; l2 norm of weights: 1.4784709061135957\n","Iteration #1470  Loss: 1.5059767235223824; l2 norm of gradients: 1.4782713405061483; l2 norm of weights: 1.4770406055919365\n","Iteration #1471  Loss: 1.5037890250397121; l2 norm of gradients: 1.4766694537734317; l2 norm of weights: 1.475611959690771\n","Iteration #1472  Loss: 1.501606051512841; l2 norm of gradients: 1.4750692162480572; l2 norm of weights: 1.4741849668108793\n","Iteration #1473  Loss: 1.4994277929520992; l2 norm of gradients: 1.4734706264630286; l2 norm of weights: 1.4727596253544728\n","Iteration #1474  Loss: 1.497254239387986; l2 norm of gradients: 1.4718736829523444; l2 norm of weights: 1.4713359337251952\n","Iteration #1475  Loss: 1.4950853808711357; l2 norm of gradients: 1.4702783842509994; l2 norm of weights: 1.4699138903281204\n","Iteration #1476  Loss: 1.4929212074722784; l2 norm of gradients: 1.4686847288949798; l2 norm of weights: 1.4684934935697511\n","Iteration #1477  Loss: 1.4907617092821992; l2 norm of gradients: 1.4670927154212634; l2 norm of weights: 1.4670747418580166\n","Iteration #1478  Loss: 1.4886068764117049; l2 norm of gradients: 1.465502342367817; l2 norm of weights: 1.4656576336022733\n","Iteration #1479  Loss: 1.4864566989915833; l2 norm of gradients: 1.4639136082735966; l2 norm of weights: 1.464242167213302\n","Iteration #1480  Loss: 1.4843111671725662; l2 norm of gradients: 1.462326511678543; l2 norm of weights: 1.4628283411033065\n","Iteration #1481  Loss: 1.4821702711252926; l2 norm of gradients: 1.4607410511235825; l2 norm of weights: 1.4614161536859132\n","Iteration #1482  Loss: 1.4800340010402695; l2 norm of gradients: 1.4591572251506244; l2 norm of weights: 1.4600056033761686\n","Iteration #1483  Loss: 1.4779023471278359; l2 norm of gradients: 1.4575750323025598; l2 norm of weights: 1.458596688590539\n","Iteration #1484  Loss: 1.475775299618126; l2 norm of gradients: 1.4559944711232593; l2 norm of weights: 1.4571894077469092\n","Iteration #1485  Loss: 1.4736528487610292; l2 norm of gradients: 1.4544155401575731; l2 norm of weights: 1.4557837592645795\n","Iteration #1486  Loss: 1.471534984826154; l2 norm of gradients: 1.452838237951327; l2 norm of weights: 1.4543797415642659\n","Iteration #1487  Loss: 1.4694216981027928; l2 norm of gradients: 1.4512625630513234; l2 norm of weights: 1.4529773530680985\n","Iteration #1488  Loss: 1.4673129788998822; l2 norm of gradients: 1.4496885140053382; l2 norm of weights: 1.4515765921996204\n","Iteration #1489  Loss: 1.4652088175459674; l2 norm of gradients: 1.4481160893621203; l2 norm of weights: 1.450177457383786\n","Iteration #1490  Loss: 1.4631092043891627; l2 norm of gradients: 1.446545287671389; l2 norm of weights: 1.4487799470469582\n","Iteration #1491  Loss: 1.4610141297971198; l2 norm of gradients: 1.4449761074838332; l2 norm of weights: 1.4473840596169107\n","Iteration #1492  Loss: 1.4589235841569843; l2 norm of gradients: 1.4434085473511107; l2 norm of weights: 1.445989793522823\n","Iteration #1493  Loss: 1.4568375578753645; l2 norm of gradients: 1.4418426058258447; l2 norm of weights: 1.4445971471952812\n","Iteration #1494  Loss: 1.4547560413782914; l2 norm of gradients: 1.4402782814616246; l2 norm of weights: 1.4432061190662757\n","Iteration #1495  Loss: 1.4526790251111825; l2 norm of gradients: 1.4387155728130034; l2 norm of weights: 1.4418167075692003\n","Iteration #1496  Loss: 1.4506064995388077; l2 norm of gradients: 1.437154478435496; l2 norm of weights: 1.4404289111388515\n","Iteration #1497  Loss: 1.4485384551452472; l2 norm of gradients: 1.4355949968855783; l2 norm of weights: 1.4390427282114249\n","Iteration #1498  Loss: 1.4464748824338622; l2 norm of gradients: 1.4340371267206864; l2 norm of weights: 1.4376581572245168\n","Iteration #1499  Loss: 1.4444157719272528; l2 norm of gradients: 1.4324808664992135; l2 norm of weights: 1.436275196617121\n","Iteration #1500  Loss: 1.442361114167224; l2 norm of gradients: 1.4309262147805109; l2 norm of weights: 1.434893844829628\n","Iteration #1501  Loss: 1.440310899714749; l2 norm of gradients: 1.4293731701248835; l2 norm of weights: 1.4335141003038236\n","Iteration #1502  Loss: 1.4382651191499327; l2 norm of gradients: 1.4278217310935921; l2 norm of weights: 1.4321359614828872\n","Iteration #1503  Loss: 1.4362237630719783; l2 norm of gradients: 1.4262718962488485; l2 norm of weights: 1.4307594268113926\n","Iteration #1504  Loss: 1.4341868220991445; l2 norm of gradients: 1.4247236641538172; l2 norm of weights: 1.4293844947353023\n","Iteration #1505  Loss: 1.4321542868687185; l2 norm of gradients: 1.423177033372612; l2 norm of weights: 1.4280111637019715\n","Iteration #1506  Loss: 1.4301261480369718; l2 norm of gradients: 1.4216320024702953; l2 norm of weights: 1.4266394321601426\n","Iteration #1507  Loss: 1.42810239627913; l2 norm of gradients: 1.4200885700128771; l2 norm of weights: 1.425269298559946\n","Iteration #1508  Loss: 1.4260830222893328; l2 norm of gradients: 1.4185467345673133; l2 norm of weights: 1.4239007613528978\n","Iteration #1509  Loss: 1.4240680167806024; l2 norm of gradients: 1.4170064947015044; l2 norm of weights: 1.4225338189918995\n","Iteration #1510  Loss: 1.4220573704848043; l2 norm of gradients: 1.4154678489842951; l2 norm of weights: 1.421168469931236\n","Iteration #1511  Loss: 1.4200510741526127; l2 norm of gradients: 1.4139307959854714; l2 norm of weights: 1.4198047126265734\n","Iteration #1512  Loss: 1.4180491185534765; l2 norm of gradients: 1.4123953342757611; l2 norm of weights: 1.4184425455349603\n","Iteration #1513  Loss: 1.4160514944755813; l2 norm of gradients: 1.4108614624268303; l2 norm of weights: 1.4170819671148236\n","Iteration #1514  Loss: 1.4140581927258162; l2 norm of gradients: 1.4093291790112852; l2 norm of weights: 1.4157229758259693\n","Iteration #1515  Loss: 1.4120692041297362; l2 norm of gradients: 1.4077984826026682; l2 norm of weights: 1.4143655701295796\n","Iteration #1516  Loss: 1.410084519531529; l2 norm of gradients: 1.4062693717754575; l2 norm of weights: 1.4130097484882131\n","Iteration #1517  Loss: 1.4081041297939785; l2 norm of gradients: 1.4047418451050664; l2 norm of weights: 1.4116555093658023\n","Iteration #1518  Loss: 1.4061280257984294; l2 norm of gradients: 1.4032159011678422; l2 norm of weights: 1.4103028512276528\n","Iteration #1519  Loss: 1.404156198444753; l2 norm of gradients: 1.4016915385410633; l2 norm of weights: 1.408951772540442\n","Iteration #1520  Loss: 1.4021886386513118; l2 norm of gradients: 1.4001687558029405; l2 norm of weights: 1.4076022717722176\n","Iteration #1521  Loss: 1.4002253373549243; l2 norm of gradients: 1.3986475515326136; l2 norm of weights: 1.406254347392397\n","Iteration #1522  Loss: 1.3982662855108294; l2 norm of gradients: 1.3971279243101515; l2 norm of weights: 1.4049079978717645\n","Iteration #1523  Loss: 1.396311474092653; l2 norm of gradients: 1.3956098727165507; l2 norm of weights: 1.4035632216824716\n","Iteration #1524  Loss: 1.3943608940923724; l2 norm of gradients: 1.3940933953337344; l2 norm of weights: 1.4022200172980355\n","Iteration #1525  Loss: 1.3924145365202811; l2 norm of gradients: 1.3925784907445509; l2 norm of weights: 1.400878383193336\n","Iteration #1526  Loss: 1.3904723924049547; l2 norm of gradients: 1.3910651575327722; l2 norm of weights: 1.3995383178446168\n","Iteration #1527  Loss: 1.3885344527932164; l2 norm of gradients: 1.3895533942830938; l2 norm of weights: 1.3981998197294825\n","Iteration #1528  Loss: 1.3866007087501016; l2 norm of gradients: 1.3880431995811333; l2 norm of weights: 1.3968628873268976\n","Iteration #1529  Loss: 1.3846711513588241; l2 norm of gradients: 1.3865345720134288; l2 norm of weights: 1.3955275191171856\n","Iteration #1530  Loss: 1.3827457717207419; l2 norm of gradients: 1.3850275101674379; l2 norm of weights: 1.3941937135820277\n","Iteration #1531  Loss: 1.3808245609553214; l2 norm of gradients: 1.3835220126315368; l2 norm of weights: 1.3928614692044607\n","Iteration #1532  Loss: 1.378907510200105; l2 norm of gradients: 1.38201807799502; l2 norm of weights: 1.3915307844688771\n","Iteration #1533  Loss: 1.3769946106106756; l2 norm of gradients: 1.3805157048480974; l2 norm of weights: 1.3902016578610223\n","Iteration #1534  Loss: 1.3750858533606216; l2 norm of gradients: 1.3790148917818945; l2 norm of weights: 1.3888740878679944\n","Iteration #1535  Loss: 1.373181229641506; l2 norm of gradients: 1.3775156373884514; l2 norm of weights: 1.3875480729782426\n","Iteration #1536  Loss: 1.371280730662828; l2 norm of gradients: 1.3760179402607216; l2 norm of weights: 1.3862236116815658\n","Iteration #1537  Loss: 1.3693843476519916; l2 norm of gradients: 1.3745217989925702; l2 norm of weights: 1.3849007024691111\n","Iteration #1538  Loss: 1.367492071854272; l2 norm of gradients: 1.3730272121787737; l2 norm of weights: 1.3835793438333734\n","Iteration #1539  Loss: 1.3656038945327797; l2 norm of gradients: 1.3715341784150188; l2 norm of weights: 1.382259534268193\n","Iteration #1540  Loss: 1.3637198069684282; l2 norm of gradients: 1.3700426962979013; l2 norm of weights: 1.3809412722687548\n","Iteration #1541  Loss: 1.3618398004599002; l2 norm of gradients: 1.3685527644249251; l2 norm of weights: 1.3796245563315879\n","Iteration #1542  Loss: 1.3599638663236124; l2 norm of gradients: 1.3670643813945014; l2 norm of weights: 1.3783093849545622\n","Iteration #1543  Loss: 1.3580919958936843; l2 norm of gradients: 1.3655775458059474; l2 norm of weights: 1.3769957566368898\n","Iteration #1544  Loss: 1.3562241805219013; l2 norm of gradients: 1.3640922562594844; l2 norm of weights: 1.3756836698791208\n","Iteration #1545  Loss: 1.3543604115776848; l2 norm of gradients: 1.3626085113562396; l2 norm of weights: 1.374373123183145\n","Iteration #1546  Loss: 1.3525006804480562; l2 norm of gradients: 1.361126309698242; l2 norm of weights: 1.3730641150521885\n","Iteration #1547  Loss: 1.350644978537603; l2 norm of gradients: 1.359645649888424; l2 norm of weights: 1.3717566439908127\n","Iteration #1548  Loss: 1.3487932972684489; l2 norm of gradients: 1.3581665305306176; l2 norm of weights: 1.3704507085049142\n","Iteration #1549  Loss: 1.346945628080216; l2 norm of gradients: 1.356688950229557; l2 norm of weights: 1.369146307101722\n","Iteration #1550  Loss: 1.345101962429996; l2 norm of gradients: 1.3552129075908739; l2 norm of weights: 1.3678434382897984\n","Iteration #1551  Loss: 1.3432622917923125; l2 norm of gradients: 1.3537384012211005; l2 norm of weights: 1.3665421005790344\n","Iteration #1552  Loss: 1.3414266076590915; l2 norm of gradients: 1.3522654297276644; l2 norm of weights: 1.3652422924806515\n","Iteration #1553  Loss: 1.3395949015396262; l2 norm of gradients: 1.3507939917188914; l2 norm of weights: 1.3639440125071993\n","Iteration #1554  Loss: 1.3377671649605456; l2 norm of gradients: 1.349324085804002; l2 norm of weights: 1.3626472591725538\n","Iteration #1555  Loss: 1.335943389465779; l2 norm of gradients: 1.3478557105931124; l2 norm of weights: 1.3613520309919165\n","Iteration #1556  Loss: 1.3341235666165274; l2 norm of gradients: 1.3463888646972315; l2 norm of weights: 1.360058326481814\n","Iteration #1557  Loss: 1.3323076879912252; l2 norm of gradients: 1.3449235467282623; l2 norm of weights: 1.358766144160095\n","Iteration #1558  Loss: 1.330495745185512; l2 norm of gradients: 1.3434597552989995; l2 norm of weights: 1.3574754825459299\n","Iteration #1559  Loss: 1.328687729812197; l2 norm of gradients: 1.3419974890231292; l2 norm of weights: 1.3561863401598104\n","Iteration #1560  Loss: 1.3268836335012284; l2 norm of gradients: 1.3405367465152278; l2 norm of weights: 1.354898715523547\n","Iteration #1561  Loss: 1.3250834478996592; l2 norm of gradients: 1.3390775263907613; l2 norm of weights: 1.3536126071602679\n","Iteration #1562  Loss: 1.3232871646716147; l2 norm of gradients: 1.3376198272660842; l2 norm of weights: 1.352328013594418\n","Iteration #1563  Loss: 1.3214947754982624; l2 norm of gradients: 1.3361636477584395; l2 norm of weights: 1.3510449333517587\n","Iteration #1564  Loss: 1.3197062720777752; l2 norm of gradients: 1.3347089864859565; l2 norm of weights: 1.3497633649593639\n","Iteration #1565  Loss: 1.3179216461253036; l2 norm of gradients: 1.3332558420676512; l2 norm of weights: 1.348483306945622\n","Iteration #1566  Loss: 1.31614088937294; l2 norm of gradients: 1.331804213123425; l2 norm of weights: 1.3472047578402317\n","Iteration #1567  Loss: 1.3143639935696882; l2 norm of gradients: 1.3303540982740636; l2 norm of weights: 1.3459277161742034\n","Iteration #1568  Loss: 1.3125909504814306; l2 norm of gradients: 1.3289054961412368; l2 norm of weights: 1.3446521804798555\n","Iteration #1569  Loss: 1.3108217518908967; l2 norm of gradients: 1.3274584053474972; l2 norm of weights: 1.3433781492908154\n","Iteration #1570  Loss: 1.3090563895976284; l2 norm of gradients: 1.3260128245162799; l2 norm of weights: 1.342105621142016\n","Iteration #1571  Loss: 1.307294855417953; l2 norm of gradients: 1.324568752271901; l2 norm of weights: 1.3408345945696964\n","Iteration #1572  Loss: 1.305537141184946; l2 norm of gradients: 1.3231261872395572; l2 norm of weights: 1.3395650681113997\n","Iteration #1573  Loss: 1.3037832387484016; l2 norm of gradients: 1.3216851280453261; l2 norm of weights: 1.3382970403059717\n","Iteration #1574  Loss: 1.3020331399748017; l2 norm of gradients: 1.3202455733161635; l2 norm of weights: 1.3370305096935604\n","Iteration #1575  Loss: 1.3002868367472822; l2 norm of gradients: 1.3188075216799036; l2 norm of weights: 1.3357654748156131\n","Iteration #1576  Loss: 1.2985443209656011; l2 norm of gradients: 1.3173709717652584; l2 norm of weights: 1.3345019342148772\n","Iteration #1577  Loss: 1.2968055845461095; l2 norm of gradients: 1.3159359222018174; l2 norm of weights: 1.3332398864353976\n","Iteration #1578  Loss: 1.2950706194217172; l2 norm of gradients: 1.3145023716200455; l2 norm of weights: 1.3319793300225162\n","Iteration #1579  Loss: 1.293339417541862; l2 norm of gradients: 1.3130703186512829; l2 norm of weights: 1.3307202635228699\n","Iteration #1580  Loss: 1.291611970872478; l2 norm of gradients: 1.3116397619277453; l2 norm of weights: 1.3294626854843898\n","Iteration #1581  Loss: 1.2898882713959652; l2 norm of gradients: 1.310210700082522; l2 norm of weights: 1.3282065944563002\n","Iteration #1582  Loss: 1.2881683111111557; l2 norm of gradients: 1.308783131749576; l2 norm of weights: 1.3269519889891166\n","Iteration #1583  Loss: 1.2864520820332854; l2 norm of gradients: 1.3073570555637417; l2 norm of weights: 1.3256988676346455\n","Iteration #1584  Loss: 1.28473957619396; l2 norm of gradients: 1.3059324701607273; l2 norm of weights: 1.324447228945982\n","Iteration #1585  Loss: 1.2830307856411254; l2 norm of gradients: 1.3045093741771105; l2 norm of weights: 1.3231970714775099\n","Iteration #1586  Loss: 1.2813257024390352; l2 norm of gradients: 1.3030877662503413; l2 norm of weights: 1.3219483937848986\n","Iteration #1587  Loss: 1.2796243186682212; l2 norm of gradients: 1.3016676450187377; l2 norm of weights: 1.3207011944251041\n","Iteration #1588  Loss: 1.2779266264254605; l2 norm of gradients: 1.300249009121488; l2 norm of weights: 1.3194554719563658\n","Iteration #1589  Loss: 1.2762326178237458; l2 norm of gradients: 1.2988318571986495; l2 norm of weights: 1.3182112249382065\n","Iteration #1590  Loss: 1.2745422849922534; l2 norm of gradients: 1.2974161878911465; l2 norm of weights: 1.3169684519314306\n","Iteration #1591  Loss: 1.2728556200763133; l2 norm of gradients: 1.2960019998407712; l2 norm of weights: 1.315727151498123\n","Iteration #1592  Loss: 1.2711726152373783; l2 norm of gradients: 1.2945892916901818; l2 norm of weights: 1.3144873222016482\n","Iteration #1593  Loss: 1.2694932626529907; l2 norm of gradients: 1.2931780620829034; l2 norm of weights: 1.3132489626066481\n","Iteration #1594  Loss: 1.2678175545167556; l2 norm of gradients: 1.291768309663326; l2 norm of weights: 1.312012071279042\n","Iteration #1595  Loss: 1.2661454830383065; l2 norm of gradients: 1.2903600330767038; l2 norm of weights: 1.3107766467860242\n","Iteration #1596  Loss: 1.2644770404432772; l2 norm of gradients: 1.2889532309691563; l2 norm of weights: 1.3095426876960639\n","Iteration #1597  Loss: 1.2628122189732696; l2 norm of gradients: 1.2875479019876657; l2 norm of weights: 1.308310192578903\n","Iteration #1598  Loss: 1.2611510108858235; l2 norm of gradients: 1.2861440447800776; l2 norm of weights: 1.307079160005555\n","Iteration #1599  Loss: 1.2594934084543872; l2 norm of gradients: 1.2847416579950999; l2 norm of weights: 1.305849588548305\n","Iteration #1600  Loss: 1.2578394039682856; l2 norm of gradients: 1.283340740282302; l2 norm of weights: 1.3046214767807072\n","Iteration #1601  Loss: 1.2561889897326903; l2 norm of gradients: 1.2819412902921146; l2 norm of weights: 1.3033948232775832\n","Iteration #1602  Loss: 1.2545421580685892; l2 norm of gradients: 1.2805433066758287; l2 norm of weights: 1.3021696266150222\n","Iteration #1603  Loss: 1.2528989013127565; l2 norm of gradients: 1.2791467880855965; l2 norm of weights: 1.300945885370379\n","Iteration #1604  Loss: 1.2512592118177222; l2 norm of gradients: 1.277751733174428; l2 norm of weights: 1.2997235981222728\n","Iteration #1605  Loss: 1.2496230819517413; l2 norm of gradients: 1.2763581405961932; l2 norm of weights: 1.2985027634505861\n","Iteration #1606  Loss: 1.247990504098766; l2 norm of gradients: 1.2749660090056203; l2 norm of weights: 1.2972833799364636\n","Iteration #1607  Loss: 1.2463614706584125; l2 norm of gradients: 1.2735753370582952; l2 norm of weights: 1.2960654461623107\n","Iteration #1608  Loss: 1.2447359740459323; l2 norm of gradients: 1.2721861234106608; l2 norm of weights: 1.2948489607117921\n","Iteration #1609  Loss: 1.2431140066921835; l2 norm of gradients: 1.2707983667200176; l2 norm of weights: 1.2936339221698314\n","Iteration #1610  Loss: 1.2414955610435996; l2 norm of gradients: 1.2694120656445211; l2 norm of weights: 1.2924203291226093\n","Iteration #1611  Loss: 1.2398806295621583; l2 norm of gradients: 1.2680272188431834; l2 norm of weights: 1.291208180157562\n","Iteration #1612  Loss: 1.2382692047253543; l2 norm of gradients: 1.2666438249758714; l2 norm of weights: 1.2899974738633802\n","Iteration #1613  Loss: 1.2366612790261677; l2 norm of gradients: 1.2652618827033062; l2 norm of weights: 1.2887882088300087\n","Iteration #1614  Loss: 1.2350568449730366; l2 norm of gradients: 1.2638813906870643; l2 norm of weights: 1.2875803836486446\n","Iteration #1615  Loss: 1.2334558950898233; l2 norm of gradients: 1.2625023475895747; l2 norm of weights: 1.2863739969117356\n","Iteration #1616  Loss: 1.2318584219157886; l2 norm of gradients: 1.2611247520741198; l2 norm of weights: 1.285169047212979\n","Iteration #1617  Loss: 1.2302644180055606; l2 norm of gradients: 1.2597486028048346; l2 norm of weights: 1.2839655331473214\n","Iteration #1618  Loss: 1.2286738759291058; l2 norm of gradients: 1.258373898446707; l2 norm of weights: 1.2827634533109566\n","Iteration #1619  Loss: 1.227086788271698; l2 norm of gradients: 1.2570006376655753; l2 norm of weights: 1.281562806301324\n","Iteration #1620  Loss: 1.2255031476338916; l2 norm of gradients: 1.2556288191281302; l2 norm of weights: 1.2803635907171085\n","Iteration #1621  Loss: 1.2239229466314892; l2 norm of gradients: 1.2542584415019122; l2 norm of weights: 1.2791658051582384\n","Iteration #1622  Loss: 1.2223461778955156; l2 norm of gradients: 1.2528895034553131; l2 norm of weights: 1.277969448225885\n","Iteration #1623  Loss: 1.2207728340721853; l2 norm of gradients: 1.2515220036575732; l2 norm of weights: 1.2767745185224602\n","Iteration #1624  Loss: 1.2192029078228757; l2 norm of gradients: 1.2501559407787832; l2 norm of weights: 1.2755810146516164\n","Iteration #1625  Loss: 1.2176363918240969; l2 norm of gradients: 1.2487913134898825; l2 norm of weights: 1.2743889352182451\n","Iteration #1626  Loss: 1.2160732787674624; l2 norm of gradients: 1.247428120462659; l2 norm of weights: 1.273198278828475\n","Iteration #1627  Loss: 1.2145135613596605; l2 norm of gradients: 1.246066360369748; l2 norm of weights: 1.2720090440896712\n","Iteration #1628  Loss: 1.2129572323224258; l2 norm of gradients: 1.244706031884633; l2 norm of weights: 1.2708212296104344\n","Iteration #1629  Loss: 1.211404284392509; l2 norm of gradients: 1.2433471336816446; l2 norm of weights: 1.2696348340005994\n","Iteration #1630  Loss: 1.2098547103216493; l2 norm of gradients: 1.2419896644359603; l2 norm of weights: 1.2684498558712334\n","Iteration #1631  Loss: 1.2083085028765437; l2 norm of gradients: 1.2406336228236035; l2 norm of weights: 1.2672662938346348\n","Iteration #1632  Loss: 1.2067656548388213; l2 norm of gradients: 1.2392790075214437; l2 norm of weights: 1.2660841465043342\n","Iteration #1633  Loss: 1.2052261590050113; l2 norm of gradients: 1.237925817207196; l2 norm of weights: 1.2649034124950893\n","Iteration #1634  Loss: 1.203690008186516; l2 norm of gradients: 1.2365740505594207; l2 norm of weights: 1.263724090422887\n","Iteration #1635  Loss: 1.2021571952095815; l2 norm of gradients: 1.2352237062575222; l2 norm of weights: 1.2625461789049401\n","Iteration #1636  Loss: 1.2006277129152705; l2 norm of gradients: 1.2338747829817498; l2 norm of weights: 1.261369676559688\n","Iteration #1637  Loss: 1.1991015541594319; l2 norm of gradients: 1.2325272794131965; l2 norm of weights: 1.2601945820067937\n","Iteration #1638  Loss: 1.1975787118126724; l2 norm of gradients: 1.2311811942337991; l2 norm of weights: 1.2590208938671432\n","Iteration #1639  Loss: 1.196059178760331; l2 norm of gradients: 1.229836526126337; l2 norm of weights: 1.2578486107628457\n","Iteration #1640  Loss: 1.1945429479024463; l2 norm of gradients: 1.228493273774433; l2 norm of weights: 1.2566777313172297\n","Iteration #1641  Loss: 1.1930300121537307; l2 norm of gradients: 1.227151435862552; l2 norm of weights: 1.2555082541548437\n","Iteration #1642  Loss: 1.1915203644435426; l2 norm of gradients: 1.2258110110760003; l2 norm of weights: 1.254340177901455\n","Iteration #1643  Loss: 1.1900139977158564; l2 norm of gradients: 1.224471998100927; l2 norm of weights: 1.2531735011840475\n","Iteration #1644  Loss: 1.1885109049292346; l2 norm of gradients: 1.2231343956243221; l2 norm of weights: 1.2520082226308213\n","Iteration #1645  Loss: 1.1870110790568005; l2 norm of gradients: 1.2217982023340166; l2 norm of weights: 1.250844340871191\n","Iteration #1646  Loss: 1.1855145130862106; l2 norm of gradients: 1.2204634169186814; l2 norm of weights: 1.2496818545357848\n","Iteration #1647  Loss: 1.1840212000196246; l2 norm of gradients: 1.2191300380678287; l2 norm of weights: 1.2485207622564436\n","Iteration #1648  Loss: 1.1825311328736798; l2 norm of gradients: 1.21779806447181; l2 norm of weights: 1.2473610626662195\n","Iteration #1649  Loss: 1.1810443046794596; l2 norm of gradients: 1.216467494821817; l2 norm of weights: 1.2462027543993732\n","Iteration #1650  Loss: 1.17956070848247; l2 norm of gradients: 1.2151383278098797; l2 norm of weights: 1.2450458360913759\n","Iteration #1651  Loss: 1.1780803373426085; l2 norm of gradients: 1.213810562128868; l2 norm of weights: 1.2438903063789053\n","Iteration #1652  Loss: 1.1766031843341378; l2 norm of gradients: 1.2124841964724902; l2 norm of weights: 1.2427361638998455\n","Iteration #1653  Loss: 1.1751292425456572; l2 norm of gradients: 1.211159229535292; l2 norm of weights: 1.2415834072932863\n","Iteration #1654  Loss: 1.1736585050800759; l2 norm of gradients: 1.2098356600126583; l2 norm of weights: 1.240432035199521\n","Iteration #1655  Loss: 1.1721909650545843; l2 norm of gradients: 1.208513486600811; l2 norm of weights: 1.239282046260046\n","Iteration #1656  Loss: 1.1707266156006269; l2 norm of gradients: 1.2071927079968097; l2 norm of weights: 1.2381334391175585\n","Iteration #1657  Loss: 1.1692654498638744; l2 norm of gradients: 1.2058733228985503; l2 norm of weights: 1.2369862124159567\n","Iteration #1658  Loss: 1.167807461004198; l2 norm of gradients: 1.204555330004767; l2 norm of weights: 1.2358403648003387\n","Iteration #1659  Loss: 1.1663526421956378; l2 norm of gradients: 1.203238728015029; l2 norm of weights: 1.2346958949169986\n","Iteration #1660  Loss: 1.16490098662638; l2 norm of gradients: 1.2019235156297419; l2 norm of weights: 1.2335528014134292\n","Iteration #1661  Loss: 1.163452487498727; l2 norm of gradients: 1.200609691550148; l2 norm of weights: 1.232411082938318\n","Iteration #1662  Loss: 1.1620071380290702; l2 norm of gradients: 1.1992972544783247; l2 norm of weights: 1.2312707381415466\n","Iteration #1663  Loss: 1.1605649314478634; l2 norm of gradients: 1.1979862031171844; l2 norm of weights: 1.2301317656741908\n","Iteration #1664  Loss: 1.1591258609995951; l2 norm of gradients: 1.1966765361704754; l2 norm of weights: 1.2289941641885176\n","Iteration #1665  Loss: 1.157689919942762; l2 norm of gradients: 1.1953682523427795; l2 norm of weights: 1.2278579323379855\n","Iteration #1666  Loss: 1.1562571015498406; l2 norm of gradients: 1.1940613503395145; l2 norm of weights: 1.226723068777242\n","Iteration #1667  Loss: 1.1548273991072615; l2 norm of gradients: 1.1927558288669318; l2 norm of weights: 1.2255895721621233\n","Iteration #1668  Loss: 1.1534008059153815; l2 norm of gradients: 1.1914516866321163; l2 norm of weights: 1.224457441149653\n","Iteration #1669  Loss: 1.151977315288457; l2 norm of gradients: 1.1901489223429873; l2 norm of weights: 1.2233266743980409\n","Iteration #1670  Loss: 1.1505569205546167; l2 norm of gradients: 1.1888475347082974; l2 norm of weights: 1.2221972705666815\n","Iteration #1671  Loss: 1.1491396150558344; l2 norm of gradients: 1.1875475224376324; l2 norm of weights: 1.2210692283161526\n","Iteration #1672  Loss: 1.147725392147904; l2 norm of gradients: 1.1862488842414114; l2 norm of weights: 1.2199425463082154\n","Iteration #1673  Loss: 1.14631424520041; l2 norm of gradients: 1.1849516188308855; l2 norm of weights: 1.2188172232058117\n","Iteration #1674  Loss: 1.1449061675967034; l2 norm of gradients: 1.1836557249181392; l2 norm of weights: 1.2176932576730641\n","Iteration #1675  Loss: 1.143501152733873; l2 norm of gradients: 1.1823612012160887; l2 norm of weights: 1.2165706483752738\n","Iteration #1676  Loss: 1.1420991940227188; l2 norm of gradients: 1.1810680464384822; l2 norm of weights: 1.2154493939789197\n","Iteration #1677  Loss: 1.1407002848877261; l2 norm of gradients: 1.1797762592999004; l2 norm of weights: 1.214329493151657\n","Iteration #1678  Loss: 1.1393044187670411; l2 norm of gradients: 1.1784858385157546; l2 norm of weights: 1.2132109445623178\n","Iteration #1679  Loss: 1.1379115891124394; l2 norm of gradients: 1.1771967828022885; l2 norm of weights: 1.2120937468809068\n","Iteration #1680  Loss: 1.1365217893893034; l2 norm of gradients: 1.175909090876576; l2 norm of weights: 1.2109778987786024\n","Iteration #1681  Loss: 1.1351350130765943; l2 norm of gradients: 1.1746227614565226; l2 norm of weights: 1.209863398927755\n","Iteration #1682  Loss: 1.133751253666827; l2 norm of gradients: 1.1733377932608642; l2 norm of weights: 1.2087502460018855\n","Iteration #1683  Loss: 1.1323705046660422; l2 norm of gradients: 1.1720541850091675; l2 norm of weights: 1.2076384386756847\n","Iteration #1684  Loss: 1.1309927595937803; l2 norm of gradients: 1.1707719354218291; l2 norm of weights: 1.206527975625011\n","Iteration #1685  Loss: 1.129618011983057; l2 norm of gradients: 1.1694910432200765; l2 norm of weights: 1.2054188555268905\n","Iteration #1686  Loss: 1.1282462553803352; l2 norm of gradients: 1.168211507125966; l2 norm of weights: 1.204311077059516\n","Iteration #1687  Loss: 1.1268774833454989; l2 norm of gradients: 1.1669333258623844; l2 norm of weights: 1.2032046389022435\n","Iteration #1688  Loss: 1.1255116894518276; l2 norm of gradients: 1.1656564981530477; l2 norm of weights: 1.2020995397355936\n","Iteration #1689  Loss: 1.1241488672859719; l2 norm of gradients: 1.1643810227225015; l2 norm of weights: 1.20099577824125\n","Iteration #1690  Loss: 1.1227890104479228; l2 norm of gradients: 1.1631068982961206; l2 norm of weights: 1.1998933531020557\n","Iteration #1691  Loss: 1.1214321125509923; l2 norm of gradients: 1.161834123600108; l2 norm of weights: 1.1987922630020162\n","Iteration #1692  Loss: 1.120078167221781; l2 norm of gradients: 1.1605626973614962; l2 norm of weights: 1.1976925066262942\n","Iteration #1693  Loss: 1.118727168100157; l2 norm of gradients: 1.1592926183081464; l2 norm of weights: 1.1965940826612111\n","Iteration #1694  Loss: 1.1173791088392266; l2 norm of gradients: 1.1580238851687479; l2 norm of weights: 1.1954969897942442\n","Iteration #1695  Loss: 1.1160339831053119; l2 norm of gradients: 1.1567564966728179; l2 norm of weights: 1.1944012267140267\n","Iteration #1696  Loss: 1.114691784577922; l2 norm of gradients: 1.1554904515507025; l2 norm of weights: 1.193306792110346\n","Iteration #1697  Loss: 1.1133525069497299; l2 norm of gradients: 1.1542257485335754; l2 norm of weights: 1.1922136846741427\n","Iteration #1698  Loss: 1.1120161439265432; l2 norm of gradients: 1.152962386353438; l2 norm of weights: 1.1911219030975084\n","Iteration #1699  Loss: 1.1106826892272834; l2 norm of gradients: 1.151700363743119; l2 norm of weights: 1.190031446073687\n","Iteration #1700  Loss: 1.1093521365839563; l2 norm of gradients: 1.1504396794362746; l2 norm of weights: 1.1889423122970706\n","Iteration #1701  Loss: 1.1080244797416285; l2 norm of gradients: 1.149180332167389; l2 norm of weights: 1.1878545004632004\n","Iteration #1702  Loss: 1.1066997124584008; l2 norm of gradients: 1.1479223206717726; l2 norm of weights: 1.1867680092687645\n","Iteration #1703  Loss: 1.1053778285053841; l2 norm of gradients: 1.1466656436855636; l2 norm of weights: 1.185682837411597\n","Iteration #1704  Loss: 1.1040588216666727; l2 norm of gradients: 1.145410299945726; l2 norm of weights: 1.1845989835906776\n","Iteration #1705  Loss: 1.1027426857393197; l2 norm of gradients: 1.144156288190052; l2 norm of weights: 1.1835164465061287\n","Iteration #1706  Loss: 1.1014294145333112; l2 norm of gradients: 1.1429036071571586; l2 norm of weights: 1.1824352248592158\n","Iteration #1707  Loss: 1.1001190018715423; l2 norm of gradients: 1.14165225558649; l2 norm of weights: 1.181355317352346\n","Iteration #1708  Loss: 1.09881144158979; l2 norm of gradients: 1.1404022322183176; l2 norm of weights: 1.1802767226890662\n","Iteration #1709  Loss: 1.097506727536689; l2 norm of gradients: 1.1391535357937368; l2 norm of weights: 1.179199439574062\n","Iteration #1710  Loss: 1.0962048535737077; l2 norm of gradients: 1.1379061650546711; l2 norm of weights: 1.1781234667131582\n","Iteration #1711  Loss: 1.0949058135751206; l2 norm of gradients: 1.1366601187438685; l2 norm of weights: 1.1770488028133153\n","Iteration #1712  Loss: 1.0936096014279855; l2 norm of gradients: 1.1354153956049036; l2 norm of weights: 1.175975446582629\n","Iteration #1713  Loss: 1.0923162110321174; l2 norm of gradients: 1.1341719943821753; l2 norm of weights: 1.1749033967303308\n","Iteration #1714  Loss: 1.091025636300064; l2 norm of gradients: 1.13292991382091; l2 norm of weights: 1.173832651966784\n","Iteration #1715  Loss: 1.0897378711570807; l2 norm of gradients: 1.1316891526671573; l2 norm of weights: 1.172763211003485\n","Iteration #1716  Loss: 1.0884529095411053; l2 norm of gradients: 1.1304497096677935; l2 norm of weights: 1.1716950725530604\n","Iteration #1717  Loss: 1.0871707454027342; l2 norm of gradients: 1.1292115835705197; l2 norm of weights: 1.170628235329267\n","Iteration #1718  Loss: 1.0858913727051964; l2 norm of gradients: 1.1279747731238614; l2 norm of weights: 1.1695626980469902\n","Iteration #1719  Loss: 1.08461478542433; l2 norm of gradients: 1.1267392770771703; l2 norm of weights: 1.1684984594222423\n","Iteration #1720  Loss: 1.083340977548556; l2 norm of gradients: 1.1255050941806208; l2 norm of weights: 1.1674355181721625\n","Iteration #1721  Loss: 1.0820699430788558; l2 norm of gradients: 1.1242722231852142; l2 norm of weights: 1.1663738730150148\n","Iteration #1722  Loss: 1.0808016760287438; l2 norm of gradients: 1.1230406628427752; l2 norm of weights: 1.165313522670187\n","Iteration #1723  Loss: 1.0795361704242468; l2 norm of gradients: 1.1218104119059529; l2 norm of weights: 1.1642544658581904\n","Iteration #1724  Loss: 1.0782734203038742; l2 norm of gradients: 1.1205814691282212; l2 norm of weights: 1.163196701300657\n","Iteration #1725  Loss: 1.077013419718599; l2 norm of gradients: 1.1193538332638777; l2 norm of weights: 1.1621402277203399\n","Iteration #1726  Loss: 1.0757561627318288; l2 norm of gradients: 1.1181275030680446; l2 norm of weights: 1.161085043841111\n","Iteration #1727  Loss: 1.0745016434193855; l2 norm of gradients: 1.1169024772966683; l2 norm of weights: 1.1600311483879615\n","Iteration #1728  Loss: 1.0732498558694767; l2 norm of gradients: 1.115678754706519; l2 norm of weights: 1.158978540086998\n","Iteration #1729  Loss: 1.072000794182676; l2 norm of gradients: 1.1144563340551903; l2 norm of weights: 1.1579272176654443\n","Iteration #1730  Loss: 1.070754452471894; l2 norm of gradients: 1.1132352141011002; l2 norm of weights: 1.156877179851638\n","Iteration #1731  Loss: 1.0695108248623582; l2 norm of gradients: 1.1120153936034898; l2 norm of weights: 1.1558284253750308\n","Iteration #1732  Loss: 1.068269905491587; l2 norm of gradients: 1.1107968713224246; l2 norm of weights: 1.154780952966187\n","Iteration #1733  Loss: 1.0670316885093638; l2 norm of gradients: 1.109579646018793; l2 norm of weights: 1.153734761356781\n","Iteration #1734  Loss: 1.0657961680777173; l2 norm of gradients: 1.1083637164543072; l2 norm of weights: 1.1526898492795985\n","Iteration #1735  Loss: 1.0645633383708932; l2 norm of gradients: 1.1071490813915024; l2 norm of weights: 1.1516462154685334\n","Iteration #1736  Loss: 1.0633331935753336; l2 norm of gradients: 1.1059357395937368; l2 norm of weights: 1.1506038586585883\n","Iteration #1737  Loss: 1.0621057278896497; l2 norm of gradients: 1.104723689825193; l2 norm of weights: 1.1495627775858712\n","Iteration #1738  Loss: 1.060880935524601; l2 norm of gradients: 1.1035129308508755; l2 norm of weights: 1.1485229709875964\n","Iteration #1739  Loss: 1.0596588107030684; l2 norm of gradients: 1.1023034614366123; l2 norm of weights: 1.1474844376020823\n","Iteration #1740  Loss: 1.0584393476600344; l2 norm of gradients: 1.1010952803490546; l2 norm of weights: 1.1464471761687507\n","Iteration #1741  Loss: 1.0572225406425546; l2 norm of gradients: 1.0998883863556757; l2 norm of weights: 1.1454111854281248\n","Iteration #1742  Loss: 1.0560083839097374; l2 norm of gradients: 1.0986827782247726; l2 norm of weights: 1.1443764641218295\n","Iteration #1743  Loss: 1.054796871732719; l2 norm of gradients: 1.097478454725465; l2 norm of weights: 1.1433430109925888\n","Iteration #1744  Loss: 1.053587998394641; l2 norm of gradients: 1.0962754146276943; l2 norm of weights: 1.1423108247842264\n","Iteration #1745  Loss: 1.0523817581906232; l2 norm of gradients: 1.0950736567022257; l2 norm of weights: 1.1412799042416613\n","Iteration #1746  Loss: 1.051178145427745; l2 norm of gradients: 1.0938731797206458; l2 norm of weights: 1.140250248110911\n","Iteration #1747  Loss: 1.0499771544250183; l2 norm of gradients: 1.0926739824553653; l2 norm of weights: 1.139221855139087\n","Iteration #1748  Loss: 1.0487787795133656; l2 norm of gradients: 1.0914760636796155; l2 norm of weights: 1.1381947240743953\n","Iteration #1749  Loss: 1.0475830150355951; l2 norm of gradients: 1.0902794221674517; l2 norm of weights: 1.1371688536661342\n","Iteration #1750  Loss: 1.0463898553463793; l2 norm of gradients: 1.08908405669375; l2 norm of weights: 1.1361442426646942\n","Iteration #1751  Loss: 1.0451992948122306; l2 norm of gradients: 1.0878899660342098; l2 norm of weights: 1.1351208898215563\n","Iteration #1752  Loss: 1.044011327811477; l2 norm of gradients: 1.0866971489653525; l2 norm of weights: 1.134098793889291\n","Iteration #1753  Loss: 1.0428259487342402; l2 norm of gradients: 1.0855056042645213; l2 norm of weights: 1.1330779536215565\n","Iteration #1754  Loss: 1.0416431519824132; l2 norm of gradients: 1.0843153307098818; l2 norm of weights: 1.1320583677730993\n","Iteration #1755  Loss: 1.0404629319696335; l2 norm of gradients: 1.0831263270804217; l2 norm of weights: 1.1310400350997505\n","Iteration #1756  Loss: 1.0392852831212647; l2 norm of gradients: 1.0819385921559497; l2 norm of weights: 1.1300229543584275\n","Iteration #1757  Loss: 1.038110199874369; l2 norm of gradients: 1.0807521247170984; l2 norm of weights: 1.12900712430713\n","Iteration #1758  Loss: 1.0369376766776872; l2 norm of gradients: 1.0795669235453202; l2 norm of weights: 1.1279925437049416\n","Iteration #1759  Loss: 1.0357677079916137; l2 norm of gradients: 1.0783829874228907; l2 norm of weights: 1.126979211312026\n","Iteration #1760  Loss: 1.0346002882881757; l2 norm of gradients: 1.0772003151329068; l2 norm of weights: 1.1259671258896289\n","Iteration #1761  Loss: 1.0334354120510079; l2 norm of gradients: 1.0760189054592868; l2 norm of weights: 1.1249562862000737\n","Iteration #1762  Loss: 1.0322730737753305; l2 norm of gradients: 1.0748387571867712; l2 norm of weights: 1.1239466910067624\n","Iteration #1763  Loss: 1.0311132679679265; l2 norm of gradients: 1.0736598691009225; l2 norm of weights: 1.1229383390741736\n","Iteration #1764  Loss: 1.0299559891471186; l2 norm of gradients: 1.0724822399881238; l2 norm of weights: 1.1219312291678623\n","Iteration #1765  Loss: 1.0288012318427477; l2 norm of gradients: 1.0713058686355803; l2 norm of weights: 1.1209253600544575\n","Iteration #1766  Loss: 1.0276489905961481; l2 norm of gradients: 1.0701307538313187; l2 norm of weights: 1.119920730501662\n","Iteration #1767  Loss: 1.0264992599601253; l2 norm of gradients: 1.0689568943641874; l2 norm of weights: 1.118917339278251\n","Iteration #1768  Loss: 1.0253520344989342; l2 norm of gradients: 1.067784289023856; l2 norm of weights: 1.1179151851540703\n","Iteration #1769  Loss: 1.0242073087882564; l2 norm of gradients: 1.0666129366008157; l2 norm of weights: 1.116914266900037\n","Iteration #1770  Loss: 1.0230650774151764; l2 norm of gradients: 1.0654428358863786; l2 norm of weights: 1.1159145832881356\n","Iteration #1771  Loss: 1.0219253349781605; l2 norm of gradients: 1.0642739856726786; l2 norm of weights: 1.1149161330914197\n","Iteration #1772  Loss: 1.0207880760870334; l2 norm of gradients: 1.0631063847526707; l2 norm of weights: 1.113918915084009\n","Iteration #1773  Loss: 1.0196532953629558; l2 norm of gradients: 1.0619400319201313; l2 norm of weights: 1.1129229280410888\n","Iteration #1774  Loss: 1.0185209874384025; l2 norm of gradients: 1.0607749259696582; l2 norm of weights: 1.1119281707389088\n","Iteration #1775  Loss: 1.0173911469571393; l2 norm of gradients: 1.05961106569667; l2 norm of weights: 1.1109346419547823\n","Iteration #1776  Loss: 1.016263768574201; l2 norm of gradients: 1.058448449897407; l2 norm of weights: 1.1099423404670843\n","Iteration #1777  Loss: 1.0151388469558698; l2 norm of gradients: 1.0572870773689298; l2 norm of weights: 1.1089512650552513\n","Iteration #1778  Loss: 1.0140163767796504; l2 norm of gradients: 1.056126946909121; l2 norm of weights: 1.107961414499779\n","Iteration #1779  Loss: 1.012896352734252; l2 norm of gradients: 1.0549680573166837; l2 norm of weights: 1.1069727875822224\n","Iteration #1780  Loss: 1.0117787695195628; l2 norm of gradients: 1.0538104073911425; l2 norm of weights: 1.1059853830851945\n","Iteration #1781  Loss: 1.010663621846628; l2 norm of gradients: 1.0526539959328431; l2 norm of weights: 1.1049991997923636\n","Iteration #1782  Loss: 1.0095509044376298; l2 norm of gradients: 1.0514988217429513; l2 norm of weights: 1.1040142364884549\n","Iteration #1783  Loss: 1.0084406120258633; l2 norm of gradients: 1.050344883623455; l2 norm of weights: 1.1030304919592466\n","Iteration #1784  Loss: 1.0073327393557152; l2 norm of gradients: 1.0491921803771624; l2 norm of weights: 1.1020479649915706\n","Iteration #1785  Loss: 1.0062272811826423; l2 norm of gradients: 1.048040710807703; l2 norm of weights: 1.1010666543733112\n","Iteration #1786  Loss: 1.0051242322731484; l2 norm of gradients: 1.0468904737195266; l2 norm of weights: 1.1000865588934028\n","Iteration #1787  Loss: 1.0040235874047632; l2 norm of gradients: 1.0457414679179047; l2 norm of weights: 1.0991076773418296\n","Iteration #1788  Loss: 1.0029253413660204; l2 norm of gradients: 1.0445936922089292; l2 norm of weights: 1.0981300085096248\n","Iteration #1789  Loss: 1.0018294889564356; l2 norm of gradients: 1.0434471453995127; l2 norm of weights: 1.0971535511888693\n","Iteration #1790  Loss: 1.0007360249864852; l2 norm of gradients: 1.042301826297389; l2 norm of weights: 1.0961783041726902\n","Iteration #1791  Loss: 0.999644944277582; l2 norm of gradients: 1.0411577337111126; l2 norm of weights: 1.0952042662552592\n","Iteration #1792  Loss: 0.9985562416620584; l2 norm of gradients: 1.0400148664500588; l2 norm of weights: 1.0942314362317935\n","Iteration #1793  Loss: 0.9974699119831396; l2 norm of gradients: 1.0388732233244233; l2 norm of weights: 1.093259812898552\n","Iteration #1794  Loss: 0.9963859500949249; l2 norm of gradients: 1.037732803145223; l2 norm of weights: 1.0922893950528363\n","Iteration #1795  Loss: 0.9953043508623654; l2 norm of gradients: 1.0365936047242956; l2 norm of weights: 1.091320181492988\n","Iteration #1796  Loss: 0.9942251091612424; l2 norm of gradients: 1.0354556268742985; l2 norm of weights: 1.0903521710183897\n","Iteration #1797  Loss: 0.9931482198781462; l2 norm of gradients: 1.0343188684087112; l2 norm of weights: 1.0893853624294612\n","Iteration #1798  Loss: 0.9920736779104531; l2 norm of gradients: 1.033183328141833; l2 norm of weights: 1.0884197545276604\n","Iteration #1799  Loss: 0.9910014781663067; l2 norm of gradients: 1.0320490048887843; l2 norm of weights: 1.0874553461154814\n","Iteration #1800  Loss: 0.9899316155645933; l2 norm of gradients: 1.0309158974655055; l2 norm of weights: 1.0864921359964532\n","Iteration #1801  Loss: 0.9888640850349235; l2 norm of gradients: 1.0297840046887583; l2 norm of weights: 1.0855301229751395\n","Iteration #1802  Loss: 0.9877988815176089; l2 norm of gradients: 1.0286533253761247; l2 norm of weights: 1.0845693058571366\n","Iteration #1803  Loss: 0.9867359999636411; l2 norm of gradients: 1.0275238583460073; l2 norm of weights: 1.0836096834490725\n","Iteration #1804  Loss: 0.9856754353346706; l2 norm of gradients: 1.0263956024176293; l2 norm of weights: 1.0826512545586056\n","Iteration #1805  Loss: 0.984617182602987; l2 norm of gradients: 1.0252685564110344; l2 norm of weights: 1.0816940179944252\n","Iteration #1806  Loss: 0.9835612367514948; l2 norm of gradients: 1.024142719147087; l2 norm of weights: 1.0807379725662476\n","Iteration #1807  Loss: 0.9825075927736948; l2 norm of gradients: 1.0230180894474723; l2 norm of weights: 1.0797831170848173\n","Iteration #1808  Loss: 0.9814562456736629; l2 norm of gradients: 1.0218946661346948; l2 norm of weights: 1.0788294503619054\n","Iteration #1809  Loss: 0.980407190466026; l2 norm of gradients: 1.0207724480320814; l2 norm of weights: 1.0778769712103067\n","Iteration #1810  Loss: 0.9793604221759462; l2 norm of gradients: 1.0196514339637779; l2 norm of weights: 1.0769256784438423\n","Iteration #1811  Loss: 0.978315935839094; l2 norm of gradients: 1.0185316227547512; l2 norm of weights: 1.075975570877354\n","Iteration #1812  Loss: 0.9772737265016325; l2 norm of gradients: 1.0174130132307888; l2 norm of weights: 1.0750266473267067\n","Iteration #1813  Loss: 0.9762337892201924; l2 norm of gradients: 1.0162956042184985; l2 norm of weights: 1.074078906608786\n","Iteration #1814  Loss: 0.975196119061854; l2 norm of gradients: 1.0151793945453085; l2 norm of weights: 1.0731323475414962\n","Iteration #1815  Loss: 0.9741607111041246; l2 norm of gradients: 1.0140643830394676; l2 norm of weights: 1.072186968943761\n","Iteration #1816  Loss: 0.973127560434919; l2 norm of gradients: 1.0129505685300448; l2 norm of weights: 1.0712427696355218\n","Iteration #1817  Loss: 0.9720966621525371; l2 norm of gradients: 1.0118379498469297; l2 norm of weights: 1.070299748437735\n","Iteration #1818  Loss: 0.9710680113656445; l2 norm of gradients: 1.0107265258208322; l2 norm of weights: 1.0693579041723729\n","Iteration #1819  Loss: 0.9700416031932517; l2 norm of gradients: 1.0096162952832826; l2 norm of weights: 1.0684172356624222\n","Iteration #1820  Loss: 0.9690174327646928; l2 norm of gradients: 1.0085072570666318; l2 norm of weights: 1.0674777417318817\n","Iteration #1821  Loss: 0.9679954952196055; l2 norm of gradients: 1.0073994100040506; l2 norm of weights: 1.066539421205763\n","Iteration #1822  Loss: 0.9669757857079098; l2 norm of gradients: 1.006292752929531; l2 norm of weights: 1.065602272910088\n","Iteration #1823  Loss: 0.9659582993897884; l2 norm of gradients: 1.005187284677884; l2 norm of weights: 1.0646662956718882\n","Iteration #1824  Loss: 0.9649430314356651; l2 norm of gradients: 1.0040830040847426; l2 norm of weights: 1.0637314883192035\n","Iteration #1825  Loss: 0.9639299770261857; l2 norm of gradients: 1.0029799099865588; l2 norm of weights: 1.062797849681082\n","Iteration #1826  Loss: 0.9629191313521959; l2 norm of gradients: 1.0018780012206054; l2 norm of weights: 1.0618653785875771\n","Iteration #1827  Loss: 0.9619104896147223; l2 norm of gradients: 1.0007772766249758; l2 norm of weights: 1.0609340738697484\n","Iteration #1828  Loss: 0.960904047024951; l2 norm of gradients: 0.9996777350385831; l2 norm of weights: 1.0600039343596586\n","Iteration #1829  Loss: 0.959899798804208; l2 norm of gradients: 0.9985793753011613; l2 norm of weights: 1.0590749588903743\n","Iteration #1830  Loss: 0.9588977401839395; l2 norm of gradients: 0.9974821962532643; l2 norm of weights: 1.0581471462959642\n","Iteration #1831  Loss: 0.957897866405689; l2 norm of gradients: 0.9963861967362665; l2 norm of weights: 1.0572204954114968\n","Iteration #1832  Loss: 0.9569001727210806; l2 norm of gradients: 0.995291375592362; l2 norm of weights: 1.0562950050730413\n","Iteration #1833  Loss: 0.9559046543917963; l2 norm of gradients: 0.9941977316645662; l2 norm of weights: 1.0553706741176652\n","Iteration #1834  Loss: 0.9549113066895567; l2 norm of gradients: 0.9931052637967137; l2 norm of weights: 1.0544475013834333\n","Iteration #1835  Loss: 0.9539201248961011; l2 norm of gradients: 0.9920139708334601; l2 norm of weights: 1.0535254857094076\n","Iteration #1836  Loss: 0.9529311043031669; l2 norm of gradients: 0.9909238516202806; l2 norm of weights: 1.0526046259356445\n","Iteration #1837  Loss: 0.9519442402124698; l2 norm of gradients: 0.9898349050034714; l2 norm of weights: 1.0516849209031953\n","Iteration #1838  Loss: 0.9509595279356842; l2 norm of gradients: 0.9887471298301479; l2 norm of weights: 1.050766369454104\n","Iteration #1839  Loss: 0.9499769627944228; l2 norm of gradients: 0.9876605249482465; l2 norm of weights: 1.0498489704314076\n","Iteration #1840  Loss: 0.9489965401202162; l2 norm of gradients: 0.9865750892065236; l2 norm of weights: 1.0489327226791327\n","Iteration #1841  Loss: 0.9480182552544942; l2 norm of gradients: 0.9854908214545556; l2 norm of weights: 1.0480176250422968\n","Iteration #1842  Loss: 0.947042103548565; l2 norm of gradients: 0.984407720542739; l2 norm of weights: 1.047103676366906\n","Iteration #1843  Loss: 0.9460680803635961; l2 norm of gradients: 0.9833257853222909; l2 norm of weights: 1.0461908754999538\n","Iteration #1844  Loss: 0.9450961810705936; l2 norm of gradients: 0.9822450146452482; l2 norm of weights: 1.0452792212894206\n","Iteration #1845  Loss: 0.9441264010503826; l2 norm of gradients: 0.9811654073644678; l2 norm of weights: 1.0443687125842724\n","Iteration #1846  Loss: 0.9431587356935884; l2 norm of gradients: 0.9800869623336271; l2 norm of weights: 1.043459348234459\n","Iteration #1847  Loss: 0.942193180400617; l2 norm of gradients: 0.9790096784072235; l2 norm of weights: 1.0425511270909147\n","Iteration #1848  Loss: 0.9412297305816328; l2 norm of gradients: 0.9779335544405745; l2 norm of weights: 1.0416440480055544\n","Iteration #1849  Loss: 0.9402683816565427; l2 norm of gradients: 0.9768585892898175; l2 norm of weights: 1.040738109831276\n","Iteration #1850  Loss: 0.9393091290549733; l2 norm of gradients: 0.9757847818119103; l2 norm of weights: 1.039833311421956\n","Iteration #1851  Loss: 0.9383519682162543; l2 norm of gradients: 0.9747121308646305; l2 norm of weights: 1.0389296516324509\n","Iteration #1852  Loss: 0.9373968945893958; l2 norm of gradients: 0.9736406353065757; l2 norm of weights: 1.038027129318594\n","Iteration #1853  Loss: 0.9364439036330718; l2 norm of gradients: 0.9725702939971641; l2 norm of weights: 1.0371257433371968\n","Iteration #1854  Loss: 0.9354929908155993; l2 norm of gradients: 0.9715011057966334; l2 norm of weights: 1.0362254925460452\n","Iteration #1855  Loss: 0.9345441516149189; l2 norm of gradients: 0.9704330695660414; l2 norm of weights: 1.035326375803901\n","Iteration #1856  Loss: 0.933597381518575; l2 norm of gradients: 0.9693661841672662; l2 norm of weights: 1.0344283919704984\n","Iteration #1857  Loss: 0.9326526760236985; l2 norm of gradients: 0.9683004484630053; l2 norm of weights: 1.033531539906545\n","Iteration #1858  Loss: 0.9317100306369852; l2 norm of gradients: 0.9672358613167769; l2 norm of weights: 1.0326358184737192\n","Iteration #1859  Loss: 0.9307694408746772; l2 norm of gradients: 0.966172421592919; l2 norm of weights: 1.0317412265346697\n","Iteration #1860  Loss: 0.9298309022625451; l2 norm of gradients: 0.9651101281565891; l2 norm of weights: 1.030847762953015\n","Iteration #1861  Loss: 0.9288944103358667; l2 norm of gradients: 0.9640489798737654; l2 norm of weights: 1.0299554265933415\n","Iteration #1862  Loss: 0.9279599606394089; l2 norm of gradients: 0.9629889756112453; l2 norm of weights: 1.0290642163212023\n","Iteration #1863  Loss: 0.9270275487274091; l2 norm of gradients: 0.9619301142366465; l2 norm of weights: 1.028174131003117\n","Iteration #1864  Loss: 0.9260971701635552; l2 norm of gradients: 0.9608723946184065; l2 norm of weights: 1.0272851695065697\n","Iteration #1865  Loss: 0.9251688205209663; l2 norm of gradients: 0.9598158156257829; l2 norm of weights: 1.0263973307000085\n","Iteration #1866  Loss: 0.9242424953821754; l2 norm of gradients: 0.9587603761288529; l2 norm of weights: 1.0255106134528447\n","Iteration #1867  Loss: 0.9233181903391087; l2 norm of gradients: 0.9577060749985137; l2 norm of weights: 1.0246250166354507\n","Iteration #1868  Loss: 0.9223959009930669; l2 norm of gradients: 0.9566529111064824; l2 norm of weights: 1.0237405391191596\n","Iteration #1869  Loss: 0.9214756229547076; l2 norm of gradients: 0.9556008833252962; l2 norm of weights: 1.0228571797762647\n","Iteration #1870  Loss: 0.9205573518440247; l2 norm of gradients: 0.9545499905283112; l2 norm of weights: 1.0219749374800167\n","Iteration #1871  Loss: 0.9196410832903303; l2 norm of gradients: 0.9535002315897043; l2 norm of weights: 1.0210938111046244\n","Iteration #1872  Loss: 0.9187268129322366; l2 norm of gradients: 0.9524516053844716; l2 norm of weights: 1.0202137995252527\n","Iteration #1873  Loss: 0.9178145364176356; l2 norm of gradients: 0.9514041107884293; l2 norm of weights: 1.0193349016180215\n","Iteration #1874  Loss: 0.9169042494036825; l2 norm of gradients: 0.9503577466782132; l2 norm of weights: 1.018457116260006\n","Iteration #1875  Loss: 0.9159959475567745; l2 norm of gradients: 0.9493125119312789; l2 norm of weights: 1.0175804423292332\n","Iteration #1876  Loss: 0.9150896265525335; l2 norm of gradients: 0.9482684054259014; l2 norm of weights: 1.0167048787046822\n","Iteration #1877  Loss: 0.9141852820757886; l2 norm of gradients: 0.947225426041176; l2 norm of weights: 1.0158304242662843\n","Iteration #1878  Loss: 0.9132829098205544; l2 norm of gradients: 0.9461835726570171; l2 norm of weights: 1.0149570778949193\n","Iteration #1879  Loss: 0.9123825054900155; l2 norm of gradients: 0.9451428441541594; l2 norm of weights: 1.0140848384724166\n","Iteration #1880  Loss: 0.9114840647965066; l2 norm of gradients: 0.9441032394141564; l2 norm of weights: 1.013213704881553\n","Iteration #1881  Loss: 0.910587583461494; l2 norm of gradients: 0.943064757319382; l2 norm of weights: 1.0123436760060527\n","Iteration #1882  Loss: 0.9096930572155568; l2 norm of gradients: 0.9420273967530293; l2 norm of weights: 1.0114747507305841\n","Iteration #1883  Loss: 0.9088004817983703; l2 norm of gradients: 0.9409911565991109; l2 norm of weights: 1.0106069279407621\n","Iteration #1884  Loss: 0.9079098529586848; l2 norm of gradients: 0.9399560357424596; l2 norm of weights: 1.0097402065231436\n","Iteration #1885  Loss: 0.907021166454309; l2 norm of gradients: 0.9389220330687268; l2 norm of weights: 1.008874585365228\n","Iteration #1886  Loss: 0.9061344180520922; l2 norm of gradients: 0.9378891474643842; l2 norm of weights: 1.008010063355457\n","Iteration #1887  Loss: 0.9052496035279046; l2 norm of gradients: 0.9368573778167225; l2 norm of weights: 1.0071466393832118\n","Iteration #1888  Loss: 0.9043667186666197; l2 norm of gradients: 0.9358267230138523; l2 norm of weights: 1.0062843123388137\n","Iteration #1889  Loss: 0.9034857592620955; l2 norm of gradients: 0.9347971819447034; l2 norm of weights: 1.005423081113521\n","Iteration #1890  Loss: 0.9026067211171578; l2 norm of gradients: 0.9337687534990249; l2 norm of weights: 1.0045629445995299\n","Iteration #1891  Loss: 0.9017296000435802; l2 norm of gradients: 0.9327414365673857; l2 norm of weights: 1.0037039016899727\n","Iteration #1892  Loss: 0.900854391862067; l2 norm of gradients: 0.9317152300411737; l2 norm of weights: 1.0028459512789165\n","Iteration #1893  Loss: 0.8999810924022353; l2 norm of gradients: 0.9306901328125964; l2 norm of weights: 1.0019890922613623\n","Iteration #1894  Loss: 0.8991096975025963; l2 norm of gradients: 0.9296661437746807; l2 norm of weights: 1.0011333235332445\n","Iteration #1895  Loss: 0.8982402030105372; l2 norm of gradients: 0.9286432618212725; l2 norm of weights: 1.0002786439914286\n","Iteration #1896  Loss: 0.897372604782304; l2 norm of gradients: 0.9276214858470373; l2 norm of weights: 0.9994250525337112\n","Iteration #1897  Loss: 0.896506898682983; l2 norm of gradients: 0.9266008147474596; l2 norm of weights: 0.9985725480588188\n","Iteration #1898  Loss: 0.8956430805864823; l2 norm of gradients: 0.9255812474188432; l2 norm of weights: 0.9977211294664063\n","Iteration #1899  Loss: 0.894781146375516; l2 norm of gradients: 0.9245627827583114; l2 norm of weights: 0.9968707956570566\n","Iteration #1900  Loss: 0.8939210919415832; l2 norm of gradients: 0.9235454196638062; l2 norm of weights: 0.9960215455322787\n","Iteration #1901  Loss: 0.893062913184953; l2 norm of gradients: 0.9225291570340892; l2 norm of weights: 0.9951733779945076\n","Iteration #1902  Loss: 0.8922066060146452; l2 norm of gradients: 0.9215139937687407; l2 norm of weights: 0.9943262919471023\n","Iteration #1903  Loss: 0.8913521663484125; l2 norm of gradients: 0.9204999287681604; l2 norm of weights: 0.9934802862943455\n","Iteration #1904  Loss: 0.8904995901127241; l2 norm of gradients: 0.9194869609335673; l2 norm of weights: 0.9926353599414423\n","Iteration #1905  Loss: 0.8896488732427463; l2 norm of gradients: 0.9184750891669985; l2 norm of weights: 0.9917915117945191\n","Iteration #1906  Loss: 0.8888000116823253; l2 norm of gradients: 0.9174643123713112; l2 norm of weights: 0.990948740760622\n","Iteration #1907  Loss: 0.8879530013839712; l2 norm of gradients: 0.9164546294501807; l2 norm of weights: 0.9901070457477176\n","Iteration #1908  Loss: 0.8871078383088372; l2 norm of gradients: 0.9154460393081021; l2 norm of weights: 0.9892664256646891\n","Iteration #1909  Loss: 0.8862645184267061; l2 norm of gradients: 0.9144385408503887; l2 norm of weights: 0.9884268794213383\n","Iteration #1910  Loss: 0.8854230377159684; l2 norm of gradients: 0.9134321329831728; l2 norm of weights: 0.987588405928382\n","Iteration #1911  Loss: 0.8845833921636086; l2 norm of gradients: 0.9124268146134059; l2 norm of weights: 0.9867510040974529\n","Iteration #1912  Loss: 0.8837455777651849; l2 norm of gradients: 0.9114225846488578; l2 norm of weights: 0.9859146728410968\n","Iteration #1913  Loss: 0.8829095905248137; l2 norm of gradients: 0.9104194419981178; l2 norm of weights: 0.985079411072773\n","Iteration #1914  Loss: 0.882075426455151; l2 norm of gradients: 0.909417385570593; l2 norm of weights: 0.9842452177068528\n","Iteration #1915  Loss: 0.8812430815773757; l2 norm of gradients: 0.9084164142765101; l2 norm of weights: 0.9834120916586181\n","Iteration #1916  Loss: 0.8804125519211721; l2 norm of gradients: 0.9074165270269141; l2 norm of weights: 0.9825800318442608\n","Iteration #1917  Loss: 0.8795838335247121; l2 norm of gradients: 0.9064177227336684; l2 norm of weights: 0.9817490371808815\n","Iteration #1918  Loss: 0.878756922434639; l2 norm of gradients: 0.9054200003094554; l2 norm of weights: 0.9809191065864886\n","Iteration #1919  Loss: 0.8779318147060486; l2 norm of gradients: 0.9044233586677758; l2 norm of weights: 0.9800902389799973\n","Iteration #1920  Loss: 0.8771085064024738; l2 norm of gradients: 0.903427796722949; l2 norm of weights: 0.979262433281228\n","Iteration #1921  Loss: 0.8762869935958661; l2 norm of gradients: 0.902433313390113; l2 norm of weights: 0.9784356884109064\n","Iteration #1922  Loss: 0.8754672723665795; l2 norm of gradients: 0.9014399075852239; l2 norm of weights: 0.9776100032906617\n","Iteration #1923  Loss: 0.874649338803352; l2 norm of gradients: 0.9004475782250563; l2 norm of weights: 0.9767853768430254\n","Iteration #1924  Loss: 0.87383318900329; l2 norm of gradients: 0.8994563242272036; l2 norm of weights: 0.9759618079914305\n","Iteration #1925  Loss: 0.8730188190718504; l2 norm of gradients: 0.898466144510077; l2 norm of weights: 0.9751392956602108\n","Iteration #1926  Loss: 0.8722062251228232; l2 norm of gradients: 0.8974770379929063; l2 norm of weights: 0.9743178387745993\n","Iteration #1927  Loss: 0.8713954032783158; l2 norm of gradients: 0.8964890035957395; l2 norm of weights: 0.9734974362607277\n","Iteration #1928  Loss: 0.8705863496687349; l2 norm of gradients: 0.8955020402394426; l2 norm of weights: 0.972678087045625\n","Iteration #1929  Loss: 0.86977906043277; l2 norm of gradients: 0.8945161468457; l2 norm of weights: 0.9718597900572165\n","Iteration #1930  Loss: 0.8689735317173763; l2 norm of gradients: 0.8935313223370144; l2 norm of weights: 0.9710425442243227\n","Iteration #1931  Loss: 0.8681697596777584; l2 norm of gradients: 0.8925475656367062; l2 norm of weights: 0.9702263484766589\n","Iteration #1932  Loss: 0.8673677404773528; l2 norm of gradients: 0.8915648756689141; l2 norm of weights: 0.9694112017448334\n","Iteration #1933  Loss: 0.866567470287811; l2 norm of gradients: 0.8905832513585946; l2 norm of weights: 0.9685971029603467\n","Iteration #1934  Loss: 0.8657689452889836; l2 norm of gradients: 0.8896026916315224; l2 norm of weights: 0.967784051055591\n","Iteration #1935  Loss: 0.8649721616689032; l2 norm of gradients: 0.8886231954142899; l2 norm of weights: 0.966972044963848\n","Iteration #1936  Loss: 0.8641771156237664; l2 norm of gradients: 0.8876447616343074; l2 norm of weights: 0.966161083619289\n","Iteration #1937  Loss: 0.8633838033579194; l2 norm of gradients: 0.8866673892198031; l2 norm of weights: 0.9653511659569736\n","Iteration #1938  Loss: 0.8625922210838397; l2 norm of gradients: 0.8856910770998229; l2 norm of weights: 0.9645422909128483\n","Iteration #1939  Loss: 0.86180236502212; l2 norm of gradients: 0.8847158242042304; l2 norm of weights: 0.9637344574237459\n","Iteration #1940  Loss: 0.8610142314014517; l2 norm of gradients: 0.8837416294637072; l2 norm of weights: 0.9629276644273841\n","Iteration #1941  Loss: 0.8602278164586079; l2 norm of gradients: 0.8827684918097518; l2 norm of weights: 0.962121910862365\n","Iteration #1942  Loss: 0.8594431164384273; l2 norm of gradients: 0.8817964101746809; l2 norm of weights: 0.9613171956681735\n","Iteration #1943  Loss: 0.8586601275937982; l2 norm of gradients: 0.8808253834916288; l2 norm of weights: 0.9605135177851767\n","Iteration #1944  Loss: 0.8578788461856404; l2 norm of gradients: 0.8798554106945465; l2 norm of weights: 0.9597108761546225\n","Iteration #1945  Loss: 0.8570992684828902; l2 norm of gradients: 0.8788864907182036; l2 norm of weights: 0.9589092697186391\n","Iteration #1946  Loss: 0.8563213907624845; l2 norm of gradients: 0.8779186224981858; l2 norm of weights: 0.9581086974202339\n","Iteration #1947  Loss: 0.8555452093093423; l2 norm of gradients: 0.8769518049708971; l2 norm of weights: 0.9573091582032915\n","Iteration #1948  Loss: 0.8547707204163498; l2 norm of gradients: 0.875986037073558; l2 norm of weights: 0.9565106510125744\n","Iteration #1949  Loss: 0.8539979203843442; l2 norm of gradients: 0.8750213177442071; l2 norm of weights: 0.9557131747937202\n","Iteration #1950  Loss: 0.8532268055220971; l2 norm of gradients: 0.8740576459216993; l2 norm of weights: 0.9549167284932422\n","Iteration #1951  Loss: 0.852457372146298; l2 norm of gradients: 0.8730950205457073; l2 norm of weights: 0.9541213110585273\n","Iteration #1952  Loss: 0.8516896165815384; l2 norm of gradients: 0.8721334405567203; l2 norm of weights: 0.9533269214378352\n","Iteration #1953  Loss: 0.8509235351602957; l2 norm of gradients: 0.8711729048960449; l2 norm of weights: 0.9525335585802976\n","Iteration #1954  Loss: 0.8501591242229165; l2 norm of gradients: 0.8702134125058042; l2 norm of weights: 0.9517412214359174\n","Iteration #1955  Loss: 0.8493963801176015; l2 norm of gradients: 0.8692549623289386; l2 norm of weights: 0.9509499089555674\n","Iteration #1956  Loss: 0.8486352992003876; l2 norm of gradients: 0.8682975533092052; l2 norm of weights: 0.9501596200909885\n","Iteration #1957  Loss: 0.8478758778351339; l2 norm of gradients: 0.8673411843911778; l2 norm of weights: 0.9493703537947905\n","Iteration #1958  Loss: 0.8471181123935045; l2 norm of gradients: 0.8663858545202469; l2 norm of weights: 0.9485821090204494\n","Iteration #1959  Loss: 0.8463619992549527; l2 norm of gradients: 0.8654315626426198; l2 norm of weights: 0.9477948847223076\n","Iteration #1960  Loss: 0.8456075348067049; l2 norm of gradients: 0.86447830770532; l2 norm of weights: 0.9470086798555718\n","Iteration #1961  Loss: 0.844854715443745; l2 norm of gradients: 0.8635260886561882; l2 norm of weights: 0.9462234933763133\n","Iteration #1962  Loss: 0.8441035375687981; l2 norm of gradients: 0.8625749044438809; l2 norm of weights: 0.9454393242414657\n","Iteration #1963  Loss: 0.8433539975923146; l2 norm of gradients: 0.8616247540178713; l2 norm of weights: 0.9446561714088244\n","Iteration #1964  Loss: 0.8426060919324552; l2 norm of gradients: 0.8606756363284489; l2 norm of weights: 0.9438740338370464\n","Iteration #1965  Loss: 0.8418598170150735; l2 norm of gradients: 0.8597275503267195; l2 norm of weights: 0.9430929104856478\n","Iteration #1966  Loss: 0.8411151692737016; l2 norm of gradients: 0.8587804949646055; l2 norm of weights: 0.942312800315004\n","Iteration #1967  Loss: 0.8403721451495336; l2 norm of gradients: 0.8578344691948446; l2 norm of weights: 0.941533702286348\n","Iteration #1968  Loss: 0.8396307410914106; l2 norm of gradients: 0.8568894719709911; l2 norm of weights: 0.9407556153617702\n","Iteration #1969  Loss: 0.8388909535558033; l2 norm of gradients: 0.8559455022474155; l2 norm of weights: 0.9399785385042163\n","Iteration #1970  Loss: 0.8381527790067991; l2 norm of gradients: 0.8550025589793039; l2 norm of weights: 0.9392024706774873\n","Iteration #1971  Loss: 0.8374162139160832; l2 norm of gradients: 0.8540606411226582; l2 norm of weights: 0.9384274108462379\n","Iteration #1972  Loss: 0.8366812547629257; l2 norm of gradients: 0.8531197476342967; l2 norm of weights: 0.9376533579759759\n","Iteration #1973  Loss: 0.8359478980341641; l2 norm of gradients: 0.8521798774718529; l2 norm of weights: 0.9368803110330608\n","Iteration #1974  Loss: 0.8352161402241892; l2 norm of gradients: 0.8512410295937765; l2 norm of weights: 0.9361082689847032\n","Iteration #1975  Loss: 0.8344859778349288; l2 norm of gradients: 0.8503032029593319; l2 norm of weights: 0.9353372307989637\n","Iteration #1976  Loss: 0.8337574073758318; l2 norm of gradients: 0.8493663965286; l2 norm of weights: 0.9345671954447514\n","Iteration #1977  Loss: 0.8330304253638536; l2 norm of gradients: 0.8484306092624769; l2 norm of weights: 0.9337981618918241\n","Iteration #1978  Loss: 0.8323050283234403; l2 norm of gradients: 0.8474958401226739; l2 norm of weights: 0.933030129110786\n","Iteration #1979  Loss: 0.8315812127865132; l2 norm of gradients: 0.846562088071718; l2 norm of weights: 0.9322630960730878\n","Iteration #1980  Loss: 0.8308589752924531; l2 norm of gradients: 0.8456293520729508; l2 norm of weights: 0.9314970617510245\n","Iteration #1981  Loss: 0.830138312388086; l2 norm of gradients: 0.8446976310905296; l2 norm of weights: 0.930732025117736\n","Iteration #1982  Loss: 0.8294192206276663; l2 norm of gradients: 0.843766924089427; l2 norm of weights: 0.9299679851472045\n","Iteration #1983  Loss: 0.8287016965728627; l2 norm of gradients: 0.8428372300354303; l2 norm of weights: 0.9292049408142548\n","Iteration #1984  Loss: 0.8279857367927426; l2 norm of gradients: 0.8419085478951417; l2 norm of weights: 0.9284428910945524\n","Iteration #1985  Loss: 0.8272713378637557; l2 norm of gradients: 0.8409808766359784; l2 norm of weights: 0.927681834964603\n","Iteration #1986  Loss: 0.8265584963697217; l2 norm of gradients: 0.8400542152261726; l2 norm of weights: 0.9269217714017517\n","Iteration #1987  Loss: 0.8258472089018114; l2 norm of gradients: 0.839128562634771; l2 norm of weights: 0.9261626993841816\n","Iteration #1988  Loss: 0.8251374720585346; l2 norm of gradients: 0.8382039178316351; l2 norm of weights: 0.9254046178909129\n","Iteration #1989  Loss: 0.8244292824457229; l2 norm of gradients: 0.8372802797874409; l2 norm of weights: 0.9246475259018021\n","Iteration #1990  Loss: 0.8237226366765158; l2 norm of gradients: 0.8363576474736788; l2 norm of weights: 0.9238914223975407\n","Iteration #1991  Loss: 0.8230175313713453; l2 norm of gradients: 0.8354360198626541; l2 norm of weights: 0.9231363063596547\n","Iteration #1992  Loss: 0.8223139631579208; l2 norm of gradients: 0.8345153959274857; l2 norm of weights: 0.9223821767705035\n","Iteration #1993  Loss: 0.8216119286712137; l2 norm of gradients: 0.8335957746421073; l2 norm of weights: 0.9216290326132782\n","Iteration #1994  Loss: 0.8209114245534435; l2 norm of gradients: 0.8326771549812666; l2 norm of weights: 0.920876872872002\n","Iteration #1995  Loss: 0.8202124474540615; l2 norm of gradients: 0.8317595359205256; l2 norm of weights: 0.9201256965315281\n","Iteration #1996  Loss: 0.819514994029737; l2 norm of gradients: 0.8308429164362603; l2 norm of weights: 0.9193755025775391\n","Iteration #1997  Loss: 0.8188190609443418; l2 norm of gradients: 0.8299272955056602; l2 norm of weights: 0.9186262899965462\n","Iteration #1998  Loss: 0.8181246448689345; l2 norm of gradients: 0.8290126721067292; l2 norm of weights: 0.9178780577758875\n","Iteration #1999  Loss: 0.8174317424817485; l2 norm of gradients: 0.8280990452182847; l2 norm of weights: 0.9171308049037287\n","Iteration #2000  Loss: 0.8167403504681732; l2 norm of gradients: 0.8271864138199576; l2 norm of weights: 0.9163845303690598\n","Iteration #2001  Loss: 0.8160504655207428; l2 norm of gradients: 0.8262747768921926; l2 norm of weights: 0.9156392331616963\n","Iteration #2002  Loss: 0.815362084339119; l2 norm of gradients: 0.8253641334162484; l2 norm of weights: 0.9148949122722768\n","Iteration #2003  Loss: 0.814675203630078; l2 norm of gradients: 0.824454482374196; l2 norm of weights: 0.9141515666922629\n","Iteration #2004  Loss: 0.8139898201074944; l2 norm of gradients: 0.823545822748921; l2 norm of weights: 0.9134091954139377\n","Iteration #2005  Loss: 0.8133059304923278; l2 norm of gradients: 0.8226381535241212; l2 norm of weights: 0.9126677974304049\n","Iteration #2006  Loss: 0.8126235315126071; l2 norm of gradients: 0.8217314736843082; l2 norm of weights: 0.9119273717355884\n","Iteration #2007  Loss: 0.8119426199034163; l2 norm of gradients: 0.8208257822148066; l2 norm of weights: 0.9111879173242304\n","Iteration #2008  Loss: 0.81126319240688; l2 norm of gradients: 0.8199210781017535; l2 norm of weights: 0.9104494331918912\n","Iteration #2009  Loss: 0.8105852457721492; l2 norm of gradients: 0.8190173603320995; l2 norm of weights: 0.9097119183349485\n","Iteration #2010  Loss: 0.8099087767553853; l2 norm of gradients: 0.8181146278936074; l2 norm of weights: 0.9089753717505948\n","Iteration #2011  Loss: 0.8092337821197475; l2 norm of gradients: 0.8172128797748535; l2 norm of weights: 0.9082397924368387\n","Iteration #2012  Loss: 0.8085602586353766; l2 norm of gradients: 0.8163121149652259; l2 norm of weights: 0.9075051793925024\n","Iteration #2013  Loss: 0.8078882030793821; l2 norm of gradients: 0.8154123324549257; l2 norm of weights: 0.9067715316172212\n","Iteration #2014  Loss: 0.8072176122358266; l2 norm of gradients: 0.8145135312349663; l2 norm of weights: 0.9060388481114425\n","Iteration #2015  Loss: 0.8065484828957117; l2 norm of gradients: 0.8136157102971733; l2 norm of weights: 0.9053071278764251\n","Iteration #2016  Loss: 0.8058808118569638; l2 norm of gradients: 0.8127188686341847; l2 norm of weights: 0.9045763699142378\n","Iteration #2017  Loss: 0.8052145959244197; l2 norm of gradients: 0.8118230052394506; l2 norm of weights: 0.903846573227759\n","Iteration #2018  Loss: 0.8045498319098128; l2 norm of gradients: 0.8109281191072332; l2 norm of weights: 0.9031177368206754\n","Iteration #2019  Loss: 0.8038865166317573; l2 norm of gradients: 0.8100342092326065; l2 norm of weights: 0.902389859697481\n","Iteration #2020  Loss: 0.8032246469157349; l2 norm of gradients: 0.8091412746114564; l2 norm of weights: 0.9016629408634759\n","Iteration #2021  Loss: 0.8025642195940816; l2 norm of gradients: 0.8082493142404804; l2 norm of weights: 0.9009369793247667\n","Iteration #2022  Loss: 0.8019052315059716; l2 norm of gradients: 0.8073583271171879; l2 norm of weights: 0.9002119740882637\n","Iteration #2023  Loss: 0.8012476794974038; l2 norm of gradients: 0.8064683122398999; l2 norm of weights: 0.8994879241616813\n","Iteration #2024  Loss: 0.8005915604211885; l2 norm of gradients: 0.8055792686077486; l2 norm of weights: 0.8987648285535366\n","Iteration #2025  Loss: 0.799936871136932; l2 norm of gradients: 0.8046911952206777; l2 norm of weights: 0.8980426862731482\n","Iteration #2026  Loss: 0.7992836085110232; l2 norm of gradients: 0.8038040910794418; l2 norm of weights: 0.8973214963306358\n","Iteration #2027  Loss: 0.7986317694166192; l2 norm of gradients: 0.8029179551856072; l2 norm of weights: 0.896601257736919\n","Iteration #2028  Loss: 0.7979813507336319; l2 norm of gradients: 0.8020327865415505; l2 norm of weights: 0.8958819695037163\n","Iteration #2029  Loss: 0.7973323493487128; l2 norm of gradients: 0.8011485841504601; l2 norm of weights: 0.8951636306435442\n","Iteration #2030  Loss: 0.7966847621552404; l2 norm of gradients: 0.8002653470163346; l2 norm of weights: 0.8944462401697164\n","Iteration #2031  Loss: 0.796038586053305; l2 norm of gradients: 0.7993830741439834; l2 norm of weights: 0.8937297970963427\n","Iteration #2032  Loss: 0.7953938179496955; l2 norm of gradients: 0.7985017645390268; l2 norm of weights: 0.893014300438328\n","Iteration #2033  Loss: 0.7947504547578853; l2 norm of gradients: 0.7976214172078953; l2 norm of weights: 0.892299749211372\n","Iteration #2034  Loss: 0.7941084933980183; l2 norm of gradients: 0.7967420311578298; l2 norm of weights: 0.8915861424319673\n","Iteration #2035  Loss: 0.7934679307968947; l2 norm of gradients: 0.7958636053968818; l2 norm of weights: 0.8908734791173991\n","Iteration #2036  Loss: 0.7928287638879583; l2 norm of gradients: 0.7949861389339127; l2 norm of weights: 0.8901617582857444\n","Iteration #2037  Loss: 0.7921909896112816; l2 norm of gradients: 0.7941096307785936; l2 norm of weights: 0.8894509789558704\n","Iteration #2038  Loss: 0.7915546049135521; l2 norm of gradients: 0.7932340799414066; l2 norm of weights: 0.8887411401474342\n","Iteration #2039  Loss: 0.7909196067480595; l2 norm of gradients: 0.7923594854336427; l2 norm of weights: 0.888032240880882\n","Iteration #2040  Loss: 0.7902859920746804; l2 norm of gradients: 0.7914858462674029; l2 norm of weights: 0.8873242801774469\n","Iteration #2041  Loss: 0.7896537578598662; l2 norm of gradients: 0.7906131614555979; l2 norm of weights: 0.8866172570591501\n","Iteration #2042  Loss: 0.7890229010766285; l2 norm of gradients: 0.7897414300119481; l2 norm of weights: 0.8859111705487981\n","Iteration #2043  Loss: 0.7883934187045253; l2 norm of gradients: 0.7888706509509825; l2 norm of weights: 0.8852060196699826\n","Iteration #2044  Loss: 0.7877653077296478; l2 norm of gradients: 0.7880008232880406; l2 norm of weights: 0.8845018034470794\n","Iteration #2045  Loss: 0.787138565144607; l2 norm of gradients: 0.7871319460392697; l2 norm of weights: 0.8837985209052479\n","Iteration #2046  Loss: 0.7865131879485199; l2 norm of gradients: 0.7862640182216275; l2 norm of weights: 0.8830961710704296\n","Iteration #2047  Loss: 0.7858891731469952; l2 norm of gradients: 0.7853970388528796; l2 norm of weights: 0.8823947529693476\n","Iteration #2048  Loss: 0.7852665177521212; l2 norm of gradients: 0.7845310069516007; l2 norm of weights: 0.8816942656295051\n","Iteration #2049  Loss: 0.7846452187824511; l2 norm of gradients: 0.7836659215371742; l2 norm of weights: 0.8809947080791856\n","Iteration #2050  Loss: 0.7840252732629902; l2 norm of gradients: 0.7828017816297924; l2 norm of weights: 0.8802960793474507\n","Iteration #2051  Loss: 0.7834066782251824; l2 norm of gradients: 0.7819385862504558; l2 norm of weights: 0.8795983784641404\n","Iteration #2052  Loss: 0.7827894307068959; l2 norm of gradients: 0.7810763344209726; l2 norm of weights: 0.8789016044598706\n","Iteration #2053  Loss: 0.7821735277524111; l2 norm of gradients: 0.7802150251639604; l2 norm of weights: 0.8782057563660344\n","Iteration #2054  Loss: 0.781558966412407; l2 norm of gradients: 0.7793546575028438; l2 norm of weights: 0.8775108332147992\n","Iteration #2055  Loss: 0.7809457437439467; l2 norm of gradients: 0.7784952304618563; l2 norm of weights: 0.8768168340391068\n","Iteration #2056  Loss: 0.7803338568104652; l2 norm of gradients: 0.7776367430660385; l2 norm of weights: 0.8761237578726724\n","Iteration #2057  Loss: 0.7797233026817562; l2 norm of gradients: 0.7767791943412389; l2 norm of weights: 0.8754316037499834\n","Iteration #2058  Loss: 0.7791140784339576; l2 norm of gradients: 0.7759225833141137; l2 norm of weights: 0.8747403707062986\n","Iteration #2059  Loss: 0.7785061811495401; l2 norm of gradients: 0.7750669090121266; l2 norm of weights: 0.8740500577776477\n","Iteration #2060  Loss: 0.7778996079172923; l2 norm of gradients: 0.7742121704635485; l2 norm of weights: 0.87336066400083\n","Iteration #2061  Loss: 0.7772943558323084; l2 norm of gradients: 0.7733583666974578; l2 norm of weights: 0.8726721884134134\n","Iteration #2062  Loss: 0.7766904219959745; l2 norm of gradients: 0.7725054967437393; l2 norm of weights: 0.8719846300537337\n","Iteration #2063  Loss: 0.7760878035159565; l2 norm of gradients: 0.7716535596330855; l2 norm of weights: 0.8712979879608941\n","Iteration #2064  Loss: 0.7754864975061856; l2 norm of gradients: 0.7708025543969952; l2 norm of weights: 0.8706122611747634\n","Iteration #2065  Loss: 0.7748865010868461; l2 norm of gradients: 0.7699524800677743; l2 norm of weights: 0.8699274487359758\n","Iteration #2066  Loss: 0.7742878113843623; l2 norm of gradients: 0.7691033356785347; l2 norm of weights: 0.8692435496859301\n","Iteration #2067  Loss: 0.7736904255313852; l2 norm of gradients: 0.7682551202631953; l2 norm of weights: 0.8685605630667884\n","Iteration #2068  Loss: 0.773094340666779; l2 norm of gradients: 0.7674078328564812; l2 norm of weights: 0.8678784879214746\n","Iteration #2069  Loss: 0.7724995539356099; l2 norm of gradients: 0.7665614724939233; l2 norm of weights: 0.867197323293676\n","Iteration #2070  Loss: 0.7719060624891305; l2 norm of gradients: 0.7657160382118586; l2 norm of weights: 0.8665170682278387\n","Iteration #2071  Loss: 0.7713138634847696; l2 norm of gradients: 0.7648715290474304; l2 norm of weights: 0.8658377217691704\n","Iteration #2072  Loss: 0.7707229540861167; l2 norm of gradients: 0.7640279440385873; l2 norm of weights: 0.8651592829636365\n","Iteration #2073  Loss: 0.7701333314629115; l2 norm of gradients: 0.7631852822240838; l2 norm of weights: 0.8644817508579613\n","Iteration #2074  Loss: 0.7695449927910296; l2 norm of gradients: 0.7623435426434799; l2 norm of weights: 0.8638051244996265\n","Iteration #2075  Loss: 0.7689579352524694; l2 norm of gradients: 0.7615027243371404; l2 norm of weights: 0.8631294029368696\n","Iteration #2076  Loss: 0.7683721560353405; l2 norm of gradients: 0.7606628263462359; l2 norm of weights: 0.8624545852186836\n","Iteration #2077  Loss: 0.76778765233385; l2 norm of gradients: 0.759823847712742; l2 norm of weights: 0.8617806703948169\n","Iteration #2078  Loss: 0.76720442134829; l2 norm of gradients: 0.7589857874794391; l2 norm of weights: 0.8611076575157708\n","Iteration #2079  Loss: 0.7666224602850251; l2 norm of gradients: 0.7581486446899122; l2 norm of weights: 0.8604355456328\n","Iteration #2080  Loss: 0.7660417663564794; l2 norm of gradients: 0.7573124183885508; l2 norm of weights: 0.8597643337979111\n","Iteration #2081  Loss: 0.7654623367811235; l2 norm of gradients: 0.75647710762055; l2 norm of weights: 0.8590940210638618\n","Iteration #2082  Loss: 0.7648841687834629; l2 norm of gradients: 0.7556427114319079; l2 norm of weights: 0.8584246064841601\n","Iteration #2083  Loss: 0.7643072595940239; l2 norm of gradients: 0.7548092288694273; l2 norm of weights: 0.8577560891130631\n","Iteration #2084  Loss: 0.7637316064493422; l2 norm of gradients: 0.7539766589807153; l2 norm of weights: 0.8570884680055773\n","Iteration #2085  Loss: 0.76315720659195; l2 norm of gradients: 0.7531450008141827; l2 norm of weights: 0.8564217422174558\n","Iteration #2086  Loss: 0.7625840572703626; l2 norm of gradients: 0.7523142534190443; l2 norm of weights: 0.855755910805199\n","Iteration #2087  Loss: 0.7620121557390676; l2 norm of gradients: 0.7514844158453181; l2 norm of weights: 0.8550909728260538\n","Iteration #2088  Loss: 0.76144149925851; l2 norm of gradients: 0.7506554871438256; l2 norm of weights: 0.8544269273380112\n","Iteration #2089  Loss: 0.7608720850950823; l2 norm of gradients: 0.749827466366192; l2 norm of weights: 0.8537637733998068\n","Iteration #2090  Loss: 0.7603039105211102; l2 norm of gradients: 0.7490003525648459; l2 norm of weights: 0.85310151007092\n","Iteration #2091  Loss: 0.7597369728148405; l2 norm of gradients: 0.7481741447930178; l2 norm of weights: 0.8524401364115722\n","Iteration #2092  Loss: 0.7591712692604293; l2 norm of gradients: 0.7473488421047424; l2 norm of weights: 0.8517796514827266\n","Iteration #2093  Loss: 0.7586067971479291; l2 norm of gradients: 0.7465244435548561; l2 norm of weights: 0.8511200543460873\n","Iteration #2094  Loss: 0.7580435537732764; l2 norm of gradients: 0.7457009481989985; l2 norm of weights: 0.8504613440640982\n","Iteration #2095  Loss: 0.7574815364382792; l2 norm of gradients: 0.7448783550936111; l2 norm of weights: 0.8498035196999423\n","Iteration #2096  Loss: 0.7569207424506058; l2 norm of gradients: 0.744056663295938; l2 norm of weights: 0.849146580317541\n","Iteration #2097  Loss: 0.7563611691237708; l2 norm of gradients: 0.7432358718640257; l2 norm of weights: 0.848490524981553\n","Iteration #2098  Loss: 0.7558028137771244; l2 norm of gradients: 0.7424159798567223; l2 norm of weights: 0.8478353527573735\n","Iteration #2099  Loss: 0.7552456737358386; l2 norm of gradients: 0.7415969863336771; l2 norm of weights: 0.8471810627111336\n","Iteration #2100  Loss: 0.7546897463308964; l2 norm of gradients: 0.740778890355342; l2 norm of weights: 0.8465276539096988\n","Iteration #2101  Loss: 0.7541350288990787; l2 norm of gradients: 0.7399616909829698; l2 norm of weights: 0.8458751254206692\n","Iteration #2102  Loss: 0.7535815187829527; l2 norm of gradients: 0.7391453872786149; l2 norm of weights: 0.8452234763123777\n","Iteration #2103  Loss: 0.7530292133308586; l2 norm of gradients: 0.7383299783051329; l2 norm of weights: 0.8445727056538894\n","Iteration #2104  Loss: 0.7524781098968997; l2 norm of gradients: 0.7375154631261799; l2 norm of weights: 0.8439228125150016\n","Iteration #2105  Loss: 0.7519282058409278; l2 norm of gradients: 0.7367018408062134; l2 norm of weights: 0.8432737959662413\n","Iteration #2106  Loss: 0.7513794985285325; l2 norm of gradients: 0.7358891104104912; l2 norm of weights: 0.842625655078866\n","Iteration #2107  Loss: 0.7508319853310291; l2 norm of gradients: 0.7350772710050718; l2 norm of weights: 0.8419783889248621\n","Iteration #2108  Loss: 0.7502856636254457; l2 norm of gradients: 0.7342663216568139; l2 norm of weights: 0.8413319965769435\n","Iteration #2109  Loss: 0.7497405307945124; l2 norm of gradients: 0.7334562614333764; l2 norm of weights: 0.8406864771085522\n","Iteration #2110  Loss: 0.7491965842266484; l2 norm of gradients: 0.7326470894032183; l2 norm of weights: 0.8400418295938563\n","Iteration #2111  Loss: 0.7486538213159504; l2 norm of gradients: 0.7318388046355983; l2 norm of weights: 0.8393980531077498\n","Iteration #2112  Loss: 0.7481122394621804; l2 norm of gradients: 0.7310314062005748; l2 norm of weights: 0.8387551467258513\n","Iteration #2113  Loss: 0.7475718360707542; l2 norm of gradients: 0.730224893169006; l2 norm of weights: 0.8381131095245031\n","Iteration #2114  Loss: 0.7470326085527292; l2 norm of gradients: 0.7294192646125489; l2 norm of weights: 0.8374719405807715\n","Iteration #2115  Loss: 0.7464945543247924; l2 norm of gradients: 0.7286145196036601; l2 norm of weights: 0.8368316389724443\n","Iteration #2116  Loss: 0.745957670809249; l2 norm of gradients: 0.7278106572155949; l2 norm of weights: 0.8361922037780313\n","Iteration #2117  Loss: 0.7454219554340098; l2 norm of gradients: 0.7270076765224076; l2 norm of weights: 0.8355536340767629\n","Iteration #2118  Loss: 0.7448874056325805; l2 norm of gradients: 0.726205576598951; l2 norm of weights: 0.8349159289485891\n","Iteration #2119  Loss: 0.7443540188440487; l2 norm of gradients: 0.7254043565208768; l2 norm of weights: 0.8342790874741792\n","Iteration #2120  Loss: 0.7438217925130733; l2 norm of gradients: 0.7246040153646343; l2 norm of weights: 0.8336431087349211\n","Iteration #2121  Loss: 0.7432907240898721; l2 norm of gradients: 0.7238045522074711; l2 norm of weights: 0.8330079918129195\n","Iteration #2122  Loss: 0.7427608110302095; l2 norm of gradients: 0.7230059661274333; l2 norm of weights: 0.8323737357909959\n","Iteration #2123  Loss: 0.7422320507953863; l2 norm of gradients: 0.7222082562033644; l2 norm of weights: 0.8317403397526879\n","Iteration #2124  Loss: 0.7417044408522271; l2 norm of gradients: 0.7214114215149051; l2 norm of weights: 0.8311078027822477\n","Iteration #2125  Loss: 0.7411779786730679; l2 norm of gradients: 0.7206154611424943; l2 norm of weights: 0.8304761239646415\n","Iteration #2126  Loss: 0.7406526617357467; l2 norm of gradients: 0.7198203741673673; l2 norm of weights: 0.8298453023855497\n","Iteration #2127  Loss: 0.7401284875235895; l2 norm of gradients: 0.7190261596715571; l2 norm of weights: 0.8292153371313643\n","Iteration #2128  Loss: 0.7396054535254007; l2 norm of gradients: 0.7182328167378934; l2 norm of weights: 0.8285862272891901\n","Iteration #2129  Loss: 0.7390835572354492; l2 norm of gradients: 0.7174403444500025; l2 norm of weights: 0.8279579719468412\n","Iteration #2130  Loss: 0.73856279615346; l2 norm of gradients: 0.7166487418923068; l2 norm of weights: 0.8273305701928438\n","Iteration #2131  Loss: 0.7380431677845996; l2 norm of gradients: 0.7158580081500258; l2 norm of weights: 0.8267040211164322\n","Iteration #2132  Loss: 0.7375246696394666; l2 norm of gradients: 0.7150681423091747; l2 norm of weights: 0.8260783238075494\n","Iteration #2133  Loss: 0.7370072992340788; l2 norm of gradients: 0.7142791434565647; l2 norm of weights: 0.8254534773568462\n","Iteration #2134  Loss: 0.736491054089864; l2 norm of gradients: 0.7134910106798027; l2 norm of weights: 0.8248294808556809\n","Iteration #2135  Loss: 0.7359759317336455; l2 norm of gradients: 0.7127037430672912; l2 norm of weights: 0.8242063333961173\n","Iteration #2136  Loss: 0.7354619296976326; l2 norm of gradients: 0.7119173397082283; l2 norm of weights: 0.8235840340709243\n","Iteration #2137  Loss: 0.7349490455194094; l2 norm of gradients: 0.7111317996926069; l2 norm of weights: 0.8229625819735764\n","Iteration #2138  Loss: 0.7344372767419225; l2 norm of gradients: 0.7103471221112152; l2 norm of weights: 0.8223419761982509\n","Iteration #2139  Loss: 0.7339266209134702; l2 norm of gradients: 0.709563306055636; l2 norm of weights: 0.8217222158398284\n","Iteration #2140  Loss: 0.733417075587691; l2 norm of gradients: 0.7087803506182471; l2 norm of weights: 0.8211032999938918\n","Iteration #2141  Loss: 0.7329086383235526; l2 norm of gradients: 0.70799825489222; l2 norm of weights: 0.8204852277567253\n","Iteration #2142  Loss: 0.7324013066853405; l2 norm of gradients: 0.7072170179715214; l2 norm of weights: 0.819867998225314\n","Iteration #2143  Loss: 0.7318950782426465; l2 norm of gradients: 0.7064366389509112; l2 norm of weights: 0.8192516104973423\n","Iteration #2144  Loss: 0.7313899505703576; l2 norm of gradients: 0.7056571169259436; l2 norm of weights: 0.8186360636711939\n","Iteration #2145  Loss: 0.7308859212486449; l2 norm of gradients: 0.7048784509929663; l2 norm of weights: 0.8180213568459509\n","Iteration #2146  Loss: 0.7303829878629527; l2 norm of gradients: 0.7041006402491204; l2 norm of weights: 0.8174074891213927\n","Iteration #2147  Loss: 0.7298811480039871; l2 norm of gradients: 0.7033236837923403; l2 norm of weights: 0.8167944595979957\n","Iteration #2148  Loss: 0.7293803992677041; l2 norm of gradients: 0.7025475807213536; l2 norm of weights: 0.8161822673769317\n","Iteration #2149  Loss: 0.7288807392552998; l2 norm of gradients: 0.7017723301356804; l2 norm of weights: 0.8155709115600681\n","Iteration #2150  Loss: 0.7283821655731988; l2 norm of gradients: 0.7009979311356338; l2 norm of weights: 0.8149603912499667\n","Iteration #2151  Loss: 0.7278846758330428; l2 norm of gradients: 0.700224382822319; l2 norm of weights: 0.8143507055498825\n","Iteration #2152  Loss: 0.72738826765168; l2 norm of gradients: 0.6994516842976339; l2 norm of weights: 0.8137418535637638\n","Iteration #2153  Loss: 0.7268929386511538; l2 norm of gradients: 0.698679834664268; l2 norm of weights: 0.8131338343962511\n","Iteration #2154  Loss: 0.7263986864586915; l2 norm of gradients: 0.6979088330257027; l2 norm of weights: 0.8125266471526753\n","Iteration #2155  Loss: 0.7259055087066946; l2 norm of gradients: 0.6971386784862114; l2 norm of weights: 0.8119202909390587\n","Iteration #2156  Loss: 0.7254134030327257; l2 norm of gradients: 0.6963693701508583; l2 norm of weights: 0.811314764862113\n","Iteration #2157  Loss: 0.7249223670795006; l2 norm of gradients: 0.6956009071254994; l2 norm of weights: 0.8107100680292392\n","Iteration #2158  Loss: 0.724432398494874; l2 norm of gradients: 0.6948332885167814; l2 norm of weights: 0.810106199548526\n","Iteration #2159  Loss: 0.7239434949318306; l2 norm of gradients: 0.6940665134321419; l2 norm of weights: 0.8095031585287505\n","Iteration #2160  Loss: 0.7234556540484747; l2 norm of gradients: 0.6933005809798091; l2 norm of weights: 0.8089009440793757\n","Iteration #2161  Loss: 0.7229688735080171; l2 norm of gradients: 0.6925354902688011; l2 norm of weights: 0.8082995553105506\n","Iteration #2162  Loss: 0.7224831509787668; l2 norm of gradients: 0.691771240408927; l2 norm of weights: 0.8076989913331101\n","Iteration #2163  Loss: 0.7219984841341187; l2 norm of gradients: 0.6910078305107854; l2 norm of weights: 0.807099251258573\n","Iteration #2164  Loss: 0.7215148706525432; l2 norm of gradients: 0.6902452596857643; l2 norm of weights: 0.8065003341991418\n","Iteration #2165  Loss: 0.7210323082175751; l2 norm of gradients: 0.689483527046042; l2 norm of weights: 0.8059022392677022\n","Iteration #2166  Loss: 0.7205507945178036; l2 norm of gradients: 0.6887226317045853; l2 norm of weights: 0.8053049655778219\n","Iteration #2167  Loss: 0.7200703272468612; l2 norm of gradients: 0.6879625727751506; l2 norm of weights: 0.8047085122437506\n","Iteration #2168  Loss: 0.7195909041034123; l2 norm of gradients: 0.6872033493722831; l2 norm of weights: 0.8041128783804178\n","Iteration #2169  Loss: 0.7191125227911439; l2 norm of gradients: 0.6864449606113163; l2 norm of weights: 0.8035180631034337\n","Iteration #2170  Loss: 0.7186351810187539; l2 norm of gradients: 0.6856874056083726; l2 norm of weights: 0.8029240655290873\n","Iteration #2171  Loss: 0.7181588764999409; l2 norm of gradients: 0.6849306834803623; l2 norm of weights: 0.8023308847743467\n","Iteration #2172  Loss: 0.7176836069533932; l2 norm of gradients: 0.6841747933449838; l2 norm of weights: 0.801738519956857\n","Iteration #2173  Loss: 0.7172093701027789; l2 norm of gradients: 0.6834197343207233; l2 norm of weights: 0.8011469701949406\n","Iteration #2174  Loss: 0.7167361636767344; l2 norm of gradients: 0.6826655055268543; l2 norm of weights: 0.8005562346075966\n","Iteration #2175  Loss: 0.7162639854088549; l2 norm of gradients: 0.681912106083438; l2 norm of weights: 0.7999663123144989\n","Iteration #2176  Loss: 0.7157928330376829; l2 norm of gradients: 0.6811595351113222; l2 norm of weights: 0.7993772024359966\n","Iteration #2177  Loss: 0.7153227043066982; l2 norm of gradients: 0.680407791732142; l2 norm of weights: 0.798788904093113\n","Iteration #2178  Loss: 0.7148535969643075; l2 norm of gradients: 0.6796568750683192; l2 norm of weights: 0.7982014164075446\n","Iteration #2179  Loss: 0.7143855087638336; l2 norm of gradients: 0.6789067842430613; l2 norm of weights: 0.7976147385016603\n","Iteration #2180  Loss: 0.7139184374635048; l2 norm of gradients: 0.6781575183803628; l2 norm of weights: 0.7970288694985012\n","Iteration #2181  Loss: 0.7134523808264455; l2 norm of gradients: 0.6774090766050035; l2 norm of weights: 0.7964438085217793\n","Iteration #2182  Loss: 0.7129873366206645; l2 norm of gradients: 0.6766614580425494; l2 norm of weights: 0.7958595546958773\n","Iteration #2183  Loss: 0.7125233026190454; l2 norm of gradients: 0.6759146618193519; l2 norm of weights: 0.7952761071458476\n","Iteration #2184  Loss: 0.712060276599336; l2 norm of gradients: 0.6751686870625474; l2 norm of weights: 0.7946934649974113\n","Iteration #2185  Loss: 0.7115982563441379; l2 norm of gradients: 0.6744235329000575; l2 norm of weights: 0.794111627376958\n","Iteration #2186  Loss: 0.7111372396408964; l2 norm of gradients: 0.6736791984605884; l2 norm of weights: 0.7935305934115449\n","Iteration #2187  Loss: 0.7106772242818897; l2 norm of gradients: 0.672935682873631; l2 norm of weights: 0.7929503622288957\n","Iteration #2188  Loss: 0.7102182080642196; l2 norm of gradients: 0.6721929852694607; l2 norm of weights: 0.7923709329574007\n","Iteration #2189  Loss: 0.7097601887898; l2 norm of gradients: 0.6714511047791365; l2 norm of weights: 0.7917923047261153\n","Iteration #2190  Loss: 0.7093031642653476; l2 norm of gradients: 0.6707100405345016; l2 norm of weights: 0.7912144766647597\n","Iteration #2191  Loss: 0.7088471323023713; l2 norm of gradients: 0.6699697916681826; l2 norm of weights: 0.7906374479037183\n","Iteration #2192  Loss: 0.708392090717162; l2 norm of gradients: 0.6692303573135895; l2 norm of weights: 0.7900612175740381\n","Iteration #2193  Loss: 0.7079380373307824; l2 norm of gradients: 0.6684917366049157; l2 norm of weights: 0.7894857848074295\n","Iteration #2194  Loss: 0.7074849699690572; l2 norm of gradients: 0.6677539286771368; l2 norm of weights: 0.7889111487362643\n","Iteration #2195  Loss: 0.7070328864625623; l2 norm of gradients: 0.6670169326660115; l2 norm of weights: 0.7883373084935758\n","Iteration #2196  Loss: 0.7065817846466151; l2 norm of gradients: 0.666280747708081; l2 norm of weights: 0.787764263213057\n","Iteration #2197  Loss: 0.7061316623612649; l2 norm of gradients: 0.6655453729406682; l2 norm of weights: 0.787192012029062\n","Iteration #2198  Loss: 0.7056825174512813; l2 norm of gradients: 0.6648108075018782; l2 norm of weights: 0.7866205540766026\n","Iteration #2199  Loss: 0.7052343477661457; l2 norm of gradients: 0.6640770505305974; l2 norm of weights: 0.7860498884913496\n","Iteration #2200  Loss: 0.704787151160041; l2 norm of gradients: 0.663344101166494; l2 norm of weights: 0.7854800144096317\n","Iteration #2201  Loss: 0.7043409254918404; l2 norm of gradients: 0.6626119585500172; l2 norm of weights: 0.7849109309684341\n","Iteration #2202  Loss: 0.7038956686250991; l2 norm of gradients: 0.6618806218223969; l2 norm of weights: 0.7843426373053987\n","Iteration #2203  Loss: 0.703451378428043; l2 norm of gradients: 0.6611500901256437; l2 norm of weights: 0.7837751325588227\n","Iteration #2204  Loss: 0.7030080527735595; l2 norm of gradients: 0.6604203626025489; l2 norm of weights: 0.7832084158676585\n","Iteration #2205  Loss: 0.702565689539187; l2 norm of gradients: 0.6596914383966833; l2 norm of weights: 0.7826424863715126\n","Iteration #2206  Loss: 0.7021242866071048; l2 norm of gradients: 0.6589633166523983; l2 norm of weights: 0.7820773432106445\n","Iteration #2207  Loss: 0.7016838418641251; l2 norm of gradients: 0.6582359965148242; l2 norm of weights: 0.7815129855259676\n","Iteration #2208  Loss: 0.7012443532016803; l2 norm of gradients: 0.6575094771298714; l2 norm of weights: 0.7809494124590468\n","Iteration #2209  Loss: 0.7008058185158155; l2 norm of gradients: 0.6567837576442286; l2 norm of weights: 0.7803866231520987\n","Iteration #2210  Loss: 0.7003682357071768; l2 norm of gradients: 0.6560588372053641; l2 norm of weights: 0.7798246167479906\n","Iteration #2211  Loss: 0.6999316026810032; l2 norm of gradients: 0.6553347149615242; l2 norm of weights: 0.7792633923902401\n","Iteration #2212  Loss: 0.6994959173471154; l2 norm of gradients: 0.6546113900617339; l2 norm of weights: 0.778702949223014\n","Iteration #2213  Loss: 0.6990611776199065; l2 norm of gradients: 0.6538888616557961; l2 norm of weights: 0.7781432863911282\n","Iteration #2214  Loss: 0.6986273814183328; l2 norm of gradients: 0.6531671288942914; l2 norm of weights: 0.7775844030400465\n","Iteration #2215  Loss: 0.6981945266659039; l2 norm of gradients: 0.6524461909285781; l2 norm of weights: 0.7770262983158805\n","Iteration #2216  Loss: 0.6977626112906716; l2 norm of gradients: 0.651726046910792; l2 norm of weights: 0.7764689713653882\n","Iteration #2217  Loss: 0.6973316332252215; l2 norm of gradients: 0.6510066959938452; l2 norm of weights: 0.7759124213359737\n","Iteration #2218  Loss: 0.6969015904066642; l2 norm of gradients: 0.6502881373314272; l2 norm of weights: 0.7753566473756869\n","Iteration #2219  Loss: 0.6964724807766234; l2 norm of gradients: 0.6495703700780038; l2 norm of weights: 0.7748016486332222\n","Iteration #2220  Loss: 0.6960443022812275; l2 norm of gradients: 0.6488533933888166; l2 norm of weights: 0.7742474242579185\n","Iteration #2221  Loss: 0.6956170528711001; l2 norm of gradients: 0.6481372064198839; l2 norm of weights: 0.7736939733997574\n","Iteration #2222  Loss: 0.6951907305013503; l2 norm of gradients: 0.647421808327999; l2 norm of weights: 0.773141295209364\n","Iteration #2223  Loss: 0.6947653331315622; l2 norm of gradients: 0.6467071982707308; l2 norm of weights: 0.7725893888380055\n","Iteration #2224  Loss: 0.6943408587257873; l2 norm of gradients: 0.6459933754064234; l2 norm of weights: 0.7720382534375905\n","Iteration #2225  Loss: 0.6939173052525327; l2 norm of gradients: 0.6452803388941957; l2 norm of weights: 0.7714878881606682\n","Iteration #2226  Loss: 0.693494670684753; l2 norm of gradients: 0.644568087893941; l2 norm of weights: 0.7709382921604281\n","Iteration #2227  Loss: 0.6930729529998406; l2 norm of gradients: 0.6438566215663271; l2 norm of weights: 0.7703894645906996\n","Iteration #2228  Loss: 0.6926521501796157; l2 norm of gradients: 0.6431459390727958; l2 norm of weights: 0.7698414046059506\n","Iteration #2229  Loss: 0.6922322602103173; l2 norm of gradients: 0.6424360395755627; l2 norm of weights: 0.7692941113612874\n","Iteration #2230  Loss: 0.6918132810825934; l2 norm of gradients: 0.6417269222376166; l2 norm of weights: 0.7687475840124539\n","Iteration #2231  Loss: 0.6913952107914922; l2 norm of gradients: 0.6410185862227197; l2 norm of weights: 0.7682018217158311\n","Iteration #2232  Loss: 0.690978047336452; l2 norm of gradients: 0.6403110306954071; l2 norm of weights: 0.7676568236284358\n","Iteration #2233  Loss: 0.6905617887212918; l2 norm of gradients: 0.639604254820986; l2 norm of weights: 0.7671125889079211\n","Iteration #2234  Loss: 0.6901464329542025; l2 norm of gradients: 0.6388982577655371; l2 norm of weights: 0.7665691167125748\n","Iteration #2235  Loss: 0.6897319780477373; l2 norm of gradients: 0.6381930386959118; l2 norm of weights: 0.7660264062013193\n","Iteration #2236  Loss: 0.6893184220188022; l2 norm of gradients: 0.6374885967797345; l2 norm of weights: 0.7654844565337107\n","Iteration #2237  Loss: 0.6889057628886464; l2 norm of gradients: 0.6367849311853999; l2 norm of weights: 0.764943266869938\n","Iteration #2238  Loss: 0.6884939986828538; l2 norm of gradients: 0.636082041082075; l2 norm of weights: 0.7644028363708231\n","Iteration #2239  Loss: 0.6880831274313334; l2 norm of gradients: 0.6353799256396968; l2 norm of weights: 0.7638631641978195\n","Iteration #2240  Loss: 0.6876731471683095; l2 norm of gradients: 0.6346785840289735; l2 norm of weights: 0.7633242495130121\n","Iteration #2241  Loss: 0.687264055932313; l2 norm of gradients: 0.6339780154213834; l2 norm of weights: 0.7627860914791165\n","Iteration #2242  Loss: 0.6868558517661725; l2 norm of gradients: 0.6332782189891749; l2 norm of weights: 0.762248689259478\n","Iteration #2243  Loss: 0.6864485327170038; l2 norm of gradients: 0.6325791939053662; l2 norm of weights: 0.7617120420180714\n","Iteration #2244  Loss: 0.6860420968362023; l2 norm of gradients: 0.6318809393437447; l2 norm of weights: 0.7611761489195005\n","Iteration #2245  Loss: 0.685636542179433; l2 norm of gradients: 0.631183454478867; l2 norm of weights: 0.760641009128997\n","Iteration #2246  Loss: 0.685231866806621; l2 norm of gradients: 0.6304867384860591; l2 norm of weights: 0.7601066218124201\n","Iteration #2247  Loss: 0.6848280687819434; l2 norm of gradients: 0.629790790541415; l2 norm of weights: 0.7595729861362562\n","Iteration #2248  Loss: 0.684425146173819; l2 norm of gradients: 0.6290956098217971; l2 norm of weights: 0.7590401012676176\n","Iteration #2249  Loss: 0.6840230970549002; l2 norm of gradients: 0.6284011955048359; l2 norm of weights: 0.7585079663742423\n","Iteration #2250  Loss: 0.6836219195020636; l2 norm of gradients: 0.6277075467689293; l2 norm of weights: 0.757976580624494\n","Iteration #2251  Loss: 0.6832216115964005; l2 norm of gradients: 0.627014662793243; l2 norm of weights: 0.75744594318736\n","Iteration #2252  Loss: 0.6828221714232086; l2 norm of gradients: 0.6263225427577096; l2 norm of weights: 0.756916053232452\n","Iteration #2253  Loss: 0.6824235970719823; l2 norm of gradients: 0.625631185843028; l2 norm of weights: 0.7563869099300049\n","Iteration #2254  Loss: 0.6820258866364042; l2 norm of gradients: 0.6249405912306646; l2 norm of weights: 0.7558585124508761\n","Iteration #2255  Loss: 0.6816290382143356; l2 norm of gradients: 0.6242507581028509; l2 norm of weights: 0.755330859966545\n","Iteration #2256  Loss: 0.6812330499078083; l2 norm of gradients: 0.623561685642585; l2 norm of weights: 0.7548039516491125\n","Iteration #2257  Loss: 0.6808379198230153; l2 norm of gradients: 0.62287337303363; l2 norm of weights: 0.7542777866713007\n","Iteration #2258  Loss: 0.6804436460703012; l2 norm of gradients: 0.6221858194605148; l2 norm of weights: 0.7537523642064515\n","Iteration #2259  Loss: 0.6800502267641539; l2 norm of gradients: 0.621499024108533; l2 norm of weights: 0.7532276834285265\n","Iteration #2260  Loss: 0.6796576600231964; l2 norm of gradients: 0.6208129861637425; l2 norm of weights: 0.7527037435121068\n","Iteration #2261  Loss: 0.679265943970176; l2 norm of gradients: 0.6201277048129661; l2 norm of weights: 0.7521805436323912\n","Iteration #2262  Loss: 0.678875076731958; l2 norm of gradients: 0.6194431792437903; l2 norm of weights: 0.7516580829651971\n","Iteration #2263  Loss: 0.6784850564395146; l2 norm of gradients: 0.6187594086445654; l2 norm of weights: 0.7511363606869591\n","Iteration #2264  Loss: 0.6780958812279168; l2 norm of gradients: 0.618076392204405; l2 norm of weights: 0.7506153759747279\n","Iteration #2265  Loss: 0.6777075492363265; l2 norm of gradients: 0.6173941291131858; l2 norm of weights: 0.7500951280061712\n","Iteration #2266  Loss: 0.6773200586079866; l2 norm of gradients: 0.6167126185615471; l2 norm of weights: 0.7495756159595716\n","Iteration #2267  Loss: 0.6769334074902125; l2 norm of gradients: 0.6160318597408913; l2 norm of weights: 0.749056839013827\n","Iteration #2268  Loss: 0.6765475940343835; l2 norm of gradients: 0.615351851843382; l2 norm of weights: 0.7485387963484497\n","Iteration #2269  Loss: 0.6761626163959342; l2 norm of gradients: 0.6146725940619453; l2 norm of weights: 0.7480214871435654\n","Iteration #2270  Loss: 0.6757784727343457; l2 norm of gradients: 0.6139940855902686; l2 norm of weights: 0.7475049105799135\n","Iteration #2271  Loss: 0.675395161213136; l2 norm of gradients: 0.6133163256228; l2 norm of weights: 0.7469890658388458\n","Iteration #2272  Loss: 0.6750126799998533; l2 norm of gradients: 0.6126393133547495; l2 norm of weights: 0.7464739521023266\n","Iteration #2273  Loss: 0.6746310272660655; l2 norm of gradients: 0.6119630479820867; l2 norm of weights: 0.7459595685529309\n","Iteration #2274  Loss: 0.6742502011873523; l2 norm of gradients: 0.6112875287015417; l2 norm of weights: 0.7454459143738457\n","Iteration #2275  Loss: 0.6738701999432964; l2 norm of gradients: 0.6106127547106045; l2 norm of weights: 0.7449329887488674\n","Iteration #2276  Loss: 0.6734910217174752; l2 norm of gradients: 0.6099387252075246; l2 norm of weights: 0.7444207908624028\n","Iteration #2277  Loss: 0.6731126646974521; l2 norm of gradients: 0.609265439391311; l2 norm of weights: 0.7439093198994678\n","Iteration #2278  Loss: 0.6727351270747672; l2 norm of gradients: 0.6085928964617312; l2 norm of weights: 0.7433985750456872\n","Iteration #2279  Loss: 0.6723584070449303; l2 norm of gradients: 0.6079210956193116; l2 norm of weights: 0.7428885554872936\n","Iteration #2280  Loss: 0.6719825028074105; l2 norm of gradients: 0.6072500360653368; l2 norm of weights: 0.7423792604111276\n","Iteration #2281  Loss: 0.6716074125656293; l2 norm of gradients: 0.606579717001849; l2 norm of weights: 0.7418706890046362\n","Iteration #2282  Loss: 0.6712331345269509; l2 norm of gradients: 0.6059101376316485; l2 norm of weights: 0.7413628404558732\n","Iteration #2283  Loss: 0.6708596669026747; l2 norm of gradients: 0.6052412971582923; l2 norm of weights: 0.7408557139534989\n","Iteration #2284  Loss: 0.6704870079080258; l2 norm of gradients: 0.6045731947860948; l2 norm of weights: 0.7403493086867782\n","Iteration #2285  Loss: 0.6701151557621478; l2 norm of gradients: 0.603905829720127; l2 norm of weights: 0.7398436238455809\n","Iteration #2286  Loss: 0.6697441086880926; l2 norm of gradients: 0.6032392011662155; l2 norm of weights: 0.7393386586203812\n","Iteration #2287  Loss: 0.6693738649128141; l2 norm of gradients: 0.6025733083309436; l2 norm of weights: 0.7388344122022573\n","Iteration #2288  Loss: 0.6690044226671583; l2 norm of gradients: 0.60190815042165; l2 norm of weights: 0.7383308837828904\n","Iteration #2289  Loss: 0.6686357801858553; l2 norm of gradients: 0.6012437266464281; l2 norm of weights: 0.7378280725545641\n","Iteration #2290  Loss: 0.6682679357075108; l2 norm of gradients: 0.600580036214127; l2 norm of weights: 0.7373259777101643\n","Iteration #2291  Loss: 0.6679008874745984; l2 norm of gradients: 0.5999170783343499; l2 norm of weights: 0.7368245984431787\n","Iteration #2292  Loss: 0.6675346337334507; l2 norm of gradients: 0.5992548522174543; l2 norm of weights: 0.7363239339476957\n","Iteration #2293  Loss: 0.6671691727342506; l2 norm of gradients: 0.5985933570745516; l2 norm of weights: 0.7358239834184044\n","Iteration #2294  Loss: 0.6668045027310241; l2 norm of gradients: 0.5979325921175069; l2 norm of weights: 0.7353247460505935\n","Iteration #2295  Loss: 0.6664406219816312; l2 norm of gradients: 0.5972725565589381; l2 norm of weights: 0.7348262210401518\n","Iteration #2296  Loss: 0.6660775287477576; l2 norm of gradients: 0.5966132496122163; l2 norm of weights: 0.7343284075835663\n","Iteration #2297  Loss: 0.6657152212949073; l2 norm of gradients: 0.5959546704914652; l2 norm of weights: 0.733831304877923\n","Iteration #2298  Loss: 0.6653536978923928; l2 norm of gradients: 0.5952968184115605; l2 norm of weights: 0.7333349121209053\n","Iteration #2299  Loss: 0.6649929568133289; l2 norm of gradients: 0.5946396925881293; l2 norm of weights: 0.7328392285107942\n","Iteration #2300  Loss: 0.6646329963346227; l2 norm of gradients: 0.5939832922375511; l2 norm of weights: 0.7323442532464672\n","Iteration #2301  Loss: 0.6642738147369667; l2 norm of gradients: 0.5933276165769557; l2 norm of weights: 0.7318499855273984\n","Iteration #2302  Loss: 0.6639154103048298; l2 norm of gradients: 0.5926726648242239; l2 norm of weights: 0.7313564245536578\n","Iteration #2303  Loss: 0.6635577813264495; l2 norm of gradients: 0.5920184361979874; l2 norm of weights: 0.7308635695259103\n","Iteration #2304  Loss: 0.6632009260938239; l2 norm of gradients: 0.5913649299176271; l2 norm of weights: 0.7303714196454154\n","Iteration #2305  Loss: 0.662844842902703; l2 norm of gradients: 0.5907121452032746; l2 norm of weights: 0.7298799741140275\n","Iteration #2306  Loss: 0.6624895300525817; l2 norm of gradients: 0.5900600812758097; l2 norm of weights: 0.729389232134194\n","Iteration #2307  Loss: 0.6621349858466905; l2 norm of gradients: 0.5894087373568623; l2 norm of weights: 0.7288991929089562\n","Iteration #2308  Loss: 0.6617812085919881; l2 norm of gradients: 0.5887581126688102; l2 norm of weights: 0.7284098556419474\n","Iteration #2309  Loss: 0.6614281965991534; l2 norm of gradients: 0.5881082064347801; l2 norm of weights: 0.7279212195373935\n","Iteration #2310  Loss: 0.6610759481825773; l2 norm of gradients: 0.587459017878646; l2 norm of weights: 0.7274332838001121\n","Iteration #2311  Loss: 0.6607244616603545; l2 norm of gradients: 0.5868105462250298; l2 norm of weights: 0.7269460476355118\n","Iteration #2312  Loss: 0.6603737353542761; l2 norm of gradients: 0.5861627906993009; l2 norm of weights: 0.7264595102495924\n","Iteration #2313  Loss: 0.6600237675898206; l2 norm of gradients: 0.585515750527575; l2 norm of weights: 0.7259736708489432\n","Iteration #2314  Loss: 0.6596745566961468; l2 norm of gradients: 0.5848694249367147; l2 norm of weights: 0.7254885286407433\n","Iteration #2315  Loss: 0.6593261010060861; l2 norm of gradients: 0.5842238131543283; l2 norm of weights: 0.7250040828327615\n","Iteration #2316  Loss: 0.6589783988561333; l2 norm of gradients: 0.5835789144087705; l2 norm of weights: 0.7245203326333549\n","Iteration #2317  Loss: 0.6586314485864402; l2 norm of gradients: 0.5829347279291407; l2 norm of weights: 0.7240372772514692\n","Iteration #2318  Loss: 0.6582852485408063; l2 norm of gradients: 0.582291252945284; l2 norm of weights: 0.7235549158966372\n","Iteration #2319  Loss: 0.6579397970666716; l2 norm of gradients: 0.5816484886877897; l2 norm of weights: 0.7230732477789794\n","Iteration #2320  Loss: 0.6575950925151098; l2 norm of gradients: 0.5810064343879916; l2 norm of weights: 0.7225922721092032\n","Iteration #2321  Loss: 0.6572511332408177; l2 norm of gradients: 0.5803650892779673; l2 norm of weights: 0.7221119880986017\n","Iteration #2322  Loss: 0.6569079176021104; l2 norm of gradients: 0.5797244525905382; l2 norm of weights: 0.7216323949590543\n","Iteration #2323  Loss: 0.6565654439609114; l2 norm of gradients: 0.5790845235592684; l2 norm of weights: 0.7211534919030258\n","Iteration #2324  Loss: 0.6562237106827455; l2 norm of gradients: 0.5784453014184656; l2 norm of weights: 0.7206752781435651\n","Iteration #2325  Loss: 0.6558827161367315; l2 norm of gradients: 0.5778067854031794; l2 norm of weights: 0.720197752894306\n","Iteration #2326  Loss: 0.6555424586955736; l2 norm of gradients: 0.5771689747492015; l2 norm of weights: 0.7197209153694664\n","Iteration #2327  Loss: 0.6552029367355539; l2 norm of gradients: 0.5765318686930655; l2 norm of weights: 0.719244764783847\n","Iteration #2328  Loss: 0.6548641486365249; l2 norm of gradients: 0.5758954664720461; l2 norm of weights: 0.7187693003528322\n","Iteration #2329  Loss: 0.6545260927819019; l2 norm of gradients: 0.5752597673241594; l2 norm of weights: 0.718294521292388\n","Iteration #2330  Loss: 0.6541887675586548; l2 norm of gradients: 0.5746247704881613; l2 norm of weights: 0.717820426819063\n","Iteration #2331  Loss: 0.6538521713573002; l2 norm of gradients: 0.5739904752035487; l2 norm of weights: 0.717347016149987\n","Iteration #2332  Loss: 0.6535163025718951; l2 norm of gradients: 0.5733568807105579; l2 norm of weights: 0.7168742885028712\n","Iteration #2333  Loss: 0.6531811596000276; l2 norm of gradients: 0.5727239862501647; l2 norm of weights: 0.7164022430960071\n","Iteration #2334  Loss: 0.6528467408428101; l2 norm of gradients: 0.572091791064084; l2 norm of weights: 0.7159308791482665\n","Iteration #2335  Loss: 0.6525130447048721; l2 norm of gradients: 0.5714602943947694; l2 norm of weights: 0.7154601958791011\n","Iteration #2336  Loss: 0.6521800695943507; l2 norm of gradients: 0.570829495485413; l2 norm of weights: 0.7149901925085412\n","Iteration #2337  Loss: 0.6518478139228863; l2 norm of gradients: 0.5701993935799441; l2 norm of weights: 0.7145208682571967\n","Iteration #2338  Loss: 0.651516276105611; l2 norm of gradients: 0.5695699879230304; l2 norm of weights: 0.7140522223462551\n","Iteration #2339  Loss: 0.6511854545611446; l2 norm of gradients: 0.568941277760076; l2 norm of weights: 0.7135842539974822\n","Iteration #2340  Loss: 0.6508553477115849; l2 norm of gradients: 0.5683132623372225; l2 norm of weights: 0.713116962433221\n","Iteration #2341  Loss: 0.6505259539825009; l2 norm of gradients: 0.5676859409013473; l2 norm of weights: 0.7126503468763917\n","Iteration #2342  Loss: 0.6501972718029252; l2 norm of gradients: 0.567059312700064; l2 norm of weights: 0.7121844065504909\n","Iteration #2343  Loss: 0.6498692996053463; l2 norm of gradients: 0.5664333769817217; l2 norm of weights: 0.711719140679591\n","Iteration #2344  Loss: 0.649542035825702; l2 norm of gradients: 0.565808132995405; l2 norm of weights: 0.7112545484883406\n","Iteration #2345  Loss: 0.6492154789033707; l2 norm of gradients: 0.5651835799909328; l2 norm of weights: 0.7107906292019632\n","Iteration #2346  Loss: 0.6488896272811644; l2 norm of gradients: 0.5645597172188591; l2 norm of weights: 0.7103273820462572\n","Iteration #2347  Loss: 0.6485644794053215; l2 norm of gradients: 0.5639365439304714; l2 norm of weights: 0.709864806247595\n","Iteration #2348  Loss: 0.6482400337254992; l2 norm of gradients: 0.5633140593777908; l2 norm of weights: 0.7094029010329231\n","Iteration #2349  Loss: 0.6479162886947663; l2 norm of gradients: 0.5626922628135721; l2 norm of weights: 0.7089416656297616\n","Iteration #2350  Loss: 0.6475932427695953; l2 norm of gradients: 0.5620711534913025; l2 norm of weights: 0.7084810992662033\n","Iteration #2351  Loss: 0.6472708944098557; l2 norm of gradients: 0.5614507306652021; l2 norm of weights: 0.7080212011709139\n","Iteration #2352  Loss: 0.6469492420788061; l2 norm of gradients: 0.5608309935902227; l2 norm of weights: 0.7075619705731313\n","Iteration #2353  Loss: 0.646628284243087; l2 norm of gradients: 0.5602119415220479; l2 norm of weights: 0.7071034067026648\n","Iteration #2354  Loss: 0.6463080193727134; l2 norm of gradients: 0.5595935737170925; l2 norm of weights: 0.7066455087898952\n","Iteration #2355  Loss: 0.6459884459410681; l2 norm of gradients: 0.5589758894325023; l2 norm of weights: 0.7061882760657743\n","Iteration #2356  Loss: 0.6456695624248932; l2 norm of gradients: 0.5583588879261534; l2 norm of weights: 0.7057317077618241\n","Iteration #2357  Loss: 0.6453513673042841; l2 norm of gradients: 0.5577425684566522; l2 norm of weights: 0.7052758031101372\n","Iteration #2358  Loss: 0.645033859062681; l2 norm of gradients: 0.5571269302833344; l2 norm of weights: 0.7048205613433749\n","Iteration #2359  Loss: 0.6447170361868628; l2 norm of gradients: 0.5565119726662652; l2 norm of weights: 0.7043659816947687\n","Iteration #2360  Loss: 0.6444008971669394; l2 norm of gradients: 0.5558976948662387; l2 norm of weights: 0.7039120633981184\n","Iteration #2361  Loss: 0.6440854404963438; l2 norm of gradients: 0.5552840961447776; l2 norm of weights: 0.7034588056877922\n","Iteration #2362  Loss: 0.643770664671826; l2 norm of gradients: 0.554671175764132; l2 norm of weights: 0.7030062077987264\n","Iteration #2363  Loss: 0.6434565681934451; l2 norm of gradients: 0.5540589329872805; l2 norm of weights: 0.7025542689664248\n","Iteration #2364  Loss: 0.6431431495645625; l2 norm of gradients: 0.5534473670779282; l2 norm of weights: 0.7021029884269585\n","Iteration #2365  Loss: 0.6428304072918344; l2 norm of gradients: 0.5528364773005074; l2 norm of weights: 0.7016523654169652\n","Iteration #2366  Loss: 0.6425183398852051; l2 norm of gradients: 0.5522262629201768; l2 norm of weights: 0.7012023991736492\n","Iteration #2367  Loss: 0.6422069458578986; l2 norm of gradients: 0.5516167232028212; l2 norm of weights: 0.7007530889347803\n","Iteration #2368  Loss: 0.641896223726414; l2 norm of gradients: 0.5510078574150507; l2 norm of weights: 0.7003044339386945\n","Iteration #2369  Loss: 0.6415861720105153; l2 norm of gradients: 0.5503996648242008; l2 norm of weights: 0.6998564334242922\n","Iteration #2370  Loss: 0.6412767892332267; l2 norm of gradients: 0.5497921446983316; l2 norm of weights: 0.6994090866310393\n","Iteration #2371  Loss: 0.6409680739208249; l2 norm of gradients: 0.5491852963062279; l2 norm of weights: 0.6989623927989657\n","Iteration #2372  Loss: 0.6406600246028309; l2 norm of gradients: 0.5485791189173983; l2 norm of weights: 0.6985163511686652\n","Iteration #2373  Loss: 0.6403526398120045; l2 norm of gradients: 0.5479736118020747; l2 norm of weights: 0.6980709609812955\n","Iteration #2374  Loss: 0.6400459180843366; l2 norm of gradients: 0.5473687742312122; l2 norm of weights: 0.6976262214785771\n","Iteration #2375  Loss: 0.6397398579590423; l2 norm of gradients: 0.546764605476489; l2 norm of weights: 0.6971821319027939\n","Iteration #2376  Loss: 0.6394344579785533; l2 norm of gradients: 0.5461611048103051; l2 norm of weights: 0.6967386914967915\n","Iteration #2377  Loss: 0.6391297166885116; l2 norm of gradients: 0.5455582715057828; l2 norm of weights: 0.6962958995039779\n","Iteration #2378  Loss: 0.6388256326377628; l2 norm of gradients: 0.5449561048367654; l2 norm of weights: 0.6958537551683229\n","Iteration #2379  Loss: 0.6385222043783483; l2 norm of gradients: 0.5443546040778177; l2 norm of weights: 0.6954122577343577\n","Iteration #2380  Loss: 0.6382194304654987; l2 norm of gradients: 0.5437537685042244; l2 norm of weights: 0.6949714064471737\n","Iteration #2381  Loss: 0.6379173094576268; l2 norm of gradients: 0.5431535973919913; l2 norm of weights: 0.6945312005524235\n","Iteration #2382  Loss: 0.6376158399163213; l2 norm of gradients: 0.5425540900178434; l2 norm of weights: 0.6940916392963196\n","Iteration #2383  Loss: 0.6373150204063389; l2 norm of gradients: 0.5419552456592251; l2 norm of weights: 0.6936527219256341\n","Iteration #2384  Loss: 0.6370148494955982; l2 norm of gradients: 0.5413570635943; l2 norm of weights: 0.6932144476876991\n","Iteration #2385  Loss: 0.6367153257551723; l2 norm of gradients: 0.54075954310195; l2 norm of weights: 0.6927768158304051\n","Iteration #2386  Loss: 0.6364164477592823; l2 norm of gradients: 0.5401626834617748; l2 norm of weights: 0.6923398256022014\n","Iteration #2387  Loss: 0.6361182140852902; l2 norm of gradients: 0.5395664839540922; l2 norm of weights: 0.691903476252096\n","Iteration #2388  Loss: 0.6358206233136927; l2 norm of gradients: 0.538970943859937; l2 norm of weights: 0.6914677670296545\n","Iteration #2389  Loss: 0.6355236740281128; l2 norm of gradients: 0.5383760624610611; l2 norm of weights: 0.691032697185\n","Iteration #2390  Loss: 0.6352273648152953; l2 norm of gradients: 0.5377818390399323; l2 norm of weights: 0.690598265968813\n","Iteration #2391  Loss: 0.6349316942650981; l2 norm of gradients: 0.5371882728797346; l2 norm of weights: 0.6901644726323309\n","Iteration #2392  Loss: 0.6346366609704861; l2 norm of gradients: 0.5365953632643675; l2 norm of weights: 0.6897313164273474\n","Iteration #2393  Loss: 0.6343422635275249; l2 norm of gradients: 0.5360031094784455; l2 norm of weights: 0.6892987966062125\n","Iteration #2394  Loss: 0.6340485005353729; l2 norm of gradients: 0.5354115108072979; l2 norm of weights: 0.6888669124218318\n","Iteration #2395  Loss: 0.633755370596276; l2 norm of gradients: 0.5348205665369681; l2 norm of weights: 0.6884356631276667\n","Iteration #2396  Loss: 0.63346287231556; l2 norm of gradients: 0.5342302759542131; l2 norm of weights: 0.6880050479777333\n","Iteration #2397  Loss: 0.6331710043016237; l2 norm of gradients: 0.5336406383465039; l2 norm of weights: 0.6875750662266027\n","Iteration #2398  Loss: 0.6328797651659328; l2 norm of gradients: 0.5330516530020235; l2 norm of weights: 0.6871457171294001\n","Iteration #2399  Loss: 0.6325891535230134; l2 norm of gradients: 0.5324633192096683; l2 norm of weights: 0.6867169999418054\n","Iteration #2400  Loss: 0.632299167990444; l2 norm of gradients: 0.5318756362590459; l2 norm of weights: 0.6862889139200511\n","Iteration #2401  Loss: 0.6320098071888509; l2 norm of gradients: 0.5312886034404761; l2 norm of weights: 0.685861458320924\n","Iteration #2402  Loss: 0.6317210697418995; l2 norm of gradients: 0.5307022200449897; l2 norm of weights: 0.6854346324017637\n","Iteration #2403  Loss: 0.6314329542762894; l2 norm of gradients: 0.5301164853643282; l2 norm of weights: 0.6850084354204624\n","Iteration #2404  Loss: 0.6311454594217464; l2 norm of gradients: 0.5295313986909433; l2 norm of weights: 0.6845828666354645\n","Iteration #2405  Loss: 0.6308585838110171; l2 norm of gradients: 0.5289469593179971; l2 norm of weights: 0.6841579253057666\n","Iteration #2406  Loss: 0.6305723260798614; l2 norm of gradients: 0.5283631665393602; l2 norm of weights: 0.6837336106909169\n","Iteration #2407  Loss: 0.6302866848670463; l2 norm of gradients: 0.5277800196496129; l2 norm of weights: 0.6833099220510152\n","Iteration #2408  Loss: 0.6300016588143399; l2 norm of gradients: 0.5271975179440438; l2 norm of weights: 0.6828868586467118\n","Iteration #2409  Loss: 0.6297172465665035; l2 norm of gradients: 0.5266156607186495; l2 norm of weights: 0.6824644197392081\n","Iteration #2410  Loss: 0.6294334467712864; l2 norm of gradients: 0.5260344472701347; l2 norm of weights: 0.6820426045902557\n","Iteration #2411  Loss: 0.629150258079419; l2 norm of gradients: 0.5254538768959104; l2 norm of weights: 0.6816214124621564\n","Iteration #2412  Loss: 0.6288676791446057; l2 norm of gradients: 0.5248739488940953; l2 norm of weights: 0.6812008426177615\n","Iteration #2413  Loss: 0.6285857086235195; l2 norm of gradients: 0.524294662563514; l2 norm of weights: 0.6807808943204717\n","Iteration #2414  Loss: 0.6283043451757946; l2 norm of gradients: 0.523716017203697; l2 norm of weights: 0.680361566834237\n","Iteration #2415  Loss: 0.6280235874640202; l2 norm of gradients: 0.5231380121148801; l2 norm of weights: 0.6799428594235559\n","Iteration #2416  Loss: 0.6277434341537342; l2 norm of gradients: 0.5225606465980043; l2 norm of weights: 0.6795247713534756\n","Iteration #2417  Loss: 0.6274638839134176; l2 norm of gradients: 0.521983919954715; l2 norm of weights: 0.6791073018895911\n","Iteration #2418  Loss: 0.6271849354144855; l2 norm of gradients: 0.5214078314873618; l2 norm of weights: 0.6786904502980454\n","Iteration #2419  Loss: 0.626906587331284; l2 norm of gradients: 0.5208323804989978; l2 norm of weights: 0.6782742158455292\n","Iteration #2420  Loss: 0.6266288383410814; l2 norm of gradients: 0.5202575662933793; l2 norm of weights: 0.6778585977992799\n","Iteration #2421  Loss: 0.6263516871240628; l2 norm of gradients: 0.5196833881749654; l2 norm of weights: 0.6774435954270822\n","Iteration #2422  Loss: 0.6260751323633237; l2 norm of gradients: 0.5191098454489175; l2 norm of weights: 0.6770292079972672\n","Iteration #2423  Loss: 0.6257991727448635; l2 norm of gradients: 0.5185369374210985; l2 norm of weights: 0.6766154347787123\n","Iteration #2424  Loss: 0.6255238069575794; l2 norm of gradients: 0.5179646633980733; l2 norm of weights: 0.6762022750408409\n","Iteration #2425  Loss: 0.6252490336932596; l2 norm of gradients: 0.5173930226871072; l2 norm of weights: 0.675789728053622\n","Iteration #2426  Loss: 0.6249748516465773; l2 norm of gradients: 0.5168220145961661; l2 norm of weights: 0.6753777930875701\n","Iteration #2427  Loss: 0.6247012595150846; l2 norm of gradients: 0.5162516384339157; l2 norm of weights: 0.6749664694137445\n","Iteration #2428  Loss: 0.624428255999206; l2 norm of gradients: 0.5156818935097219; l2 norm of weights: 0.6745557563037496\n","Iteration #2429  Loss: 0.6241558398022322; l2 norm of gradients: 0.5151127791336493; l2 norm of weights: 0.6741456530297342\n","Iteration #2430  Loss: 0.6238840096303134; l2 norm of gradients: 0.5145442946164609; l2 norm of weights: 0.673736158864391\n","Iteration #2431  Loss: 0.6236127641924541; l2 norm of gradients: 0.5139764392696182; l2 norm of weights: 0.6733272730809571\n","Iteration #2432  Loss: 0.6233421022005057; l2 norm of gradients: 0.5134092124052801; l2 norm of weights: 0.6729189949532127\n","Iteration #2433  Loss: 0.6230720223691609; l2 norm of gradients: 0.5128426133363035; l2 norm of weights: 0.6725113237554817\n","Iteration #2434  Loss: 0.6228025234159477; l2 norm of gradients: 0.5122766413762414; l2 norm of weights: 0.6721042587626308\n","Iteration #2435  Loss: 0.6225336040612228; l2 norm of gradients: 0.5117112958393433; l2 norm of weights: 0.6716977992500696\n","Iteration #2436  Loss: 0.6222652630281656; l2 norm of gradients: 0.5111465760405547; l2 norm of weights: 0.67129194449375\n","Iteration #2437  Loss: 0.6219974990427717; l2 norm of gradients: 0.5105824812955165; l2 norm of weights: 0.6708866937701662\n","Iteration #2438  Loss: 0.6217303108338474; l2 norm of gradients: 0.5100190109205647; l2 norm of weights: 0.6704820463563543\n","Iteration #2439  Loss: 0.6214636971330032; l2 norm of gradients: 0.5094561642327294; l2 norm of weights: 0.6700780015298922\n","Iteration #2440  Loss: 0.6211976566746473; l2 norm of gradients: 0.5088939405497352; l2 norm of weights: 0.6696745585688986\n","Iteration #2441  Loss: 0.6209321881959806; l2 norm of gradients: 0.50833233919; l2 norm of weights: 0.669271716752034\n","Iteration #2442  Loss: 0.620667290436989; l2 norm of gradients: 0.5077713594726349; l2 norm of weights: 0.6688694753584989\n","Iteration #2443  Loss: 0.620402962140439; l2 norm of gradients: 0.5072110007174435; l2 norm of weights: 0.6684678336680353\n","Iteration #2444  Loss: 0.6201392020518701; l2 norm of gradients: 0.506651262244922; l2 norm of weights: 0.6680667909609245\n","Iteration #2445  Loss: 0.6198760089195906; l2 norm of gradients: 0.5060921433762577; l2 norm of weights: 0.6676663465179885\n","Iteration #2446  Loss: 0.619613381494669; l2 norm of gradients: 0.5055336434333296; l2 norm of weights: 0.6672664996205887\n","Iteration #2447  Loss: 0.6193513185309303; l2 norm of gradients: 0.5049757617387075; l2 norm of weights: 0.6668672495506259\n","Iteration #2448  Loss: 0.619089818784949; l2 norm of gradients: 0.5044184976156513; l2 norm of weights: 0.6664685955905403\n","Iteration #2449  Loss: 0.6188288810160427; l2 norm of gradients: 0.5038618503881105; l2 norm of weights: 0.6660705370233109\n","Iteration #2450  Loss: 0.6185685039862674; l2 norm of gradients: 0.5033058193807246; l2 norm of weights: 0.6656730731324553\n","Iteration #2451  Loss: 0.6183086864604099; l2 norm of gradients: 0.5027504039188215; l2 norm of weights: 0.66527620320203\n","Iteration #2452  Loss: 0.6180494272059831; l2 norm of gradients: 0.502195603328418; l2 norm of weights: 0.6648799265166289\n","Iteration #2453  Loss: 0.6177907249932191; l2 norm of gradients: 0.5016414169362181; l2 norm of weights: 0.6644842423613845\n","Iteration #2454  Loss: 0.6175325785950644; l2 norm of gradients: 0.501087844069614; l2 norm of weights: 0.6640891500219664\n","Iteration #2455  Loss: 0.6172749867871729; l2 norm of gradients: 0.5005348840566849; l2 norm of weights: 0.6636946487845822\n","Iteration #2456  Loss: 0.6170179483479001; l2 norm of gradients: 0.49998253622619604; l2 norm of weights: 0.6633007379359759\n","Iteration #2457  Loss: 0.6167614620582981; l2 norm of gradients: 0.49943079990759903; l2 norm of weights: 0.662907416763429\n","Iteration #2458  Loss: 0.6165055267021087; l2 norm of gradients: 0.4988796744310311; l2 norm of weights: 0.6625146845547595\n","Iteration #2459  Loss: 0.6162501410657576; l2 norm of gradients: 0.49832915912731457; l2 norm of weights: 0.6621225405983215\n","Iteration #2460  Loss: 0.6159953039383497; l2 norm of gradients: 0.49777925332795636; l2 norm of weights: 0.6617309841830058\n","Iteration #2461  Loss: 0.6157410141116615; l2 norm of gradients: 0.4972299563651477; l2 norm of weights: 0.6613400145982388\n","Iteration #2462  Loss: 0.6154872703801365; l2 norm of gradients: 0.49668126757176323; l2 norm of weights: 0.6609496311339823\n","Iteration #2463  Loss: 0.6152340715408793; l2 norm of gradients: 0.49613318628136105; l2 norm of weights: 0.6605598330807343\n","Iteration #2464  Loss: 0.6149814163936488; l2 norm of gradients: 0.49558571182818206; l2 norm of weights: 0.6601706197295271\n","Iteration #2465  Loss: 0.6147293037408537; l2 norm of gradients: 0.49503884354714917; l2 norm of weights: 0.6597819903719286\n","Iteration #2466  Loss: 0.6144777323875454; l2 norm of gradients: 0.4944925807738672; l2 norm of weights: 0.6593939443000413\n","Iteration #2467  Loss: 0.6142267011414139; l2 norm of gradients: 0.4939469228446223; l2 norm of weights: 0.659006480806502\n","Iteration #2468  Loss: 0.6139762088127799; l2 norm of gradients: 0.4934018690963815; l2 norm of weights: 0.6586195991844818\n","Iteration #2469  Loss: 0.6137262542145909; l2 norm of gradients: 0.492857418866792; l2 norm of weights: 0.6582332987276861\n","Iteration #2470  Loss: 0.6134768361624147; l2 norm of gradients: 0.49231357149418087; l2 norm of weights: 0.6578475787303538\n","Iteration #2471  Loss: 0.6132279534744332; l2 norm of gradients: 0.4917703263175549; l2 norm of weights: 0.6574624384872575\n","Iteration #2472  Loss: 0.6129796049714376; l2 norm of gradients: 0.4912276826765993; l2 norm of weights: 0.657077877293703\n","Iteration #2473  Loss: 0.6127317894768223; l2 norm of gradients: 0.49068563991167796; l2 norm of weights: 0.6566938944455297\n","Iteration #2474  Loss: 0.6124845058165788; l2 norm of gradients: 0.49014419736383286; l2 norm of weights: 0.6563104892391092\n","Iteration #2475  Loss: 0.6122377528192908; l2 norm of gradients: 0.48960335437478303; l2 norm of weights: 0.6559276609713462\n","Iteration #2476  Loss: 0.6119915293161277; l2 norm of gradients: 0.4890631102869249; l2 norm of weights: 0.6555454089396779\n","Iteration #2477  Loss: 0.6117458341408396; l2 norm of gradients: 0.4885234644433309; l2 norm of weights: 0.6551637324420736\n","Iteration #2478  Loss: 0.6115006661297514; l2 norm of gradients: 0.48798441618774996; l2 norm of weights: 0.6547826307770345\n","Iteration #2479  Loss: 0.6112560241217571; l2 norm of gradients: 0.48744596486460634; l2 norm of weights: 0.6544021032435939\n","Iteration #2480  Loss: 0.6110119069583143; l2 norm of gradients: 0.48690810981899896; l2 norm of weights: 0.6540221491413165\n","Iteration #2481  Loss: 0.6107683134834384; l2 norm of gradients: 0.48637085039670186; l2 norm of weights: 0.6536427677702985\n","Iteration #2482  Loss: 0.6105252425436973; l2 norm of gradients: 0.48583418594416294; l2 norm of weights: 0.6532639584311671\n","Iteration #2483  Loss: 0.6102826929882055; l2 norm of gradients: 0.48529811580850335; l2 norm of weights: 0.6528857204250806\n","Iteration #2484  Loss: 0.6100406636686188; l2 norm of gradients: 0.48476263933751784; l2 norm of weights: 0.6525080530537282\n","Iteration #2485  Loss: 0.6097991534391282; l2 norm of gradients: 0.48422775587967326; l2 norm of weights: 0.652130955619329\n","Iteration #2486  Loss: 0.6095581611564552; l2 norm of gradients: 0.4836934647841089; l2 norm of weights: 0.6517544274246333\n","Iteration #2487  Loss: 0.6093176856798459; l2 norm of gradients: 0.48315976540063554; l2 norm of weights: 0.6513784677729211\n","Iteration #2488  Loss: 0.6090777258710649; l2 norm of gradients: 0.4826266570797352; l2 norm of weights: 0.6510030759680021\n","Iteration #2489  Loss: 0.6088382805943906; l2 norm of gradients: 0.4820941391725603; l2 norm of weights: 0.6506282513142165\n","Iteration #2490  Loss: 0.6085993487166089; l2 norm of gradients: 0.4815622110309335; l2 norm of weights: 0.6502539931164331\n","Iteration #2491  Loss: 0.6083609291070091; l2 norm of gradients: 0.4810308720073473; l2 norm of weights: 0.6498803006800508\n","Iteration #2492  Loss: 0.6081230206373762; l2 norm of gradients: 0.48050012145496307; l2 norm of weights: 0.6495071733109969\n","Iteration #2493  Loss: 0.6078856221819879; l2 norm of gradients: 0.47996995872761106; l2 norm of weights: 0.6491346103157285\n","Iteration #2494  Loss: 0.6076487326176072; l2 norm of gradients: 0.4794403831797896; l2 norm of weights: 0.6487626110012309\n","Iteration #2495  Loss: 0.6074123508234781; l2 norm of gradients: 0.4789113941666646; l2 norm of weights: 0.6483911746750181\n","Iteration #2496  Loss: 0.6071764756813192; l2 norm of gradients: 0.47838299104406945; l2 norm of weights: 0.6480203006451323\n","Iteration #2497  Loss: 0.6069411060753196; l2 norm of gradients: 0.477855173168504; l2 norm of weights: 0.6476499882201443\n","Iteration #2498  Loss: 0.6067062408921321; l2 norm of gradients: 0.47732793989713435; l2 norm of weights: 0.6472802367091525\n","Iteration #2499  Loss: 0.6064718790208689; l2 norm of gradients: 0.4768012905877924; l2 norm of weights: 0.6469110454217831\n","Iteration #2500  Loss: 0.6062380193530952; l2 norm of gradients: 0.47627522459897503; l2 norm of weights: 0.6465424136681903\n","Iteration #2501  Loss: 0.6060046607828249; l2 norm of gradients: 0.475749741289844; l2 norm of weights: 0.6461743407590553\n","Iteration #2502  Loss: 0.6057718022065144; l2 norm of gradients: 0.4752248400202254; l2 norm of weights: 0.6458068260055869\n","Iteration #2503  Loss: 0.6055394425230577; l2 norm of gradients: 0.4747005201506089; l2 norm of weights: 0.645439868719521\n","Iteration #2504  Loss: 0.6053075806337805; l2 norm of gradients: 0.4741767810421473; l2 norm of weights: 0.6450734682131198\n","Iteration #2505  Loss: 0.605076215442436; l2 norm of gradients: 0.4736536220566562; l2 norm of weights: 0.644707623799173\n","Iteration #2506  Loss: 0.6048453458551981; l2 norm of gradients: 0.4731310425566136; l2 norm of weights: 0.6443423347909965\n","Iteration #2507  Loss: 0.604614970780657; l2 norm of gradients: 0.47260904190515907; l2 norm of weights: 0.6439776005024324\n","Iteration #2508  Loss: 0.6043850891298141; l2 norm of gradients: 0.47208761946609346; l2 norm of weights: 0.6436134202478491\n","Iteration #2509  Loss: 0.6041556998160758; l2 norm of gradients: 0.47156677460387825; l2 norm of weights: 0.6432497933421412\n","Iteration #2510  Loss: 0.6039268017552496; l2 norm of gradients: 0.47104650668363524; l2 norm of weights: 0.642886719100729\n","Iteration #2511  Loss: 0.6036983938655368; l2 norm of gradients: 0.4705268150711461; l2 norm of weights: 0.6425241968395584\n","Iteration #2512  Loss: 0.6034704750675295; l2 norm of gradients: 0.47000769913285145; l2 norm of weights: 0.6421622258751012\n","Iteration #2513  Loss: 0.603243044284204; l2 norm of gradients: 0.46948915823585075; l2 norm of weights: 0.641800805524354\n","Iteration #2514  Loss: 0.6030161004409158; l2 norm of gradients: 0.46897119174790197; l2 norm of weights: 0.641439935104839\n","Iteration #2515  Loss: 0.6027896424653942; l2 norm of gradients: 0.4684537990374204; l2 norm of weights: 0.6410796139346031\n","Iteration #2516  Loss: 0.6025636692877381; l2 norm of gradients: 0.4679369794734788; l2 norm of weights: 0.6407198413322184\n","Iteration #2517  Loss: 0.6023381798404092; l2 norm of gradients: 0.46742073242580656; l2 norm of weights: 0.6403606166167815\n","Iteration #2518  Loss: 0.6021131730582281; l2 norm of gradients: 0.46690505726478937; l2 norm of weights: 0.6400019391079133\n","Iteration #2519  Loss: 0.6018886478783689; l2 norm of gradients: 0.46638995336146843; l2 norm of weights: 0.6396438081257594\n","Iteration #2520  Loss: 0.6016646032403534; l2 norm of gradients: 0.4658754200875406; l2 norm of weights: 0.6392862229909895\n","Iteration #2521  Loss: 0.6014410380860467; l2 norm of gradients: 0.46536145681535707; l2 norm of weights: 0.6389291830247976\n","Iteration #2522  Loss: 0.6012179513596513; l2 norm of gradients: 0.4648480629179233; l2 norm of weights: 0.6385726875489011\n","Iteration #2523  Loss: 0.600995342007703; l2 norm of gradients: 0.46433523776889857; l2 norm of weights: 0.6382167358855415\n","Iteration #2524  Loss: 0.6007732089790649; l2 norm of gradients: 0.46382298074259526; l2 norm of weights: 0.6378613273574839\n","Iteration #2525  Loss: 0.6005515512249227; l2 norm of gradients: 0.46331129121397857; l2 norm of weights: 0.6375064612880167\n","Iteration #2526  Loss: 0.6003303676987789; l2 norm of gradients: 0.4628001685586657; l2 norm of weights: 0.6371521370009516\n","Iteration #2527  Loss: 0.600109657356449; l2 norm of gradients: 0.46228961215292563; l2 norm of weights: 0.6367983538206237\n","Iteration #2528  Loss: 0.5998894191560558; l2 norm of gradients: 0.4617796213736787; l2 norm of weights: 0.6364451110718907\n","Iteration #2529  Loss: 0.5996696520580236; l2 norm of gradients: 0.4612701955984957; l2 norm of weights: 0.6360924080801336\n","Iteration #2530  Loss: 0.5994503550250739; l2 norm of gradients: 0.4607613342055975; l2 norm of weights: 0.6357402441712555\n","Iteration #2531  Loss: 0.5992315270222208; l2 norm of gradients: 0.460253036573855; l2 norm of weights: 0.6353886186716827\n","Iteration #2532  Loss: 0.5990131670167652; l2 norm of gradients: 0.45974530208278785; l2 norm of weights: 0.6350375309083637\n","Iteration #2533  Loss: 0.5987952739782898; l2 norm of gradients: 0.4592381301125646; l2 norm of weights: 0.6346869802087691\n","Iteration #2534  Loss: 0.5985778468786542; l2 norm of gradients: 0.4587315200440018; l2 norm of weights: 0.6343369659008918\n","Iteration #2535  Loss: 0.5983608846919904; l2 norm of gradients: 0.4582254712585637; l2 norm of weights: 0.6339874873132468\n","Iteration #2536  Loss: 0.5981443863946971; l2 norm of gradients: 0.4577199831383617; l2 norm of weights: 0.6336385437748707\n","Iteration #2537  Loss: 0.5979283509654352; l2 norm of gradients: 0.4572150550661537; l2 norm of weights: 0.6332901346153222\n","Iteration #2538  Loss: 0.5977127773851225; l2 norm of gradients: 0.4567106864253437; l2 norm of weights: 0.6329422591646813\n","Iteration #2539  Loss: 0.5974976646369289; l2 norm of gradients: 0.4562068765999813; l2 norm of weights: 0.6325949167535494\n","Iteration #2540  Loss: 0.5972830117062716; l2 norm of gradients: 0.4557036249747615; l2 norm of weights: 0.6322481067130495\n","Iteration #2541  Loss: 0.59706881758081; l2 norm of gradients: 0.45520093093502334; l2 norm of weights: 0.6319018283748258\n","Iteration #2542  Loss: 0.5968550812504404; l2 norm of gradients: 0.4546987938667502; l2 norm of weights: 0.6315560810710433\n","Iteration #2543  Loss: 0.5966418017072921; l2 norm of gradients: 0.4541972131565691; l2 norm of weights: 0.6312108641343881\n","Iteration #2544  Loss: 0.5964289779457214; l2 norm of gradients: 0.4536961881917497; l2 norm of weights: 0.6308661768980671\n","Iteration #2545  Loss: 0.5962166089623073; l2 norm of gradients: 0.4531957183602047; l2 norm of weights: 0.6305220186958079\n","Iteration #2546  Loss: 0.5960046937558465; l2 norm of gradients: 0.4526958030504885; l2 norm of weights: 0.6301783888618587\n","Iteration #2547  Loss: 0.5957932313273486; l2 norm of gradients: 0.45219644165179707; l2 norm of weights: 0.6298352867309882\n","Iteration #2548  Loss: 0.5955822206800312; l2 norm of gradients: 0.45169763355396725; l2 norm of weights: 0.6294927116384853\n","Iteration #2549  Loss: 0.5953716608193147; l2 norm of gradients: 0.45119937814747657; l2 norm of weights: 0.6291506629201589\n","Iteration #2550  Loss: 0.5951615507528177; l2 norm of gradients: 0.4507016748234424; l2 norm of weights: 0.6288091399123383\n","Iteration #2551  Loss: 0.5949518894903532; l2 norm of gradients: 0.4502045229736217; l2 norm of weights: 0.6284681419518727\n","Iteration #2552  Loss: 0.5947426760439214; l2 norm of gradients: 0.4497079219904099; l2 norm of weights: 0.6281276683761312\n","Iteration #2553  Loss: 0.5945339094277072; l2 norm of gradients: 0.4492118712668415; l2 norm of weights: 0.6277877185230023\n","Iteration #2554  Loss: 0.5943255886580745; l2 norm of gradients: 0.44871637019658844; l2 norm of weights: 0.6274482917308944\n","Iteration #2555  Loss: 0.5941177127535608; l2 norm of gradients: 0.4482214181739603; l2 norm of weights: 0.6271093873387352\n","Iteration #2556  Loss: 0.5939102807348737; l2 norm of gradients: 0.4477270145939033; l2 norm of weights: 0.6267710046859721\n","Iteration #2557  Loss: 0.5937032916248851; l2 norm of gradients: 0.44723315885200027; l2 norm of weights: 0.6264331431125715\n","Iteration #2558  Loss: 0.5934967444486268; l2 norm of gradients: 0.44673985034446984; l2 norm of weights: 0.6260958019590188\n","Iteration #2559  Loss: 0.5932906382332861; l2 norm of gradients: 0.4462470884681658; l2 norm of weights: 0.625758980566319\n","Iteration #2560  Loss: 0.5930849720082; l2 norm of gradients: 0.4457548726205769; l2 norm of weights: 0.6254226782759953\n","Iteration #2561  Loss: 0.592879744804852; l2 norm of gradients: 0.4452632021998261; l2 norm of weights: 0.6250868944300904\n","Iteration #2562  Loss: 0.5926749556568661; l2 norm of gradients: 0.4447720766046704; l2 norm of weights: 0.6247516283711655\n","Iteration #2563  Loss: 0.5924706036000026; l2 norm of gradients: 0.4442814952344996; l2 norm of weights: 0.6244168794423002\n","Iteration #2564  Loss: 0.5922666876721534; l2 norm of gradients: 0.4437914574893367; l2 norm of weights: 0.6240826469870927\n","Iteration #2565  Loss: 0.5920632069133374; l2 norm of gradients: 0.44330196276983663; l2 norm of weights: 0.62374893034966\n","Iteration #2566  Loss: 0.5918601603656952; l2 norm of gradients: 0.4428130104772863; l2 norm of weights: 0.6234157288746366\n","Iteration #2567  Loss: 0.591657547073486; l2 norm of gradients: 0.4423246000136036; l2 norm of weights: 0.6230830419071762\n","Iteration #2568  Loss: 0.5914553660830808; l2 norm of gradients: 0.4418367307813371; l2 norm of weights: 0.62275086879295\n","Iteration #2569  Loss: 0.591253616442959; l2 norm of gradients: 0.4413494021836658; l2 norm of weights: 0.6224192088781472\n","Iteration #2570  Loss: 0.591052297203704; l2 norm of gradients: 0.440862613624398; l2 norm of weights: 0.6220880615094749\n","Iteration #2571  Loss: 0.590851407417998; l2 norm of gradients: 0.4403763645079713; l2 norm of weights: 0.6217574260341585\n","Iteration #2572  Loss: 0.5906509461406174; l2 norm of gradients: 0.439890654239452; l2 norm of weights: 0.6214273017999404\n","Iteration #2573  Loss: 0.5904509124284285; l2 norm of gradients: 0.43940548222453435; l2 norm of weights: 0.6210976881550813\n","Iteration #2574  Loss: 0.5902513053403825; l2 norm of gradients: 0.43892084786954005; l2 norm of weights: 0.6207685844483589\n","Iteration #2575  Loss: 0.5900521239375116; l2 norm of gradients: 0.4384367505814181; l2 norm of weights: 0.6204399900290688\n","Iteration #2576  Loss: 0.5898533672829234; l2 norm of gradients: 0.43795318976774406; l2 norm of weights: 0.6201119042470232\n","Iteration #2577  Loss: 0.5896550344417973; l2 norm of gradients: 0.4374701648367193; l2 norm of weights: 0.6197843264525527\n","Iteration #2578  Loss: 0.5894571244813793; l2 norm of gradients: 0.43698767519717074; l2 norm of weights: 0.6194572559965038\n","Iteration #2579  Loss: 0.5892596364709779; l2 norm of gradients: 0.4365057202585503; l2 norm of weights: 0.6191306922302411\n","Iteration #2580  Loss: 0.5890625694819596; l2 norm of gradients: 0.43602429943093424; l2 norm of weights: 0.6188046345056458\n","Iteration #2581  Loss: 0.5888659225877432; l2 norm of gradients: 0.4355434121250232; l2 norm of weights: 0.6184790821751157\n","Iteration #2582  Loss: 0.588669694863797; l2 norm of gradients: 0.43506305775214055; l2 norm of weights: 0.6181540345915658\n","Iteration #2583  Loss: 0.5884738853876336; l2 norm of gradients: 0.434583235724233; l2 norm of weights: 0.6178294911084279\n","Iteration #2584  Loss: 0.5882784932388047; l2 norm of gradients: 0.4341039454538697; l2 norm of weights: 0.6175054510796503\n","Iteration #2585  Loss: 0.5880835174988978; l2 norm of gradients: 0.4336251863542413; l2 norm of weights: 0.6171819138596977\n","Iteration #2586  Loss: 0.5878889572515305; l2 norm of gradients: 0.43314695783916; l2 norm of weights: 0.6168588788035514\n","Iteration #2587  Loss: 0.5876948115823473; l2 norm of gradients: 0.4326692593230588; l2 norm of weights: 0.6165363452667093\n","Iteration #2588  Loss: 0.587501079579014; l2 norm of gradients: 0.43219209022099087; l2 norm of weights: 0.6162143126051853\n","Iteration #2589  Loss: 0.5873077603312142; l2 norm of gradients: 0.43171544994862937; l2 norm of weights: 0.6158927801755101\n","Iteration #2590  Loss: 0.5871148529306442; l2 norm of gradients: 0.4312393379222664; l2 norm of weights: 0.6155717473347299\n","Iteration #2591  Loss: 0.5869223564710084; l2 norm of gradients: 0.4307637535588131; l2 norm of weights: 0.6152512134404073\n","Iteration #2592  Loss: 0.586730270048016; l2 norm of gradients: 0.4302886962757986; l2 norm of weights: 0.614931177850621\n","Iteration #2593  Loss: 0.5865385927593751; l2 norm of gradients: 0.4298141654913696; l2 norm of weights: 0.6146116399239655\n","Iteration #2594  Loss: 0.5863473237047897; l2 norm of gradients: 0.4293401606242903; l2 norm of weights: 0.6142925990195511\n","Iteration #2595  Loss: 0.5861564619859542; l2 norm of gradients: 0.4288666810939413; l2 norm of weights: 0.6139740544970045\n","Iteration #2596  Loss: 0.5859660067065494; l2 norm of gradients: 0.42839372632031936; l2 norm of weights: 0.6136560057164671\n","Iteration #2597  Loss: 0.5857759569722386; l2 norm of gradients: 0.42792129572403675; l2 norm of weights: 0.6133384520385966\n","Iteration #2598  Loss: 0.5855863118906626; l2 norm of gradients: 0.42744938872632093; l2 norm of weights: 0.6130213928245665\n","Iteration #2599  Loss: 0.5853970705714354; l2 norm of gradients: 0.4269780047490138; l2 norm of weights: 0.612704827436065\n","Iteration #2600  Loss: 0.5852082321261404; l2 norm of gradients: 0.42650714321457134; l2 norm of weights: 0.6123887552352968\n","Iteration #2601  Loss: 0.5850197956683252; l2 norm of gradients: 0.4260368035460628; l2 norm of weights: 0.612073175584981\n","Iteration #2602  Loss: 0.5848317603134982; l2 norm of gradients: 0.42556698516717084; l2 norm of weights: 0.6117580878483526\n","Iteration #2603  Loss: 0.5846441251791233; l2 norm of gradients: 0.42509768750219024; l2 norm of weights: 0.6114434913891617\n","Iteration #2604  Loss: 0.5844568893846169; l2 norm of gradients: 0.42462890997602776; l2 norm of weights: 0.6111293855716735\n","Iteration #2605  Loss: 0.5842700520513423; l2 norm of gradients: 0.4241606520142016; l2 norm of weights: 0.6108157697606683\n","Iteration #2606  Loss: 0.5840836123026061; l2 norm of gradients: 0.4236929130428406; l2 norm of weights: 0.6105026433214419\n","Iteration #2607  Loss: 0.5838975692636535; l2 norm of gradients: 0.4232256924886845; l2 norm of weights: 0.6101900056198044\n","Iteration #2608  Loss: 0.5837119220616644; l2 norm of gradients: 0.42275898977908233; l2 norm of weights: 0.6098778560220812\n","Iteration #2609  Loss: 0.5835266698257495; l2 norm of gradients: 0.42229280434199257; l2 norm of weights: 0.6095661938951127\n","Iteration #2610  Loss: 0.5833418116869448; l2 norm of gradients: 0.4218271356059825; l2 norm of weights: 0.6092550186062536\n","Iteration #2611  Loss: 0.5831573467782085; l2 norm of gradients: 0.42136198300022787; l2 norm of weights: 0.6089443295233742\n","Iteration #2612  Loss: 0.5829732742344165; l2 norm of gradients: 0.42089734595451167; l2 norm of weights: 0.6086341260148587\n","Iteration #2613  Loss: 0.582789593192358; l2 norm of gradients: 0.42043322389922444; l2 norm of weights: 0.6083244074496063\n","Iteration #2614  Loss: 0.5826063027907312; l2 norm of gradients: 0.41996961626536333; l2 norm of weights: 0.6080151731970305\n","Iteration #2615  Loss: 0.5824234021701392; l2 norm of gradients: 0.4195065224845318; l2 norm of weights: 0.6077064226270598\n","Iteration #2616  Loss: 0.5822408904730862; l2 norm of gradients: 0.41904394198893863; l2 norm of weights: 0.6073981551101368\n","Iteration #2617  Loss: 0.5820587668439728; l2 norm of gradients: 0.4185818742113977; l2 norm of weights: 0.6070903700172188\n","Iteration #2618  Loss: 0.5818770304290917; l2 norm of gradients: 0.4181203185853277; l2 norm of weights: 0.6067830667197771\n","Iteration #2619  Loss: 0.5816956803766239; l2 norm of gradients: 0.41765927454475144; l2 norm of weights: 0.6064762445897975\n","Iteration #2620  Loss: 0.5815147158366347; l2 norm of gradients: 0.417198741524295; l2 norm of weights: 0.6061699029997804\n","Iteration #2621  Loss: 0.5813341359610693; l2 norm of gradients: 0.4167387189591874; l2 norm of weights: 0.6058640413227399\n","Iteration #2622  Loss: 0.5811539399037481; l2 norm of gradients: 0.4162792062852604; l2 norm of weights: 0.6055586589322042\n","Iteration #2623  Loss: 0.5809741268203636; l2 norm of gradients: 0.4158202029389475; l2 norm of weights: 0.605253755202216\n","Iteration #2624  Loss: 0.5807946958684759; l2 norm of gradients: 0.41536170835728375; l2 norm of weights: 0.6049493295073322\n","Iteration #2625  Loss: 0.580615646207508; l2 norm of gradients: 0.4149037219779051; l2 norm of weights: 0.6046453812226231\n","Iteration #2626  Loss: 0.5804369769987423; l2 norm of gradients: 0.41444624323904766; l2 norm of weights: 0.6043419097236734\n","Iteration #2627  Loss: 0.5802586874053166; l2 norm of gradients: 0.4139892715795477; l2 norm of weights: 0.6040389143865817\n","Iteration #2628  Loss: 0.5800807765922196; l2 norm of gradients: 0.4135328064388406; l2 norm of weights: 0.6037363945879605\n","Iteration #2629  Loss: 0.5799032437262869; l2 norm of gradients: 0.4130768472569605; l2 norm of weights: 0.6034343497049359\n","Iteration #2630  Loss: 0.5797260879761972; l2 norm of gradients: 0.41262139347453997; l2 norm of weights: 0.6031327791151478\n","Iteration #2631  Loss: 0.5795493085124679; l2 norm of gradients: 0.4121664445328091; l2 norm of weights: 0.6028316821967503\n","Iteration #2632  Loss: 0.5793729045074512; l2 norm of gradients: 0.4117119998735955; l2 norm of weights: 0.6025310583284106\n","Iteration #2633  Loss: 0.57919687513533; l2 norm of gradients: 0.4112580589393231; l2 norm of weights: 0.6022309068893099\n","Iteration #2634  Loss: 0.5790212195721144; l2 norm of gradients: 0.41080462117301236; l2 norm of weights: 0.6019312272591432\n","Iteration #2635  Loss: 0.5788459369956362; l2 norm of gradients: 0.4103516860182789; l2 norm of weights: 0.6016320188181185\n","Iteration #2636  Loss: 0.5786710265855471; l2 norm of gradients: 0.40989925291933405; l2 norm of weights: 0.6013332809469579\n","Iteration #2637  Loss: 0.5784964875233124; l2 norm of gradients: 0.40944732132098327; l2 norm of weights: 0.6010350130268968\n","Iteration #2638  Loss: 0.5783223189922085; l2 norm of gradients: 0.4089958906686261; l2 norm of weights: 0.6007372144396839\n","Iteration #2639  Loss: 0.5781485201773184; l2 norm of gradients: 0.40854496040825583; l2 norm of weights: 0.6004398845675816\n","Iteration #2640  Loss: 0.577975090265528; l2 norm of gradients: 0.4080945299864586; l2 norm of weights: 0.6001430227933655\n","Iteration #2641  Loss: 0.5778020284455214; l2 norm of gradients: 0.40764459885041304; l2 norm of weights: 0.5998466285003248\n","Iteration #2642  Loss: 0.5776293339077777; l2 norm of gradients: 0.4071951664478896; l2 norm of weights: 0.5995507010722616\n","Iteration #2643  Loss: 0.5774570058445669; l2 norm of gradients: 0.40674623222725054; l2 norm of weights: 0.5992552398934916\n","Iteration #2644  Loss: 0.5772850434499457; l2 norm of gradients: 0.40629779563744856; l2 norm of weights: 0.5989602443488439\n","Iteration #2645  Loss: 0.5771134459197533; l2 norm of gradients: 0.405849856128027; l2 norm of weights: 0.5986657138236602\n","Iteration #2646  Loss: 0.5769422124516087; l2 norm of gradients: 0.40540241314911896; l2 norm of weights: 0.5983716477037959\n","Iteration #2647  Loss: 0.5767713422449051; l2 norm of gradients: 0.40495546615144673; l2 norm of weights: 0.5980780453756196\n","Iteration #2648  Loss: 0.5766008345008071; l2 norm of gradients: 0.4045090145863216; l2 norm of weights: 0.5977849062260125\n","Iteration #2649  Loss: 0.5764306884222468; l2 norm of gradients: 0.4040630579056431; l2 norm of weights: 0.5974922296423697\n","Iteration #2650  Loss: 0.576260903213919; l2 norm of gradients: 0.40361759556189825; l2 norm of weights: 0.5972000150125985\n","Iteration #2651  Loss: 0.5760914780822783; l2 norm of gradients: 0.40317262700816153; l2 norm of weights: 0.5969082617251196\n","Iteration #2652  Loss: 0.5759224122355346; l2 norm of gradients: 0.40272815169809406; l2 norm of weights: 0.5966169691688668\n","Iteration #2653  Loss: 0.5757537048836499; l2 norm of gradients: 0.40228416908594283; l2 norm of weights: 0.5963261367332867\n","Iteration #2654  Loss: 0.5755853552383332; l2 norm of gradients: 0.4018406786265408; l2 norm of weights: 0.5960357638083391\n","Iteration #2655  Loss: 0.5754173625130381; l2 norm of gradients: 0.4013976797753061; l2 norm of weights: 0.5957458497844963\n","Iteration #2656  Loss: 0.575249725922958; l2 norm of gradients: 0.40095517198824093; l2 norm of weights: 0.5954563940527438\n","Iteration #2657  Loss: 0.5750824446850226; l2 norm of gradients: 0.400513154721932; l2 norm of weights: 0.5951673960045797\n","Iteration #2658  Loss: 0.574915518017894; l2 norm of gradients: 0.40007162743354924; l2 norm of weights: 0.5948788550320153\n","Iteration #2659  Loss: 0.5747489451419628; l2 norm of gradients: 0.3996305895808458; l2 norm of weights: 0.5945907705275746\n","Iteration #2660  Loss: 0.5745827252793445; l2 norm of gradients: 0.3991900406221571; l2 norm of weights: 0.5943031418842939\n","Iteration #2661  Loss: 0.5744168576538756; l2 norm of gradients: 0.3987499800164007; l2 norm of weights: 0.594015968495723\n","Iteration #2662  Loss: 0.5742513414911092; l2 norm of gradients: 0.3983104072230754; l2 norm of weights: 0.5937292497559238\n","Iteration #2663  Loss: 0.5740861760183126; l2 norm of gradients: 0.39787132170226097; l2 norm of weights: 0.5934429850594714\n","Iteration #2664  Loss: 0.5739213604644621; l2 norm of gradients: 0.39743272291461745; l2 norm of weights: 0.5931571738014532\n","Iteration #2665  Loss: 0.5737568940602401; l2 norm of gradients: 0.3969946103213846; l2 norm of weights: 0.5928718153774694\n","Iteration #2666  Loss: 0.5735927760380308; l2 norm of gradients: 0.39655698338438183; l2 norm of weights: 0.5925869091836331\n","Iteration #2667  Loss: 0.573429005631917; l2 norm of gradients: 0.39611984156600716; l2 norm of weights: 0.5923024546165697\n","Iteration #2668  Loss: 0.573265582077676; l2 norm of gradients: 0.3956831843292366; l2 norm of weights: 0.5920184510734174\n","Iteration #2669  Loss: 0.5731025046127753; l2 norm of gradients: 0.39524701113762434; l2 norm of weights: 0.5917348979518269\n","Iteration #2670  Loss: 0.5729397724763703; l2 norm of gradients: 0.3948113214553014; l2 norm of weights: 0.5914517946499611\n","Iteration #2671  Loss: 0.5727773849092993; l2 norm of gradients: 0.3943761147469757; l2 norm of weights: 0.5911691405664963\n","Iteration #2672  Loss: 0.5726153411540803; l2 norm of gradients: 0.39394139047793103; l2 norm of weights: 0.5908869351006206\n","Iteration #2673  Loss: 0.5724536404549071; l2 norm of gradients: 0.39350714811402704; l2 norm of weights: 0.5906051776520347\n","Iteration #2674  Loss: 0.572292282057646; l2 norm of gradients: 0.39307338712169837; l2 norm of weights: 0.5903238676209522\n","Iteration #2675  Loss: 0.5721312652098316; l2 norm of gradients: 0.3926401069679543; l2 norm of weights: 0.5900430044080988\n","Iteration #2676  Loss: 0.5719705891606635; l2 norm of gradients: 0.392207307120378; l2 norm of weights: 0.5897625874147128\n","Iteration #2677  Loss: 0.5718102531610019; l2 norm of gradients: 0.39177498704712627; l2 norm of weights: 0.5894826160425445\n","Iteration #2678  Loss: 0.5716502564633654; l2 norm of gradients: 0.39134314621692884; l2 norm of weights: 0.5892030896938577\n","Iteration #2679  Loss: 0.5714905983219256; l2 norm of gradients: 0.390911784099088; l2 norm of weights: 0.5889240077714273\n","Iteration #2680  Loss: 0.571331277992505; l2 norm of gradients: 0.3904809001634777; l2 norm of weights: 0.5886453696785416\n","Iteration #2681  Loss: 0.5711722947325721; l2 norm of gradients: 0.3900504938805436; l2 norm of weights: 0.5883671748190008\n","Iteration #2682  Loss: 0.5710136478012385; l2 norm of gradients: 0.389620564721302; l2 norm of weights: 0.5880894225971175\n","Iteration #2683  Loss: 0.570855336459255; l2 norm of gradients: 0.38919111215733976; l2 norm of weights: 0.5878121124177167\n","Iteration #2684  Loss: 0.5706973599690082; l2 norm of gradients: 0.3887621356608134; l2 norm of weights: 0.587535243686136\n","Iteration #2685  Loss: 0.5705397175945165; l2 norm of gradients: 0.38833363470444887; l2 norm of weights: 0.5872588158082247\n","Iteration #2686  Loss: 0.5703824086014269; l2 norm of gradients: 0.3879056087615408; l2 norm of weights: 0.586982828190345\n","Iteration #2687  Loss: 0.5702254322570113; l2 norm of gradients: 0.387478057305952; l2 norm of weights: 0.5867072802393711\n","Iteration #2688  Loss: 0.5700687878301626; l2 norm of gradients: 0.3870509798121132; l2 norm of weights: 0.5864321713626897\n","Iteration #2689  Loss: 0.5699124745913916; l2 norm of gradients: 0.3866243757550222; l2 norm of weights: 0.5861575009681995\n","Iteration #2690  Loss: 0.5697564918128231; l2 norm of gradients: 0.3861982446102435; l2 norm of weights: 0.5858832684643118\n","Iteration #2691  Loss: 0.5696008387681922; l2 norm of gradients: 0.3857725858539076; l2 norm of weights: 0.5856094732599497\n","Iteration #2692  Loss: 0.5694455147328414; l2 norm of gradients: 0.3853473989627111; l2 norm of weights: 0.585336114764549\n","Iteration #2693  Loss: 0.5692905189837166; l2 norm of gradients: 0.384922683413915; l2 norm of weights: 0.5850631923880576\n","Iteration #2694  Loss: 0.5691358507993628; l2 norm of gradients: 0.3844984386853453; l2 norm of weights: 0.5847907055409354\n","Iteration #2695  Loss: 0.5689815094599224; l2 norm of gradients: 0.384074664255392; l2 norm of weights: 0.5845186536341548\n","Iteration #2696  Loss: 0.56882749424713; l2 norm of gradients: 0.38365135960300867; l2 norm of weights: 0.5842470360792003\n","Iteration #2697  Loss: 0.5686738044443095; l2 norm of gradients: 0.3832285242077118; l2 norm of weights: 0.5839758522880684\n","Iteration #2698  Loss: 0.5685204393363706; l2 norm of gradients: 0.38280615754958014; l2 norm of weights: 0.5837051016732682\n","Iteration #2699  Loss: 0.5683673982098056; l2 norm of gradients: 0.3823842591092546; l2 norm of weights: 0.5834347836478206\n","Iteration #2700  Loss: 0.5682146803526853; l2 norm of gradients: 0.3819628283679378; l2 norm of weights: 0.583164897625259\n","Iteration #2701  Loss: 0.5680622850546557; l2 norm of gradients: 0.3815418648073926; l2 norm of weights: 0.5828954430196286\n","Iteration #2702  Loss: 0.5679102116069351; l2 norm of gradients: 0.3811213679099428; l2 norm of weights: 0.5826264192454873\n","Iteration #2703  Loss: 0.5677584593023094; l2 norm of gradients: 0.38070133715847165; l2 norm of weights: 0.5823578257179044\n","Iteration #2704  Loss: 0.5676070274351298; l2 norm of gradients: 0.380281772036422; l2 norm of weights: 0.5820896618524619\n","Iteration #2705  Loss: 0.567455915301309; l2 norm of gradients: 0.37986267202779533; l2 norm of weights: 0.581821927065254\n","Iteration #2706  Loss: 0.5673051221983176; l2 norm of gradients: 0.37944403661715165; l2 norm of weights: 0.5815546207728867\n","Iteration #2707  Loss: 0.5671546474251805; l2 norm of gradients: 0.37902586528960824; l2 norm of weights: 0.581287742392478\n","Iteration #2708  Loss: 0.5670044902824739; l2 norm of gradients: 0.37860815753084015; l2 norm of weights: 0.5810212913416587\n","Iteration #2709  Loss: 0.5668546500723216; l2 norm of gradients: 0.3781909128270787; l2 norm of weights: 0.5807552670385712\n","Iteration #2710  Loss: 0.5667051260983915; l2 norm of gradients: 0.3777741306651116; l2 norm of weights: 0.5804896689018698\n","Iteration #2711  Loss: 0.5665559176658927; l2 norm of gradients: 0.3773578105322823; l2 norm of weights: 0.5802244963507216\n","Iteration #2712  Loss: 0.5664070240815712; l2 norm of gradients: 0.3769419519164891; l2 norm of weights: 0.579959748804805\n","Iteration #2713  Loss: 0.5662584446537072; l2 norm of gradients: 0.37652655430618504; l2 norm of weights: 0.5796954256843114\n","Iteration #2714  Loss: 0.5661101786921121; l2 norm of gradients: 0.3761116171903775; l2 norm of weights: 0.5794315264099438\n","Iteration #2715  Loss: 0.5659622255081236; l2 norm of gradients: 0.375697140058627; l2 norm of weights: 0.5791680504029169\n","Iteration #2716  Loss: 0.5658145844146037; l2 norm of gradients: 0.3752831224010474; l2 norm of weights: 0.5789049970849581\n","Iteration #2717  Loss: 0.5656672547259354; l2 norm of gradients: 0.37486956370830493; l2 norm of weights: 0.5786423658783069\n","Iteration #2718  Loss: 0.5655202357580177; l2 norm of gradients: 0.37445646347161776; l2 norm of weights: 0.5783801562057144\n","Iteration #2719  Loss: 0.5653735268282645; l2 norm of gradients: 0.3740438211827558; l2 norm of weights: 0.5781183674904441\n","Iteration #2720  Loss: 0.5652271272555994; l2 norm of gradients: 0.3736316363340397; l2 norm of weights: 0.5778569991562716\n","Iteration #2721  Loss: 0.5650810363604535; l2 norm of gradients: 0.3732199084183404; l2 norm of weights: 0.5775960506274846\n","Iteration #2722  Loss: 0.5649352534647614; l2 norm of gradients: 0.3728086369290791; l2 norm of weights: 0.5773355213288826\n","Iteration #2723  Loss: 0.5647897778919584; l2 norm of gradients: 0.3723978213602261; l2 norm of weights: 0.5770754106857773\n","Iteration #2724  Loss: 0.5646446089669768; l2 norm of gradients: 0.3719874612063007; l2 norm of weights: 0.5768157181239926\n","Iteration #2725  Loss: 0.5644997460162425; l2 norm of gradients: 0.37157755596237046; l2 norm of weights: 0.5765564430698644\n","Iteration #2726  Loss: 0.5643551883676724; l2 norm of gradients: 0.3711681051240508; l2 norm of weights: 0.5762975849502406\n","Iteration #2727  Loss: 0.56421093535067; l2 norm of gradients: 0.3707591081875045; l2 norm of weights: 0.5760391431924812\n","Iteration #2728  Loss: 0.5640669862961234; l2 norm of gradients: 0.37035056464944094; l2 norm of weights: 0.5757811172244582\n","Iteration #2729  Loss: 0.5639233405364009; l2 norm of gradients: 0.3699424740071159; l2 norm of weights: 0.5755235064745561\n","Iteration #2730  Loss: 0.5637799974053481; l2 norm of gradients: 0.3695348357583309; l2 norm of weights: 0.5752663103716706\n","Iteration #2731  Loss: 0.5636369562382851; l2 norm of gradients: 0.36912764940143244; l2 norm of weights: 0.5750095283452104\n","Iteration #2732  Loss: 0.5634942163720024; l2 norm of gradients: 0.3687209144353122; l2 norm of weights: 0.5747531598250953\n","Iteration #2733  Loss: 0.5633517771447584; l2 norm of gradients: 0.3683146303594054; l2 norm of weights: 0.574497204241758\n","Iteration #2734  Loss: 0.5632096378962756; l2 norm of gradients: 0.3679087966736913; l2 norm of weights: 0.5742416610261428\n","Iteration #2735  Loss: 0.563067797967738; l2 norm of gradients: 0.3675034128786923; l2 norm of weights: 0.5739865296097063\n","Iteration #2736  Loss: 0.5629262567017872; l2 norm of gradients: 0.36709847847547333; l2 norm of weights: 0.5737318094244169\n","Iteration #2737  Loss: 0.5627850134425193; l2 norm of gradients: 0.3666939929656414; l2 norm of weights: 0.5734774999027553\n","Iteration #2738  Loss: 0.5626440675354821; l2 norm of gradients: 0.366289955851345; l2 norm of weights: 0.573223600477714\n","Iteration #2739  Loss: 0.5625034183276717; l2 norm of gradients: 0.36588636663527385; l2 norm of weights: 0.5729701105827979\n","Iteration #2740  Loss: 0.5623630651675289; l2 norm of gradients: 0.3654832248206581; l2 norm of weights: 0.5727170296520236\n","Iteration #2741  Loss: 0.5622230074049366; l2 norm of gradients: 0.3650805299112678; l2 norm of weights: 0.5724643571199202\n","Iteration #2742  Loss: 0.562083244391216; l2 norm of gradients: 0.3646782814114128; l2 norm of weights: 0.572212092421528\n","Iteration #2743  Loss: 0.5619437754791243; l2 norm of gradients: 0.3642764788259417; l2 norm of weights: 0.5719602349924006\n","Iteration #2744  Loss: 0.5618046000228504; l2 norm of gradients: 0.36387512166024155; l2 norm of weights: 0.5717087842686025\n","Iteration #2745  Loss: 0.5616657173780127; l2 norm of gradients: 0.36347420942023734; l2 norm of weights: 0.571457739686711\n","Iteration #2746  Loss: 0.5615271269016552; l2 norm of gradients: 0.3630737416123916; l2 norm of weights: 0.5712071006838149\n","Iteration #2747  Loss: 0.5613888279522454; l2 norm of gradients: 0.3626737177437036; l2 norm of weights: 0.5709568666975158\n","Iteration #2748  Loss: 0.5612508198896693; l2 norm of gradients: 0.36227413732170904; l2 norm of weights: 0.5707070371659264\n","Iteration #2749  Loss: 0.5611131020752306; l2 norm of gradients: 0.3618749998544797; l2 norm of weights: 0.5704576115276724\n","Iteration #2750  Loss: 0.5609756738716457; l2 norm of gradients: 0.3614763048506223; l2 norm of weights: 0.5702085892218911\n","Iteration #2751  Loss: 0.5608385346430411; l2 norm of gradients: 0.3610780518192787; l2 norm of weights: 0.5699599696882317\n","Iteration #2752  Loss: 0.5607016837549511; l2 norm of gradients: 0.36068024027012485; l2 norm of weights: 0.5697117523668558\n","Iteration #2753  Loss: 0.5605651205743134; l2 norm of gradients: 0.3602828697133708; l2 norm of weights: 0.5694639366984368\n","Iteration #2754  Loss: 0.560428844469467; l2 norm of gradients: 0.3598859396597596; l2 norm of weights: 0.5692165221241605\n","Iteration #2755  Loss: 0.5602928548101485; l2 norm of gradients: 0.3594894496205671; l2 norm of weights: 0.5689695080857243\n","Iteration #2756  Loss: 0.5601571509674891; l2 norm of gradients: 0.3590933991076015; l2 norm of weights: 0.568722894025338\n","Iteration #2757  Loss: 0.560021732314012; l2 norm of gradients: 0.35869778763320265; l2 norm of weights: 0.5684766793857235\n","Iteration #2758  Loss: 0.5598865982236286; l2 norm of gradients: 0.35830261471024166; l2 norm of weights: 0.5682308636101145\n","Iteration #2759  Loss: 0.5597517480716359; l2 norm of gradients: 0.35790787985212025; l2 norm of weights: 0.567985446142257\n","Iteration #2760  Loss: 0.5596171812347134; l2 norm of gradients: 0.3575135825727704; l2 norm of weights: 0.5677404264264089\n","Iteration #2761  Loss: 0.55948289709092; l2 norm of gradients: 0.3571197223866536; l2 norm of weights: 0.5674958039073403\n","Iteration #2762  Loss: 0.5593488950196905; l2 norm of gradients: 0.35672629880876083; l2 norm of weights: 0.5672515780303332\n","Iteration #2763  Loss: 0.5592151744018335; l2 norm of gradients: 0.3563333113546113; l2 norm of weights: 0.5670077482411818\n","Iteration #2764  Loss: 0.5590817346195275; l2 norm of gradients: 0.35594075954025256; l2 norm of weights: 0.5667643139861925\n","Iteration #2765  Loss: 0.5589485750563182; l2 norm of gradients: 0.3555486428822598; l2 norm of weights: 0.5665212747121835\n","Iteration #2766  Loss: 0.5588156950971155; l2 norm of gradients: 0.3551569608977351; l2 norm of weights: 0.566278629866485\n","Iteration #2767  Loss: 0.5586830941281904; l2 norm of gradients: 0.35476571310430727; l2 norm of weights: 0.5660363788969398\n","Iteration #2768  Loss: 0.5585507715371723; l2 norm of gradients: 0.35437489902013114; l2 norm of weights: 0.5657945212519022\n","Iteration #2769  Loss: 0.5584187267130454; l2 norm of gradients: 0.3539845181638871; l2 norm of weights: 0.5655530563802388\n","Iteration #2770  Loss: 0.5582869590461459; l2 norm of gradients: 0.3535945700547805; l2 norm of weights: 0.5653119837313282\n","Iteration #2771  Loss: 0.5581554679281598; l2 norm of gradients: 0.3532050542125413; l2 norm of weights: 0.5650713027550613\n","Iteration #2772  Loss: 0.5580242527521184; l2 norm of gradients: 0.35281597015742333; l2 norm of weights: 0.5648310129018407\n","Iteration #2773  Loss: 0.5578933129123969; l2 norm of gradients: 0.35242731741020417; l2 norm of weights: 0.5645911136225814\n","Iteration #2774  Loss: 0.5577626478047101; l2 norm of gradients: 0.35203909549218404; l2 norm of weights: 0.5643516043687102\n","Iteration #2775  Loss: 0.5576322568261106; l2 norm of gradients: 0.35165130392518584; l2 norm of weights: 0.5641124845921662\n","Iteration #2776  Loss: 0.5575021393749846; l2 norm of gradients: 0.3512639422315544; l2 norm of weights: 0.5638737537454005\n","Iteration #2777  Loss: 0.5573722948510502; l2 norm of gradients: 0.350877009934156; l2 norm of weights: 0.5636354112813761\n","Iteration #2778  Loss: 0.5572427226553538; l2 norm of gradients: 0.3504905065563777; l2 norm of weights: 0.5633974566535681\n","Iteration #2779  Loss: 0.5571134221902668; l2 norm of gradients: 0.3501044316221272; l2 norm of weights: 0.5631598893159642\n","Iteration #2780  Loss: 0.5569843928594835; l2 norm of gradients: 0.3497187846558319; l2 norm of weights: 0.562922708723063\n","Iteration #2781  Loss: 0.5568556340680177; l2 norm of gradients: 0.3493335651824387; l2 norm of weights: 0.5626859143298765\n","Iteration #2782  Loss: 0.5567271452221997; l2 norm of gradients: 0.34894877272741337; l2 norm of weights: 0.5624495055919279\n","Iteration #2783  Loss: 0.5565989257296737; l2 norm of gradients: 0.34856440681674006; l2 norm of weights: 0.5622134819652527\n","Iteration #2784  Loss: 0.5564709749993946; l2 norm of gradients: 0.3481804669769208; l2 norm of weights: 0.5619778429063984\n","Iteration #2785  Loss: 0.5563432924416254; l2 norm of gradients: 0.3477969527349748; l2 norm of weights: 0.5617425878724247\n","Iteration #2786  Loss: 0.5562158774679339; l2 norm of gradients: 0.34741386361843835; l2 norm of weights: 0.5615077163209033\n","Iteration #2787  Loss: 0.55608872949119; l2 norm of gradients: 0.347031199155364; l2 norm of weights: 0.5612732277099178\n","Iteration #2788  Loss: 0.5559618479255632; l2 norm of gradients: 0.34664895887431996; l2 norm of weights: 0.561039121498064\n","Iteration #2789  Loss: 0.5558352321865193; l2 norm of gradients: 0.34626714230438993; l2 norm of weights: 0.5608053971444497\n","Iteration #2790  Loss: 0.5557088816908173; l2 norm of gradients: 0.34588574897517227; l2 norm of weights: 0.5605720541086947\n","Iteration #2791  Loss: 0.555582795856507; l2 norm of gradients: 0.3455047784167798; l2 norm of weights: 0.5603390918509311\n","Iteration #2792  Loss: 0.5554569741029262; l2 norm of gradients: 0.34512423015983895; l2 norm of weights: 0.5601065098318025\n","Iteration #2793  Loss: 0.5553314158506973; l2 norm of gradients: 0.3447441037354896; l2 norm of weights: 0.5598743075124651\n","Iteration #2794  Loss: 0.555206120521725; l2 norm of gradients: 0.34436439867538415; l2 norm of weights: 0.5596424843545869\n","Iteration #2795  Loss: 0.5550810875391933; l2 norm of gradients: 0.3439851145116875; l2 norm of weights: 0.5594110398203476\n","Iteration #2796  Loss: 0.5549563163275624; l2 norm of gradients: 0.3436062507770762; l2 norm of weights: 0.5591799733724395\n","Iteration #2797  Loss: 0.5548318063125663; l2 norm of gradients: 0.3432278070047381; l2 norm of weights: 0.5589492844740667\n","Iteration #2798  Loss: 0.5547075569212099; l2 norm of gradients: 0.3428497827283715; l2 norm of weights: 0.5587189725889451\n","Iteration #2799  Loss: 0.5545835675817654; l2 norm of gradients: 0.3424721774821854; l2 norm of weights: 0.5584890371813027\n","Iteration #2800  Loss: 0.5544598377237712; l2 norm of gradients: 0.342094990800898; l2 norm of weights: 0.5582594777158799\n","Iteration #2801  Loss: 0.554336366778027; l2 norm of gradients: 0.34171822221973724; l2 norm of weights: 0.5580302936579286\n","Iteration #2802  Loss: 0.5542131541765929; l2 norm of gradients: 0.3413418712744393; l2 norm of weights: 0.5578014844732129\n","Iteration #2803  Loss: 0.5540901993527851; l2 norm of gradients: 0.34096593750124893; l2 norm of weights: 0.5575730496280087\n","Iteration #2804  Loss: 0.5539675017411744; l2 norm of gradients: 0.34059042043691834; l2 norm of weights: 0.5573449885891041\n","Iteration #2805  Loss: 0.5538450607775823; l2 norm of gradients: 0.34021531961870694; l2 norm of weights: 0.5571173008237993\n","Iteration #2806  Loss: 0.5537228758990791; l2 norm of gradients: 0.33984063458438085; l2 norm of weights: 0.5568899857999061\n","Iteration #2807  Loss: 0.5536009465439806; l2 norm of gradients: 0.33946636487221254; l2 norm of weights: 0.5566630429857485\n","Iteration #2808  Loss: 0.5534792721518454; l2 norm of gradients: 0.33909251002097984; l2 norm of weights: 0.5564364718501624\n","Iteration #2809  Loss: 0.5533578521634726; l2 norm of gradients: 0.33871906956996606; l2 norm of weights: 0.5562102718624956\n","Iteration #2810  Loss: 0.5532366860208984; l2 norm of gradients: 0.33834604305895893; l2 norm of weights: 0.5559844424926081\n","Iteration #2811  Loss: 0.5531157731673938; l2 norm of gradients: 0.3379734300282504; l2 norm of weights: 0.5557589832108714\n","Iteration #2812  Loss: 0.5529951130474619; l2 norm of gradients: 0.33760123001863623; l2 norm of weights: 0.5555338934881693\n","Iteration #2813  Loss: 0.5528747051068347; l2 norm of gradients: 0.33722944257141513; l2 norm of weights: 0.5553091727958972\n","Iteration #2814  Loss: 0.552754548792471; l2 norm of gradients: 0.33685806722838846; l2 norm of weights: 0.5550848206059626\n","Iteration #2815  Loss: 0.5526346435525531; l2 norm of gradients: 0.33648710353185995; l2 norm of weights: 0.5548608363907851\n","Iteration #2816  Loss: 0.5525149888364845; l2 norm of gradients: 0.33611655102463467; l2 norm of weights: 0.5546372196232957\n","Iteration #2817  Loss: 0.5523955840948873; l2 norm of gradients: 0.3357464092500191; l2 norm of weights: 0.5544139697769378\n","Iteration #2818  Loss: 0.5522764287795989; l2 norm of gradients: 0.33537667775182023; l2 norm of weights: 0.5541910863256662\n","Iteration #2819  Loss: 0.5521575223436697; l2 norm of gradients: 0.33500735607434506; l2 norm of weights: 0.5539685687439478\n","Iteration #2820  Loss: 0.5520388642413606; l2 norm of gradients: 0.3346384437624005; l2 norm of weights: 0.5537464165067615\n","Iteration #2821  Loss: 0.5519204539281402; l2 norm of gradients: 0.3342699403612924; l2 norm of weights: 0.5535246290895975\n","Iteration #2822  Loss: 0.5518022908606813; l2 norm of gradients: 0.33390184541682516; l2 norm of weights: 0.5533032059684587\n","Iteration #2823  Loss: 0.5516843744968598; l2 norm of gradients: 0.3335341584753016; l2 norm of weights: 0.5530821466198589\n","Iteration #2824  Loss: 0.551566704295751; l2 norm of gradients: 0.3331668790835219; l2 norm of weights: 0.5528614505208244\n","Iteration #2825  Loss: 0.5514492797176269; l2 norm of gradients: 0.33280000678878346; l2 norm of weights: 0.5526411171488926\n","Iteration #2826  Loss: 0.551332100223954; l2 norm of gradients: 0.3324335411388804; l2 norm of weights: 0.5524211459821136\n","Iteration #2827  Loss: 0.5512151652773905; l2 norm of gradients: 0.3320674816821028; l2 norm of weights: 0.5522015364990484\n","Iteration #2828  Loss: 0.5510984743417829; l2 norm of gradients: 0.33170182796723646; l2 norm of weights: 0.5519822881787702\n","Iteration #2829  Loss: 0.5509820268821652; l2 norm of gradients: 0.33133657954356227; l2 norm of weights: 0.5517634005008637\n","Iteration #2830  Loss: 0.5508658223647545; l2 norm of gradients: 0.33097173596085583; l2 norm of weights: 0.5515448729454258\n","Iteration #2831  Loss: 0.5507498602569489; l2 norm of gradients: 0.33060729676938677; l2 norm of weights: 0.5513267049930646\n","Iteration #2832  Loss: 0.5506341400273256; l2 norm of gradients: 0.3302432615199184; l2 norm of weights: 0.5511088961249\n","Iteration #2833  Loss: 0.5505186611456367; l2 norm of gradients: 0.3298796297637072; l2 norm of weights: 0.5508914458225637\n","Iteration #2834  Loss: 0.5504034230828088; l2 norm of gradients: 0.32951640105250224; l2 norm of weights: 0.5506743535681989\n","Iteration #2835  Loss: 0.5502884253109384; l2 norm of gradients: 0.3291535749385446; l2 norm of weights: 0.5504576188444609\n","Iteration #2836  Loss: 0.5501736673032902; l2 norm of gradients: 0.32879115097456724; l2 norm of weights: 0.550241241134516\n","Iteration #2837  Loss: 0.5500591485342946; l2 norm of gradients: 0.3284291287137942; l2 norm of weights: 0.5500252199220426\n","Iteration #2838  Loss: 0.5499448684795447; l2 norm of gradients: 0.3280675077099401; l2 norm of weights: 0.5498095546912303\n","Iteration #2839  Loss: 0.5498308266157942; l2 norm of gradients: 0.3277062875172096; l2 norm of weights: 0.5495942449267807\n","Iteration #2840  Loss: 0.5497170224209547; l2 norm of gradients: 0.3273454676902973; l2 norm of weights: 0.5493792901139067\n","Iteration #2841  Loss: 0.5496034553740923; l2 norm of gradients: 0.3269850477843867; l2 norm of weights: 0.5491646897383328\n","Iteration #2842  Loss: 0.5494901249554265; l2 norm of gradients: 0.32662502735515014; l2 norm of weights: 0.5489504432862949\n","Iteration #2843  Loss: 0.5493770306463266; l2 norm of gradients: 0.326265405958748; l2 norm of weights: 0.5487365502445408\n","Iteration #2844  Loss: 0.5492641719293097; l2 norm of gradients: 0.3259061831518283; l2 norm of weights: 0.5485230101003293\n","Iteration #2845  Loss: 0.5491515482880374; l2 norm of gradients: 0.3255473584915265; l2 norm of weights: 0.5483098223414309\n","Iteration #2846  Loss: 0.5490391592073143; l2 norm of gradients: 0.32518893153546446; l2 norm of weights: 0.5480969864561277\n","Iteration #2847  Loss: 0.5489270041730847; l2 norm of gradients: 0.3248309018417503; l2 norm of weights: 0.5478845019332129\n","Iteration #2848  Loss: 0.5488150826724303; l2 norm of gradients: 0.3244732689689779; l2 norm of weights: 0.5476723682619913\n","Iteration #2849  Loss: 0.5487033941935678; l2 norm of gradients: 0.3241160324762262; l2 norm of weights: 0.547460584932279\n","Iteration #2850  Loss: 0.5485919382258463; l2 norm of gradients: 0.32375919192305913; l2 norm of weights: 0.5472491514344037\n","Iteration #2851  Loss: 0.5484807142597445; l2 norm of gradients: 0.3234027468695245; l2 norm of weights: 0.5470380672592038\n","Iteration #2852  Loss: 0.548369721786869; l2 norm of gradients: 0.3230466968761541; l2 norm of weights: 0.5468273318980299\n","Iteration #2853  Loss: 0.5482589602999509; l2 norm of gradients: 0.3226910415039627; l2 norm of weights: 0.5466169448427433\n","Iteration #2854  Loss: 0.5481484292928436; l2 norm of gradients: 0.3223357803144481; l2 norm of weights: 0.5464069055857165\n","Iteration #2855  Loss: 0.5480381282605206; l2 norm of gradients: 0.3219809128695901; l2 norm of weights: 0.5461972136198336\n","Iteration #2856  Loss: 0.5479280566990729; l2 norm of gradients: 0.3216264387318503; l2 norm of weights: 0.5459878684384899\n","Iteration #2857  Loss: 0.547818214105706; l2 norm of gradients: 0.32127235746417165; l2 norm of weights: 0.5457788695355914\n","Iteration #2858  Loss: 0.5477085999787383; l2 norm of gradients: 0.32091866862997787; l2 norm of weights: 0.5455702164055559\n","Iteration #2859  Loss: 0.5475992138175985; l2 norm of gradients: 0.3205653717931728; l2 norm of weights: 0.5453619085433119\n","Iteration #2860  Loss: 0.5474900551228217; l2 norm of gradients: 0.3202124665181404; l2 norm of weights: 0.5451539454442992\n","Iteration #2861  Loss: 0.5473811233960493; l2 norm of gradients: 0.31985995236974346; l2 norm of weights: 0.5449463266044685\n","Iteration #2862  Loss: 0.5472724181400246; l2 norm of gradients: 0.31950782891332397; l2 norm of weights: 0.5447390515202818\n","Iteration #2863  Loss: 0.5471639388585916; l2 norm of gradients: 0.31915609571470205; l2 norm of weights: 0.5445321196887121\n","Iteration #2864  Loss: 0.5470556850566916; l2 norm of gradients: 0.31880475234017575; l2 norm of weights: 0.5443255306072429\n","Iteration #2865  Loss: 0.5469476562403615; l2 norm of gradients: 0.3184537983565204; l2 norm of weights: 0.5441192837738695\n","Iteration #2866  Loss: 0.5468398519167309; l2 norm of gradients: 0.31810323333098817; l2 norm of weights: 0.5439133786870975\n","Iteration #2867  Loss: 0.5467322715940204; l2 norm of gradients: 0.3177530568313077; l2 norm of weights: 0.5437078148459437\n","Iteration #2868  Loss: 0.5466249147815377; l2 norm of gradients: 0.3174032684256834; l2 norm of weights: 0.5435025917499355\n","Iteration #2869  Loss: 0.5465177809896772; l2 norm of gradients: 0.317053867682795; l2 norm of weights: 0.5432977088991114\n","Iteration #2870  Loss: 0.5464108697299157; l2 norm of gradients: 0.31670485417179733; l2 norm of weights: 0.5430931657940206\n","Iteration #2871  Loss: 0.5463041805148116; l2 norm of gradients: 0.31635622746231956; l2 norm of weights: 0.5428889619357231\n","Iteration #2872  Loss: 0.546197712858001; l2 norm of gradients: 0.3160079871244647; l2 norm of weights: 0.5426850968257897\n","Iteration #2873  Loss: 0.5460914662741965; l2 norm of gradients: 0.3156601327288094; l2 norm of weights: 0.5424815699663017\n","Iteration #2874  Loss: 0.5459854402791845; l2 norm of gradients: 0.315312663846403; l2 norm of weights: 0.5422783808598514\n","Iteration #2875  Loss: 0.5458796343898222; l2 norm of gradients: 0.3149655800487676; l2 norm of weights: 0.5420755290095414\n","Iteration #2876  Loss: 0.5457740481240363; l2 norm of gradients: 0.31461888090789697; l2 norm of weights: 0.541873013918985\n","Iteration #2877  Loss: 0.5456686810008193; l2 norm of gradients: 0.3142725659962567; l2 norm of weights: 0.5416708350923063\n","Iteration #2878  Loss: 0.545563532540229; l2 norm of gradients: 0.3139266348867831; l2 norm of weights: 0.5414689920341397\n","Iteration #2879  Loss: 0.5454586022633838; l2 norm of gradients: 0.31358108715288335; l2 norm of weights: 0.54126748424963\n","Iteration #2880  Loss: 0.5453538896924623; l2 norm of gradients: 0.31323592236843434; l2 norm of weights: 0.5410663112444327\n","Iteration #2881  Loss: 0.5452493943507002; l2 norm of gradients: 0.3128911401077827; l2 norm of weights: 0.5408654725247137\n","Iteration #2882  Loss: 0.5451451157623879; l2 norm of gradients: 0.31254673994574406; l2 norm of weights: 0.5406649675971491\n","Iteration #2883  Loss: 0.5450410534528679; l2 norm of gradients: 0.31220272145760286; l2 norm of weights: 0.5404647959689254\n","Iteration #2884  Loss: 0.5449372069485334; l2 norm of gradients: 0.3118590842191113; l2 norm of weights: 0.5402649571477394\n","Iteration #2885  Loss: 0.544833575776825; l2 norm of gradients: 0.31151582780648945; l2 norm of weights: 0.5400654506417981\n","Iteration #2886  Loss: 0.5447301594662289; l2 norm of gradients: 0.3111729517964246; l2 norm of weights: 0.5398662759598191\n","Iteration #2887  Loss: 0.5446269575462743; l2 norm of gradients: 0.3108304557660705; l2 norm of weights: 0.5396674326110295\n","Iteration #2888  Loss: 0.5445239695475317; l2 norm of gradients: 0.31048833929304714; l2 norm of weights: 0.539468920105167\n","Iteration #2889  Loss: 0.5444211950016095; l2 norm of gradients: 0.3101466019554403; l2 norm of weights: 0.5392707379524794\n","Iteration #2890  Loss: 0.5443186334411525; l2 norm of gradients: 0.309805243331801; l2 norm of weights: 0.5390728856637244\n","Iteration #2891  Loss: 0.5442162843998398; l2 norm of gradients: 0.30946426300114477; l2 norm of weights: 0.5388753627501695\n","Iteration #2892  Loss: 0.5441141474123816; l2 norm of gradients: 0.3091236605429517; l2 norm of weights: 0.5386781687235924\n","Iteration #2893  Loss: 0.5440122220145182; l2 norm of gradients: 0.30878343553716553; l2 norm of weights: 0.538481303096281\n","Iteration #2894  Loss: 0.5439105077430161; l2 norm of gradients: 0.30844358756419327; l2 norm of weights: 0.5382847653810324\n","Iteration #2895  Loss: 0.543809004135667; l2 norm of gradients: 0.3081041162049048; l2 norm of weights: 0.538088555091154\n","Iteration #2896  Loss: 0.5437077107312854; l2 norm of gradients: 0.30776502104063236; l2 norm of weights: 0.537892671740463\n","Iteration #2897  Loss: 0.5436066270697055; l2 norm of gradients: 0.30742630165317003; l2 norm of weights: 0.5376971148432856\n","Iteration #2898  Loss: 0.5435057526917798; l2 norm of gradients: 0.30708795762477326; l2 norm of weights: 0.5375018839144586\n","Iteration #2899  Loss: 0.5434050871393766; l2 norm of gradients: 0.3067499885381585; l2 norm of weights: 0.5373069784693282\n","Iteration #2900  Loss: 0.5433046299553775; l2 norm of gradients: 0.30641239397650255; l2 norm of weights: 0.5371123980237494\n","Iteration #2901  Loss: 0.5432043806836755; l2 norm of gradients: 0.30607517352344205; l2 norm of weights: 0.5369181420940878\n","Iteration #2902  Loss: 0.5431043388691725; l2 norm of gradients: 0.3057383267630735; l2 norm of weights: 0.5367242101972179\n","Iteration #2903  Loss: 0.5430045040577771; l2 norm of gradients: 0.30540185327995195; l2 norm of weights: 0.5365306018505237\n","Iteration #2904  Loss: 0.5429048757964026; l2 norm of gradients: 0.30506575265909125; l2 norm of weights: 0.5363373165718984\n","Iteration #2905  Loss: 0.5428054536329647; l2 norm of gradients: 0.3047300244859633; l2 norm of weights: 0.536144353879745\n","Iteration #2906  Loss: 0.5427062371163787; l2 norm of gradients: 0.3043946683464975; l2 norm of weights: 0.5359517132929751\n","Iteration #2907  Loss: 0.5426072257965581; l2 norm of gradients: 0.3040596838270802; l2 norm of weights: 0.5357593943310099\n","Iteration #2908  Loss: 0.5425084192244121; l2 norm of gradients: 0.3037250705145547; l2 norm of weights: 0.5355673965137799\n","Iteration #2909  Loss: 0.5424098169518433; l2 norm of gradients: 0.3033908279962203; l2 norm of weights: 0.5353757193617243\n","Iteration #2910  Loss: 0.5423114185317455; l2 norm of gradients: 0.3030569558598318; l2 norm of weights: 0.5351843623957916\n","Iteration #2911  Loss: 0.5422132235180012; l2 norm of gradients: 0.3027234536935995; l2 norm of weights: 0.5349933251374389\n","Iteration #2912  Loss: 0.5421152314654806; l2 norm of gradients: 0.30239032108618824; l2 norm of weights: 0.534802607108633\n","Iteration #2913  Loss: 0.5420174419300379; l2 norm of gradients: 0.30205755762671704; l2 norm of weights: 0.5346122078318487\n","Iteration #2914  Loss: 0.5419198544685098; l2 norm of gradients: 0.30172516290475904; l2 norm of weights: 0.5344221268300701\n","Iteration #2915  Loss: 0.5418224686387135; l2 norm of gradients: 0.30139313651034044; l2 norm of weights: 0.5342323636267899\n","Iteration #2916  Loss: 0.5417252839994444; l2 norm of gradients: 0.3010614780339402; l2 norm of weights: 0.5340429177460094\n","Iteration #2917  Loss: 0.5416283001104737; l2 norm of gradients: 0.3007301870664898; l2 norm of weights: 0.5338537887122389\n","Iteration #2918  Loss: 0.5415315165325463; l2 norm of gradients: 0.3003992631993726; l2 norm of weights: 0.5336649760504967\n","Iteration #2919  Loss: 0.5414349328273791; l2 norm of gradients: 0.30006870602442337; l2 norm of weights: 0.5334764792863099\n","Iteration #2920  Loss: 0.5413385485576581; l2 norm of gradients: 0.29973851513392774; l2 norm of weights: 0.5332882979457141\n","Iteration #2921  Loss: 0.5412423632870368; l2 norm of gradients: 0.29940869012062205; l2 norm of weights: 0.5331004315552532\n","Iteration #2922  Loss: 0.5411463765801339; l2 norm of gradients: 0.29907923057769226; l2 norm of weights: 0.5329128796419793\n","Iteration #2923  Loss: 0.5410505880025313; l2 norm of gradients: 0.2987501360987743; l2 norm of weights: 0.5327256417334527\n","Iteration #2924  Loss: 0.5409549971207716; l2 norm of gradients: 0.29842140627795294; l2 norm of weights: 0.5325387173577424\n","Iteration #2925  Loss: 0.5408596035023561; l2 norm of gradients: 0.2980930407097616; l2 norm of weights: 0.5323521060434245\n","Iteration #2926  Loss: 0.5407644067157429; l2 norm of gradients: 0.2977650389891818; l2 norm of weights: 0.5321658073195843\n","Iteration #2927  Loss: 0.5406694063303448; l2 norm of gradients: 0.29743740071164276; l2 norm of weights: 0.5319798207158141\n","Iteration #2928  Loss: 0.540574601916527; l2 norm of gradients: 0.2971101254730209; l2 norm of weights: 0.5317941457622147\n","Iteration #2929  Loss: 0.5404799930456046; l2 norm of gradients: 0.2967832128696394; l2 norm of weights: 0.5316087819893945\n","Iteration #2930  Loss: 0.5403855792898412; l2 norm of gradients: 0.2964566624982675; l2 norm of weights: 0.5314237289284698\n","Iteration #2931  Loss: 0.5402913602224467; l2 norm of gradients: 0.2961304739561207; l2 norm of weights: 0.5312389861110641\n","Iteration #2932  Loss: 0.5401973354175749; l2 norm of gradients: 0.2958046468408592; l2 norm of weights: 0.5310545530693095\n","Iteration #2933  Loss: 0.5401035044503208; l2 norm of gradients: 0.29547918075058865; l2 norm of weights: 0.5308704293358445\n","Iteration #2934  Loss: 0.5400098668967205; l2 norm of gradients: 0.29515407528385873; l2 norm of weights: 0.5306866144438159\n","Iteration #2935  Loss: 0.5399164223337467; l2 norm of gradients: 0.29482933003966316; l2 norm of weights: 0.5305031079268777\n","Iteration #2936  Loss: 0.5398231703393086; l2 norm of gradients: 0.2945049446174391; l2 norm of weights: 0.5303199093191909\n","Iteration #2937  Loss: 0.5397301104922483; l2 norm of gradients: 0.29418091861706674; l2 norm of weights: 0.5301370181554241\n","Iteration #2938  Loss: 0.5396372423723401; l2 norm of gradients: 0.29385725163886883; l2 norm of weights: 0.529954433970753\n","Iteration #2939  Loss: 0.5395445655602871; l2 norm of gradients: 0.29353394328361015; l2 norm of weights: 0.5297721563008605\n","Iteration #2940  Loss: 0.5394520796377201; l2 norm of gradients: 0.29321099315249705; l2 norm of weights: 0.5295901846819361\n","Iteration #2941  Loss: 0.5393597841871953; l2 norm of gradients: 0.29288840084717715; l2 norm of weights: 0.5294085186506766\n","Iteration #2942  Loss: 0.539267678792192; l2 norm of gradients: 0.2925661659697386; l2 norm of weights: 0.5292271577442856\n","Iteration #2943  Loss: 0.539175763037111; l2 norm of gradients: 0.2922442881227099; l2 norm of weights: 0.5290461015004736\n","Iteration #2944  Loss: 0.539084036507272; l2 norm of gradients: 0.29192276690905933; l2 norm of weights: 0.5288653494574574\n","Iteration #2945  Loss: 0.5389924987889119; l2 norm of gradients: 0.2916016019321942; l2 norm of weights: 0.5286849011539609\n","Iteration #2946  Loss: 0.5389011494691829; l2 norm of gradients: 0.29128079279596086; l2 norm of weights: 0.5285047561292143\n","Iteration #2947  Loss: 0.5388099881361503; l2 norm of gradients: 0.2909603391046441; l2 norm of weights: 0.5283249139229542\n","Iteration #2948  Loss: 0.5387190143787901; l2 norm of gradients: 0.29064024046296644; l2 norm of weights: 0.5281453740754236\n","Iteration #2949  Loss: 0.5386282277869878; l2 norm of gradients: 0.290320496476088; l2 norm of weights: 0.5279661361273718\n","Iteration #2950  Loss: 0.5385376279515356; l2 norm of gradients: 0.29000110674960566; l2 norm of weights: 0.5277871996200543\n","Iteration #2951  Loss: 0.5384472144641309; l2 norm of gradients: 0.28968207088955317; l2 norm of weights: 0.5276085640952328\n","Iteration #2952  Loss: 0.5383569869173742; l2 norm of gradients: 0.2893633885024001; l2 norm of weights: 0.527430229095175\n","Iteration #2953  Loss: 0.5382669449047667; l2 norm of gradients: 0.2890450591950517; l2 norm of weights: 0.5272521941626546\n","Iteration #2954  Loss: 0.5381770880207088; l2 norm of gradients: 0.28872708257484825; l2 norm of weights: 0.5270744588409506\n","Iteration #2955  Loss: 0.5380874158604981; l2 norm of gradients: 0.288409458249565; l2 norm of weights: 0.5268970226738487\n","Iteration #2956  Loss: 0.5379979280203269; l2 norm of gradients: 0.2880921858274112; l2 norm of weights: 0.5267198852056397\n","Iteration #2957  Loss: 0.5379086240972807; l2 norm of gradients: 0.28777526491702987; l2 norm of weights: 0.5265430459811199\n","Iteration #2958  Loss: 0.537819503689336; l2 norm of gradients: 0.2874586951274975; l2 norm of weights: 0.5263665045455914\n","Iteration #2959  Loss: 0.5377305663953584; l2 norm of gradients: 0.2871424760683234; l2 norm of weights: 0.5261902604448614\n","Iteration #2960  Loss: 0.537641811815101; l2 norm of gradients: 0.28682660734944915; l2 norm of weights: 0.5260143132252426\n","Iteration #2961  Loss: 0.5375532395492011; l2 norm of gradients: 0.28651108858124846; l2 norm of weights: 0.525838662433553\n","Iteration #2962  Loss: 0.5374648491991807; l2 norm of gradients: 0.2861959193745264; l2 norm of weights: 0.5256633076171157\n","Iteration #2963  Loss: 0.5373766403674411; l2 norm of gradients: 0.285881099340519; l2 norm of weights: 0.5254882483237583\n","Iteration #2964  Loss: 0.5372886126572647; l2 norm of gradients: 0.28556662809089317; l2 norm of weights: 0.5253134841018141\n","Iteration #2965  Loss: 0.5372007656728103; l2 norm of gradients: 0.2852525052377457; l2 norm of weights: 0.5251390145001208\n","Iteration #2966  Loss: 0.5371130990191116; l2 norm of gradients: 0.28493873039360323; l2 norm of weights: 0.5249648390680207\n","Iteration #2967  Loss: 0.5370256123020771; l2 norm of gradients: 0.28462530317142126; l2 norm of weights: 0.5247909573553612\n","Iteration #2968  Loss: 0.536938305128486; l2 norm of gradients: 0.28431222318458454; l2 norm of weights: 0.5246173689124939\n","Iteration #2969  Loss: 0.5368511771059865; l2 norm of gradients: 0.2839994900469057; l2 norm of weights: 0.5244440732902748\n","Iteration #2970  Loss: 0.5367642278430957; l2 norm of gradients: 0.2836871033726256; l2 norm of weights: 0.5242710700400643\n","Iteration #2971  Loss: 0.5366774569491954; l2 norm of gradients: 0.28337506277641206; l2 norm of weights: 0.5240983587137273\n","Iteration #2972  Loss: 0.5365908640345318; l2 norm of gradients: 0.28306336787336034; l2 norm of weights: 0.5239259388636325\n","Iteration #2973  Loss: 0.5365044487102129; l2 norm of gradients: 0.2827520182789918; l2 norm of weights: 0.5237538100426526\n","Iteration #2974  Loss: 0.5364182105882062; l2 norm of gradients: 0.28244101360925417; l2 norm of weights: 0.5235819718041644\n","Iteration #2975  Loss: 0.5363321492813378; l2 norm of gradients: 0.28213035348052046; l2 norm of weights: 0.5234104237020485\n","Iteration #2976  Loss: 0.5362462644032897; l2 norm of gradients: 0.2818200375095891; l2 norm of weights: 0.5232391652906893\n","Iteration #2977  Loss: 0.5361605555685982; l2 norm of gradients: 0.281510065313683; l2 norm of weights: 0.5230681961249743\n","Iteration #2978  Loss: 0.5360750223926523; l2 norm of gradients: 0.2812004365104494; l2 norm of weights: 0.5228975157602953\n","Iteration #2979  Loss: 0.5359896644916906; l2 norm of gradients: 0.2808911507179595; l2 norm of weights: 0.5227271237525469\n","Iteration #2980  Loss: 0.5359044814828013; l2 norm of gradients: 0.2805822075547076; l2 norm of weights: 0.5225570196581271\n","Iteration #2981  Loss: 0.5358194729839187; l2 norm of gradients: 0.2802736066396108; l2 norm of weights: 0.522387203033937\n","Iteration #2982  Loss: 0.5357346386138222; l2 norm of gradients: 0.27996534759200914; l2 norm of weights: 0.5222176734373811\n","Iteration #2983  Loss: 0.5356499779921341; l2 norm of gradients: 0.27965743003166416; l2 norm of weights: 0.5220484304263666\n","Iteration #2984  Loss: 0.5355654907393176; l2 norm of gradients: 0.27934985357875913; l2 norm of weights: 0.5218794735593034\n","Iteration #2985  Loss: 0.5354811764766755; l2 norm of gradients: 0.2790426178538985; l2 norm of weights: 0.5217108023951045\n","Iteration #2986  Loss: 0.5353970348263475; l2 norm of gradients: 0.2787357224781073; l2 norm of weights: 0.521542416493185\n","Iteration #2987  Loss: 0.5353130654113092; l2 norm of gradients: 0.2784291670728307; l2 norm of weights: 0.5213743154134632\n","Iteration #2988  Loss: 0.5352292678553696; l2 norm of gradients: 0.27812295125993375; l2 norm of weights: 0.521206498716359\n","Iteration #2989  Loss: 0.5351456417831698; l2 norm of gradients: 0.27781707466170075; l2 norm of weights: 0.5210389659627951\n","Iteration #2990  Loss: 0.5350621868201804; l2 norm of gradients: 0.27751153690083485; l2 norm of weights: 0.5208717167141962\n","Iteration #2991  Loss: 0.5349789025927008; l2 norm of gradients: 0.27720633760045776; l2 norm of weights: 0.520704750532489\n","Iteration #2992  Loss: 0.5348957887278557; l2 norm of gradients: 0.276901476384109; l2 norm of weights: 0.520538066980102\n","Iteration #2993  Loss: 0.5348128448535954; l2 norm of gradients: 0.2765969528757456; l2 norm of weights: 0.5203716656199656\n","Iteration #2994  Loss: 0.5347300705986918; l2 norm of gradients: 0.27629276669974195; l2 norm of weights: 0.5202055460155122\n","Iteration #2995  Loss: 0.5346474655927382; l2 norm of gradients: 0.2759889174808888; l2 norm of weights: 0.5200397077306751\n","Iteration #2996  Loss: 0.5345650294661468; l2 norm of gradients: 0.2756854048443932; l2 norm of weights: 0.5198741503298894\n","Iteration #2997  Loss: 0.534482761850147; l2 norm of gradients: 0.27538222841587795; l2 norm of weights: 0.5197088733780917\n","Iteration #2998  Loss: 0.534400662376783; l2 norm of gradients: 0.2750793878213812; l2 norm of weights: 0.5195438764407191\n","Iteration #2999  Loss: 0.5343187306789133; l2 norm of gradients: 0.27477688268735606; l2 norm of weights: 0.5193791590837106\n","Iteration #3000  Loss: 0.5342369663902077; l2 norm of gradients: 0.2744747126406698; l2 norm of weights: 0.5192147208735055\n","Iteration #3001  Loss: 0.534155369145146; l2 norm of gradients: 0.27417287730860396; l2 norm of weights: 0.5190505613770442\n","Iteration #3002  Loss: 0.5340739385790164; l2 norm of gradients: 0.27387137631885344; l2 norm of weights: 0.5188866801617676\n","Iteration #3003  Loss: 0.5339926743279131; l2 norm of gradients: 0.2735702092995263; l2 norm of weights: 0.5187230767956176\n","Iteration #3004  Loss: 0.5339115760287348; l2 norm of gradients: 0.27326937587914335; l2 norm of weights: 0.5185597508470356\n","Iteration #3005  Loss: 0.5338306433191834; l2 norm of gradients: 0.27296887568663747; l2 norm of weights: 0.5183967018849642\n","Iteration #3006  Loss: 0.5337498758377616; l2 norm of gradients: 0.27266870835135354; l2 norm of weights: 0.5182339294788461\n","Iteration #3007  Loss: 0.5336692732237709; l2 norm of gradients: 0.27236887350304745; l2 norm of weights: 0.5180714331986231\n","Iteration #3008  Loss: 0.5335888351173111; l2 norm of gradients: 0.27206937077188637; l2 norm of weights: 0.5179092126147379\n","Iteration #3009  Loss: 0.5335085611592769; l2 norm of gradients: 0.27177019978844774; l2 norm of weights: 0.5177472672981325\n","Iteration #3010  Loss: 0.5334284509913575; l2 norm of gradients: 0.2714713601837191; l2 norm of weights: 0.5175855968202488\n","Iteration #3011  Loss: 0.5333485042560335; l2 norm of gradients: 0.2711728515890974; l2 norm of weights: 0.5174242007530279\n","Iteration #3012  Loss: 0.5332687205965769; l2 norm of gradients: 0.270874673636389; l2 norm of weights: 0.5172630786689104\n","Iteration #3013  Loss: 0.5331890996570473; l2 norm of gradients: 0.27057682595780885; l2 norm of weights: 0.517102230140836\n","Iteration #3014  Loss: 0.5331096410822921; l2 norm of gradients: 0.2702793081859801; l2 norm of weights: 0.5169416547422437\n","Iteration #3015  Loss: 0.5330303445179433; l2 norm of gradients: 0.26998211995393395; l2 norm of weights: 0.5167813520470714\n","Iteration #3016  Loss: 0.5329512096104163; l2 norm of gradients: 0.26968526089510886; l2 norm of weights: 0.5166213216297556\n","Iteration #3017  Loss: 0.5328722360069086; l2 norm of gradients: 0.2693887306433502; l2 norm of weights: 0.5164615630652317\n","Iteration #3018  Loss: 0.5327934233553969; l2 norm of gradients: 0.2690925288329101; l2 norm of weights: 0.5163020759289335\n","Iteration #3019  Loss: 0.5327147713046372; l2 norm of gradients: 0.2687966550984466; l2 norm of weights: 0.5161428597967933\n","Iteration #3020  Loss: 0.5326362795041609; l2 norm of gradients: 0.26850110907502334; l2 norm of weights: 0.5159839142452414\n","Iteration #3021  Loss: 0.5325579476042744; l2 norm of gradients: 0.2682058903981095; l2 norm of weights: 0.5158252388512062\n","Iteration #3022  Loss: 0.5324797752560582; l2 norm of gradients: 0.26791099870357876; l2 norm of weights: 0.5156668331921149\n","Iteration #3023  Loss: 0.5324017621113621; l2 norm of gradients: 0.26761643362770915; l2 norm of weights: 0.5155086968458911\n","Iteration #3024  Loss: 0.5323239078228077; l2 norm of gradients: 0.2673221948071829; l2 norm of weights: 0.5153508293909571\n","Iteration #3025  Loss: 0.5322462120437829; l2 norm of gradients: 0.2670282818790856; l2 norm of weights: 0.5151932304062324\n","Iteration #3026  Loss: 0.5321686744284426; l2 norm of gradients: 0.26673469448090587; l2 norm of weights: 0.515035899471134\n","Iteration #3027  Loss: 0.5320912946317059; l2 norm of gradients: 0.266441432250535; l2 norm of weights: 0.5148788361655757\n","Iteration #3028  Loss: 0.5320140723092548; l2 norm of gradients: 0.2661484948262663; l2 norm of weights: 0.5147220400699688\n","Iteration #3029  Loss: 0.5319370071175324; l2 norm of gradients: 0.2658558818467952; l2 norm of weights: 0.5145655107652215\n","Iteration #3030  Loss: 0.5318600987137416; l2 norm of gradients: 0.2655635929512182; l2 norm of weights: 0.5144092478327387\n","Iteration #3031  Loss: 0.5317833467558422; l2 norm of gradients: 0.2652716277790328; l2 norm of weights: 0.5142532508544216\n","Iteration #3032  Loss: 0.5317067509025506; l2 norm of gradients: 0.2649799859701369; l2 norm of weights: 0.5140975194126683\n","Iteration #3033  Loss: 0.531630310813338; l2 norm of gradients: 0.2646886671648286; l2 norm of weights: 0.5139420530903732\n","Iteration #3034  Loss: 0.5315540261484275; l2 norm of gradients: 0.2643976710038054; l2 norm of weights: 0.5137868514709266\n","Iteration #3035  Loss: 0.5314778965687936; l2 norm of gradients: 0.2641069971281643; l2 norm of weights: 0.5136319141382149\n","Iteration #3036  Loss: 0.5314019217361603; l2 norm of gradients: 0.2638166451794008; l2 norm of weights: 0.5134772406766204\n","Iteration #3037  Loss: 0.5313261013129993; l2 norm of gradients: 0.26352661479940875; l2 norm of weights: 0.513322830671021\n","Iteration #3038  Loss: 0.531250434962528; l2 norm of gradients: 0.26323690563048; l2 norm of weights: 0.5131686837067903\n","Iteration #3039  Loss: 0.5311749223487086; l2 norm of gradients: 0.26294751731530386; l2 norm of weights: 0.5130147993697971\n","Iteration #3040  Loss: 0.5310995631362458; l2 norm of gradients: 0.26265844949696665; l2 norm of weights: 0.5128611772464053\n","Iteration #3041  Loss: 0.5310243569905858; l2 norm of gradients: 0.2623697018189513; l2 norm of weights: 0.5127078169234743\n","Iteration #3042  Loss: 0.5309493035779138; l2 norm of gradients: 0.262081273925137; l2 norm of weights: 0.5125547179883582\n","Iteration #3043  Loss: 0.5308744025651531; l2 norm of gradients: 0.2617931654597987; l2 norm of weights: 0.5124018800289054\n","Iteration #3044  Loss: 0.530799653619963; l2 norm of gradients: 0.26150537606760654; l2 norm of weights: 0.5122493026334595\n","Iteration #3045  Loss: 0.5307250564107373; l2 norm of gradients: 0.26121790539362594; l2 norm of weights: 0.5120969853908579\n","Iteration #3046  Loss: 0.5306506106066033; l2 norm of gradients: 0.2609307530833163; l2 norm of weights: 0.511944927890433\n","Iteration #3047  Loss: 0.5305763158774187; l2 norm of gradients: 0.26064391878253157; l2 norm of weights: 0.5117931297220103\n","Iteration #3048  Loss: 0.5305021718937719; l2 norm of gradients: 0.260357402137519; l2 norm of weights: 0.5116415904759102\n","Iteration #3049  Loss: 0.5304281783269783; l2 norm of gradients: 0.26007120279491935; l2 norm of weights: 0.5114903097429461\n","Iteration #3050  Loss: 0.5303543348490805; l2 norm of gradients: 0.25978532040176594; l2 norm of weights: 0.5113392871144252\n","Iteration #3051  Loss: 0.5302806411328456; l2 norm of gradients: 0.2594997546054846; l2 norm of weights: 0.5111885221821484\n","Iteration #3052  Loss: 0.530207096851764; l2 norm of gradients: 0.259214505053893; l2 norm of weights: 0.5110380145384092\n","Iteration #3053  Loss: 0.5301337016800476; l2 norm of gradients: 0.2589295713952004; l2 norm of weights: 0.5108877637759948\n","Iteration #3054  Loss: 0.5300604552926289; l2 norm of gradients: 0.2586449532780072; l2 norm of weights: 0.5107377694881848\n","Iteration #3055  Loss: 0.5299873573651578; l2 norm of gradients: 0.25836065035130445; l2 norm of weights: 0.5105880312687519\n","Iteration #3056  Loss: 0.529914407574002; l2 norm of gradients: 0.2580766622644735; l2 norm of weights: 0.510438548711961\n","Iteration #3057  Loss: 0.5298416055962439; l2 norm of gradients: 0.25779298866728556; l2 norm of weights: 0.5102893214125697\n","Iteration #3058  Loss: 0.5297689511096796; l2 norm of gradients: 0.25750962920990106; l2 norm of weights: 0.5101403489658274\n","Iteration #3059  Loss: 0.5296964437928176; l2 norm of gradients: 0.25722658354286976; l2 norm of weights: 0.5099916309674758\n","Iteration #3060  Loss: 0.5296240833248764; l2 norm of gradients: 0.25694385131712966; l2 norm of weights: 0.5098431670137483\n","Iteration #3061  Loss: 0.5295518693857838; l2 norm of gradients: 0.2566614321840072; l2 norm of weights: 0.5096949567013703\n","Iteration #3062  Loss: 0.5294798016561746; l2 norm of gradients: 0.25637932579521644; l2 norm of weights: 0.5095469996275581\n","Iteration #3063  Loss: 0.5294078798173902; l2 norm of gradients: 0.25609753180285877; l2 norm of weights: 0.5093992953900199\n","Iteration #3064  Loss: 0.529336103551475; l2 norm of gradients: 0.2558160498594225; l2 norm of weights: 0.5092518435869543\n","Iteration #3065  Loss: 0.5292644725411766; l2 norm of gradients: 0.2555348796177825; l2 norm of weights: 0.5091046438170516\n","Iteration #3066  Loss: 0.5291929864699442; l2 norm of gradients: 0.25525402073119957; l2 norm of weights: 0.5089576956794925\n","Iteration #3067  Loss: 0.5291216450219257; l2 norm of gradients: 0.2549734728533203; l2 norm of weights: 0.508810998773948\n","Iteration #3068  Loss: 0.5290504478819675; l2 norm of gradients: 0.25469323563817636; l2 norm of weights: 0.5086645527005802\n","Iteration #3069  Loss: 0.5289793947356123; l2 norm of gradients: 0.2544133087401844; l2 norm of weights: 0.5085183570600407\n","Iteration #3070  Loss: 0.5289084852690975; l2 norm of gradients: 0.2541336918141455; l2 norm of weights: 0.5083724114534713\n","Iteration #3071  Loss: 0.5288377191693541; l2 norm of gradients: 0.2538543845152446; l2 norm of weights: 0.5082267154825041\n","Iteration #3072  Loss: 0.5287670961240049; l2 norm of gradients: 0.2535753864990502; l2 norm of weights: 0.5080812687492603\n","Iteration #3073  Loss: 0.528696615821363; l2 norm of gradients: 0.25329669742151406; l2 norm of weights: 0.5079360708563506\n","Iteration #3074  Loss: 0.5286262779504298; l2 norm of gradients: 0.2530183169389707; l2 norm of weights: 0.5077911214068753\n","Iteration #3075  Loss: 0.5285560822008945; l2 norm of gradients: 0.2527402447081369; l2 norm of weights: 0.5076464200044234\n","Iteration #3076  Loss: 0.5284860282631318; l2 norm of gradients: 0.2524624803861114; l2 norm of weights: 0.5075019662530731\n","Iteration #3077  Loss: 0.5284161158282005; l2 norm of gradients: 0.25218502363037437; l2 norm of weights: 0.5073577597573913\n","Iteration #3078  Loss: 0.5283463445878421; l2 norm of gradients: 0.25190787409878723; l2 norm of weights: 0.5072138001224329\n","Iteration #3079  Loss: 0.5282767142344791; l2 norm of gradients: 0.2516310314495919; l2 norm of weights: 0.5070700869537417\n","Iteration #3080  Loss: 0.5282072244612139; l2 norm of gradients: 0.2513544953414108; l2 norm of weights: 0.5069266198573493\n","Iteration #3081  Loss: 0.5281378749618268; l2 norm of gradients: 0.25107826543324585; l2 norm of weights: 0.5067833984397754\n","Iteration #3082  Loss: 0.5280686654307751; l2 norm of gradients: 0.25080234138447877; l2 norm of weights: 0.5066404223080273\n","Iteration #3083  Loss: 0.5279995955631905; l2 norm of gradients: 0.2505267228548701; l2 norm of weights: 0.5064976910695997\n","Iteration #3084  Loss: 0.527930665054879; l2 norm of gradients: 0.2502514095045591; l2 norm of weights: 0.5063552043324749\n","Iteration #3085  Loss: 0.5278618736023182; l2 norm of gradients: 0.24997640099406324; l2 norm of weights: 0.5062129617051223\n","Iteration #3086  Loss: 0.5277932209026568; l2 norm of gradients: 0.24970169698427774; l2 norm of weights: 0.5060709627964979\n","Iteration #3087  Loss: 0.5277247066537125; l2 norm of gradients: 0.2494272971364752; l2 norm of weights: 0.5059292072160447\n","Iteration #3088  Loss: 0.5276563305539704; l2 norm of gradients: 0.24915320111230535; l2 norm of weights: 0.5057876945736924\n","Iteration #3089  Loss: 0.5275880923025824; l2 norm of gradients: 0.24887940857379434; l2 norm of weights: 0.5056464244798567\n","Iteration #3090  Loss: 0.5275199915993644; l2 norm of gradients: 0.24860591918334451; l2 norm of weights: 0.5055053965454394\n","Iteration #3091  Loss: 0.5274520281447959; l2 norm of gradients: 0.24833273260373404; l2 norm of weights: 0.5053646103818286\n","Iteration #3092  Loss: 0.527384201640018; l2 norm of gradients: 0.24805984849811635; l2 norm of weights: 0.5052240656008976\n","Iteration #3093  Loss: 0.5273165117868324; l2 norm of gradients: 0.24778726653001995; l2 norm of weights: 0.5050837618150056\n","Iteration #3094  Loss: 0.5272489582876994; l2 norm of gradients: 0.2475149863633477; l2 norm of weights: 0.5049436986369971\n","Iteration #3095  Loss: 0.5271815408457363; l2 norm of gradients: 0.24724300766237683; l2 norm of weights: 0.5048038756802014\n","Iteration #3096  Loss: 0.5271142591647175; l2 norm of gradients: 0.246971330091758; l2 norm of weights: 0.5046642925584329\n","Iteration #3097  Loss: 0.5270471129490703; l2 norm of gradients: 0.2466999533165155; l2 norm of weights: 0.5045249488859906\n","Iteration #3098  Loss: 0.5269801019038762; l2 norm of gradients: 0.2464288770020463; l2 norm of weights: 0.5043858442776581\n","Iteration #3099  Loss: 0.5269132257348675; l2 norm of gradients: 0.24615810081411998; l2 norm of weights: 0.504246978348703\n","Iteration #3100  Loss: 0.5268464841484273; l2 norm of gradients: 0.2458876244188782; l2 norm of weights: 0.5041083507148774\n","Iteration #3101  Loss: 0.5267798768515866; l2 norm of gradients: 0.24561744748283434; l2 norm of weights: 0.5039699609924164\n","Iteration #3102  Loss: 0.5267134035520242; l2 norm of gradients: 0.24534756967287302; l2 norm of weights: 0.5038318087980399\n","Iteration #3103  Loss: 0.5266470639580645; l2 norm of gradients: 0.24507799065624988; l2 norm of weights: 0.50369389374895\n","Iteration #3104  Loss: 0.5265808577786765; l2 norm of gradients: 0.2448087101005909; l2 norm of weights: 0.5035562154628328\n","Iteration #3105  Loss: 0.5265147847234715; l2 norm of gradients: 0.24453972767389226; l2 norm of weights: 0.5034187735578571\n","Iteration #3106  Loss: 0.5264488445027031; l2 norm of gradients: 0.24427104304451977; l2 norm of weights: 0.5032815676526746\n","Iteration #3107  Loss: 0.5263830368272644; l2 norm of gradients: 0.24400265588120848; l2 norm of weights: 0.5031445973664191\n","Iteration #3108  Loss: 0.5263173614086875; l2 norm of gradients: 0.2437345658530624; l2 norm of weights: 0.5030078623187073\n","Iteration #3109  Loss: 0.5262518179591413; l2 norm of gradients: 0.24346677262955405; l2 norm of weights: 0.5028713621296376\n","Iteration #3110  Loss: 0.5261864061914312; l2 norm of gradients: 0.2431992758805239; l2 norm of weights: 0.5027350964197904\n","Iteration #3111  Loss: 0.5261211258189967; l2 norm of gradients: 0.24293207527618021; l2 norm of weights: 0.5025990648102281\n","Iteration #3112  Loss: 0.52605597655591; l2 norm of gradients: 0.24266517048709846; l2 norm of weights: 0.5024632669224938\n","Iteration #3113  Loss: 0.5259909581168754; l2 norm of gradients: 0.24239856118422104; l2 norm of weights: 0.5023277023786124\n","Iteration #3114  Loss: 0.5259260702172273; l2 norm of gradients: 0.24213224703885677; l2 norm of weights: 0.5021923708010896\n","Iteration #3115  Loss: 0.5258613125729285; l2 norm of gradients: 0.24186622772268068; l2 norm of weights: 0.5020572718129118\n","Iteration #3116  Loss: 0.5257966849005699; l2 norm of gradients: 0.24160050290773336; l2 norm of weights: 0.5019224050375464\n","Iteration #3117  Loss: 0.5257321869173679; l2 norm of gradients: 0.24133507226642065; l2 norm of weights: 0.5017877700989404\n","Iteration #3118  Loss: 0.5256678183411638; l2 norm of gradients: 0.2410699354715134; l2 norm of weights: 0.5016533666215214\n","Iteration #3119  Loss: 0.5256035788904222; l2 norm of gradients: 0.24080509219614687; l2 norm of weights: 0.5015191942301965\n","Iteration #3120  Loss: 0.5255394682842291; l2 norm of gradients: 0.24054054211382045; l2 norm of weights: 0.5013852525503529\n","Iteration #3121  Loss: 0.5254754862422917; l2 norm of gradients: 0.24027628489839722; l2 norm of weights: 0.501251541207857\n","Iteration #3122  Loss: 0.5254116324849359; l2 norm of gradients: 0.24001232022410357; l2 norm of weights: 0.501118059829054\n","Iteration #3123  Loss: 0.5253479067331053; l2 norm of gradients: 0.2397486477655287; l2 norm of weights: 0.5009848080407686\n","Iteration #3124  Loss: 0.5252843087083601; l2 norm of gradients: 0.2394852671976244; l2 norm of weights: 0.500851785470304\n","Iteration #3125  Loss: 0.5252208381328751; l2 norm of gradients: 0.2392221781957046; l2 norm of weights: 0.5007189917454417\n","Iteration #3126  Loss: 0.5251574947294393; l2 norm of gradients: 0.2389593804354449; l2 norm of weights: 0.5005864264944415\n","Iteration #3127  Loss: 0.5250942782214538; l2 norm of gradients: 0.23869687359288222; l2 norm of weights: 0.5004540893460416\n","Iteration #3128  Loss: 0.5250311883329304; l2 norm of gradients: 0.2384346573444144; l2 norm of weights: 0.5003219799294576\n","Iteration #3129  Loss: 0.5249682247884906; l2 norm of gradients: 0.23817273136679987; l2 norm of weights: 0.5001900978743827\n","Iteration #3130  Loss: 0.524905387313364; l2 norm of gradients: 0.23791109533715718; l2 norm of weights: 0.5000584428109874\n","Iteration #3131  Loss: 0.5248426756333876; l2 norm of gradients: 0.23764974893296456; l2 norm of weights: 0.4999270143699193\n","Iteration #3132  Loss: 0.5247800894750031; l2 norm of gradients: 0.2373886918320597; l2 norm of weights: 0.49979581218230285\n","Iteration #3133  Loss: 0.5247176285652569; l2 norm of gradients: 0.23712792371263922; l2 norm of weights: 0.499664835879739\n","Iteration #3134  Loss: 0.5246552926317986; l2 norm of gradients: 0.23686744425325823; l2 norm of weights: 0.4995340850943052\n","Iteration #3135  Loss: 0.5245930814028782; l2 norm of gradients: 0.23660725313283015; l2 norm of weights: 0.49940355945855486\n","Iteration #3136  Loss: 0.5245309946073469; l2 norm of gradients: 0.23634735003062615; l2 norm of weights: 0.49927325860551725\n","Iteration #3137  Loss: 0.5244690319746542; l2 norm of gradients: 0.23608773462627475; l2 norm of weights: 0.4991431821686974\n","Iteration #3138  Loss: 0.5244071932348473; l2 norm of gradients: 0.2358284065997615; l2 norm of weights: 0.4990133297820756\n","Iteration #3139  Loss: 0.5243454781185695; l2 norm of gradients: 0.23556936563142858; l2 norm of weights: 0.49888370108010727\n","Iteration #3140  Loss: 0.5242838863570591; l2 norm of gradients: 0.23531061140197448; l2 norm of weights: 0.49875429569772284\n","Iteration #3141  Loss: 0.524222417682148; l2 norm of gradients: 0.23505214359245333; l2 norm of weights: 0.49862511327032727\n","Iteration #3142  Loss: 0.5241610718262597; l2 norm of gradients: 0.23479396188427487; l2 norm of weights: 0.4984961534337998\n","Iteration #3143  Loss: 0.5240998485224095; l2 norm of gradients: 0.23453606595920395; l2 norm of weights: 0.4983674158244942\n","Iteration #3144  Loss: 0.5240387475042019; l2 norm of gradients: 0.23427845549935988; l2 norm of weights: 0.49823890007923777\n","Iteration #3145  Loss: 0.5239777685058296; l2 norm of gradients: 0.23402113018721651; l2 norm of weights: 0.4981106058353317\n","Iteration #3146  Loss: 0.5239169112620724; l2 norm of gradients: 0.23376408970560142; l2 norm of weights: 0.49798253273055026\n","Iteration #3147  Loss: 0.5238561755082957; l2 norm of gradients: 0.23350733373769572; l2 norm of weights: 0.4978546804031413\n","Iteration #3148  Loss: 0.5237955609804494; l2 norm of gradients: 0.23325086196703373; l2 norm of weights: 0.4977270484918253\n","Iteration #3149  Loss: 0.5237350674150667; l2 norm of gradients: 0.2329946740775025; l2 norm of weights: 0.4975996366357954\n","Iteration #3150  Loss: 0.523674694549262; l2 norm of gradients: 0.23273876975334126; l2 norm of weights: 0.4974724444747171\n","Iteration #3151  Loss: 0.5236144421207308; l2 norm of gradients: 0.2324831486791415; l2 norm of weights: 0.4973454716487284\n","Iteration #3152  Loss: 0.5235543098677475; l2 norm of gradients: 0.23222781053984604; l2 norm of weights: 0.4972187177984385\n","Iteration #3153  Loss: 0.5234942975291648; l2 norm of gradients: 0.23197275502074902; l2 norm of weights: 0.497092182564929\n","Iteration #3154  Loss: 0.5234344048444113; l2 norm of gradients: 0.23171798180749548; l2 norm of weights: 0.49696586558975225\n","Iteration #3155  Loss: 0.5233746315534918; l2 norm of gradients: 0.23146349058608065; l2 norm of weights: 0.49683976651493206\n","Iteration #3156  Loss: 0.5233149773969851; l2 norm of gradients: 0.23120928104285; l2 norm of weights: 0.496713884982963\n","Iteration #3157  Loss: 0.5232554421160421; l2 norm of gradients: 0.23095535286449861; l2 norm of weights: 0.4965882206368102\n","Iteration #3158  Loss: 0.5231960254523862; l2 norm of gradients: 0.23070170573807086; l2 norm of weights: 0.49646277311990916\n","Iteration #3159  Loss: 0.5231367271483104; l2 norm of gradients: 0.23044833935096007; l2 norm of weights: 0.4963375420761655\n","Iteration #3160  Loss: 0.523077546946677; l2 norm of gradients: 0.23019525339090788; l2 norm of weights: 0.4962125271499547\n","Iteration #3161  Loss: 0.5230184845909162; l2 norm of gradients: 0.22994244754600435; l2 norm of weights: 0.49608772798612155\n","Iteration #3162  Loss: 0.5229595398250245; l2 norm of gradients: 0.2296899215046871; l2 norm of weights: 0.49596314422998045\n","Iteration #3163  Loss: 0.5229007123935637; l2 norm of gradients: 0.2294376749557411; l2 norm of weights: 0.49583877552731453\n","Iteration #3164  Loss: 0.5228420020416595; l2 norm of gradients: 0.22918570758829848; l2 norm of weights: 0.495714621524376\n","Iteration #3165  Loss: 0.5227834085150008; l2 norm of gradients: 0.228934019091838; l2 norm of weights: 0.49559068186788535\n","Iteration #3166  Loss: 0.5227249315598373; l2 norm of gradients: 0.22868260915618435; l2 norm of weights: 0.4954669562050313\n","Iteration #3167  Loss: 0.5226665709229794; l2 norm of gradients: 0.22843147747150833; l2 norm of weights: 0.49534344418347065\n","Iteration #3168  Loss: 0.5226083263517963; l2 norm of gradients: 0.2281806237283262; l2 norm of weights: 0.4952201454513277\n","Iteration #3169  Loss: 0.5225501975942154; l2 norm of gradients: 0.22793004761749927; l2 norm of weights: 0.49509705965719464\n","Iteration #3170  Loss: 0.52249218439872; l2 norm of gradients: 0.22767974883023354; l2 norm of weights: 0.49497418645013014\n","Iteration #3171  Loss: 0.5224342865143493; l2 norm of gradients: 0.22742972705807937; l2 norm of weights: 0.4948515254796603\n","Iteration #3172  Loss: 0.522376503690696; l2 norm of gradients: 0.22717998199293116; l2 norm of weights: 0.49472907639577784\n","Iteration #3173  Loss: 0.5223188356779062; l2 norm of gradients: 0.22693051332702677; l2 norm of weights: 0.49460683884894135\n","Iteration #3174  Loss: 0.5222612822266771; l2 norm of gradients: 0.22668132075294736; l2 norm of weights: 0.4944848124900761\n","Iteration #3175  Loss: 0.5222038430882567; l2 norm of gradients: 0.22643240396361686; l2 norm of weights: 0.4943629969705726\n","Iteration #3176  Loss: 0.5221465180144419; l2 norm of gradients: 0.22618376265230172; l2 norm of weights: 0.4942413919422874\n","Iteration #3177  Loss: 0.5220893067575776; l2 norm of gradients: 0.2259353965126104; l2 norm of weights: 0.49411999705754195\n","Iteration #3178  Loss: 0.5220322090705556; l2 norm of gradients: 0.22568730523849312; l2 norm of weights: 0.4939988119691228\n","Iteration #3179  Loss: 0.5219752247068132; l2 norm of gradients: 0.2254394885242414; l2 norm of weights: 0.49387783633028126\n","Iteration #3180  Loss: 0.521918353420332; l2 norm of gradients: 0.22519194606448767; l2 norm of weights: 0.49375706979473316\n","Iteration #3181  Loss: 0.5218615949656361; l2 norm of gradients: 0.22494467755420514; l2 norm of weights: 0.49363651201665804\n","Iteration #3182  Loss: 0.5218049490977926; l2 norm of gradients: 0.2246976826887069; l2 norm of weights: 0.49351616265069986\n","Iteration #3183  Loss: 0.5217484155724085; l2 norm of gradients: 0.22445096116364613; l2 norm of weights: 0.49339602135196586\n","Iteration #3184  Loss: 0.5216919941456305; l2 norm of gradients: 0.22420451267501545; l2 norm of weights: 0.4932760877760267\n","Iteration #3185  Loss: 0.5216356845741438; l2 norm of gradients: 0.2239583369191465; l2 norm of weights: 0.49315636157891607\n","Iteration #3186  Loss: 0.5215794866151705; l2 norm of gradients: 0.22371243359270954; l2 norm of weights: 0.4930368424171303\n","Iteration #3187  Loss: 0.5215234000264688; l2 norm of gradients: 0.22346680239271335; l2 norm of weights: 0.4929175299476284\n","Iteration #3188  Loss: 0.5214674245663313; l2 norm of gradients: 0.22322144301650465; l2 norm of weights: 0.49279842382783134\n","Iteration #3189  Loss: 0.5214115599935845; l2 norm of gradients: 0.22297635516176764; l2 norm of weights: 0.4926795237156222\n","Iteration #3190  Loss: 0.5213558060675871; l2 norm of gradients: 0.22273153852652391; l2 norm of weights: 0.4925608292693456\n","Iteration #3191  Loss: 0.5213001625482292; l2 norm of gradients: 0.22248699280913178; l2 norm of weights: 0.49244234014780736\n","Iteration #3192  Loss: 0.5212446291959304; l2 norm of gradients: 0.22224271770828607; l2 norm of weights: 0.4923240560102746\n","Iteration #3193  Loss: 0.5211892057716397; l2 norm of gradients: 0.22199871292301782; l2 norm of weights: 0.49220597651647513\n","Iteration #3194  Loss: 0.5211338920368336; l2 norm of gradients: 0.22175497815269374; l2 norm of weights: 0.4920881013265971\n","Iteration #3195  Loss: 0.5210786877535145; l2 norm of gradients: 0.22151151309701592; l2 norm of weights: 0.491970430101289\n","Iteration #3196  Loss: 0.5210235926842112; l2 norm of gradients: 0.2212683174560215; l2 norm of weights: 0.49185296250165905\n","Iteration #3197  Loss: 0.5209686065919757; l2 norm of gradients: 0.22102539093008217; l2 norm of weights: 0.49173569818927537\n","Iteration #3198  Loss: 0.5209137292403836; l2 norm of gradients: 0.22078273321990405; l2 norm of weights: 0.4916186368261651\n","Iteration #3199  Loss: 0.5208589603935322; l2 norm of gradients: 0.22054034402652703; l2 norm of weights: 0.4915017780748146\n","Iteration #3200  Loss: 0.5208042998160389; l2 norm of gradients: 0.22029822305132454; l2 norm of weights: 0.4913851215981689\n","Iteration #3201  Loss: 0.5207497472730417; l2 norm of gradients: 0.22005636999600334; l2 norm of weights: 0.4912686670596314\n","Iteration #3202  Loss: 0.520695302530196; l2 norm of gradients: 0.21981478456260287; l2 norm of weights: 0.49115241412306393\n","Iteration #3203  Loss: 0.520640965353675; l2 norm of gradients: 0.21957346645349496; l2 norm of weights: 0.4910363624527858\n","Iteration #3204  Loss: 0.5205867355101677; l2 norm of gradients: 0.21933241537138362; l2 norm of weights: 0.49092051171357415\n","Iteration #3205  Loss: 0.5205326127668779; l2 norm of gradients: 0.2190916310193045; l2 norm of weights: 0.49080486157066344\n","Iteration #3206  Loss: 0.5204785968915238; l2 norm of gradients: 0.21885111310062472; l2 norm of weights: 0.4906894116897447\n","Iteration #3207  Loss: 0.5204246876523355; l2 norm of gradients: 0.2186108613190422; l2 norm of weights: 0.4905741617369663\n","Iteration #3208  Loss: 0.5203708848180553; l2 norm of gradients: 0.2183708753785856; l2 norm of weights: 0.4904591113789323\n","Iteration #3209  Loss: 0.5203171881579352; l2 norm of gradients: 0.2181311549836138; l2 norm of weights: 0.4903442602827033\n","Iteration #3210  Loss: 0.520263597441737; l2 norm of gradients: 0.2178916998388155; l2 norm of weights: 0.49022960811579547\n","Iteration #3211  Loss: 0.5202101124397303; l2 norm of gradients: 0.21765250964920907; l2 norm of weights: 0.49011515454618065\n","Iteration #3212  Loss: 0.5201567329226917; l2 norm of gradients: 0.21741358412014186; l2 norm of weights: 0.4900008992422857\n","Iteration #3213  Loss: 0.5201034586619041; l2 norm of gradients: 0.21717492295729024; l2 norm of weights: 0.48988684187299236\n","Iteration #3214  Loss: 0.5200502894291544; l2 norm of gradients: 0.2169365258666588; l2 norm of weights: 0.48977298210763703\n","Iteration #3215  Loss: 0.5199972249967337; l2 norm of gradients: 0.2166983925545803; l2 norm of weights: 0.4896593196160105\n","Iteration #3216  Loss: 0.5199442651374356; l2 norm of gradients: 0.21646052272771524; l2 norm of weights: 0.48954585406835727\n","Iteration #3217  Loss: 0.5198914096245547; l2 norm of gradients: 0.21622291609305147; l2 norm of weights: 0.4894325851353757\n","Iteration #3218  Loss: 0.5198386582318864; l2 norm of gradients: 0.2159855723579037; l2 norm of weights: 0.4893195124882175\n","Iteration #3219  Loss: 0.5197860107337248; l2 norm of gradients: 0.2157484912299135; l2 norm of weights: 0.4892066357984874\n","Iteration #3220  Loss: 0.5197334669048621; l2 norm of gradients: 0.2155116724170485; l2 norm of weights: 0.4890939547382429\n","Iteration #3221  Loss: 0.519681026520588; l2 norm of gradients: 0.21527511562760243; l2 norm of weights: 0.488981468979994\n","Iteration #3222  Loss: 0.5196286893566874; l2 norm of gradients: 0.21503882057019436; l2 norm of weights: 0.4888691781967028\n","Iteration #3223  Loss: 0.5195764551894404; l2 norm of gradients: 0.21480278695376873; l2 norm of weights: 0.4887570820617832\n","Iteration #3224  Loss: 0.5195243237956205; l2 norm of gradients: 0.21456701448759474; l2 norm of weights: 0.48864518024910075\n","Iteration #3225  Loss: 0.5194722949524939; l2 norm of gradients: 0.21433150288126612; l2 norm of weights: 0.4885334724329721\n","Iteration #3226  Loss: 0.5194203684378185; l2 norm of gradients: 0.2140962518447006; l2 norm of weights: 0.4884219582881649\n","Iteration #3227  Loss: 0.519368544029842; l2 norm of gradients: 0.21386126108813988; l2 norm of weights: 0.48831063748989745\n","Iteration #3228  Loss: 0.5193168215073021; l2 norm of gradients: 0.2136265303221488; l2 norm of weights: 0.4881995097138381\n","Iteration #3229  Loss: 0.519265200649424; l2 norm of gradients: 0.2133920592576154; l2 norm of weights: 0.48808857463610533\n","Iteration #3230  Loss: 0.5192136812359209; l2 norm of gradients: 0.2131578476057505; l2 norm of weights: 0.48797783193326744\n","Iteration #3231  Loss: 0.5191622630469912; l2 norm of gradients: 0.21292389507808707; l2 norm of weights: 0.4878672812823418\n","Iteration #3232  Loss: 0.5191109458633185; l2 norm of gradients: 0.21269020138648015; l2 norm of weights: 0.487756922360795\n","Iteration #3233  Loss: 0.5190597294660709; l2 norm of gradients: 0.21245676624310644; l2 norm of weights: 0.4876467548465422\n","Iteration #3234  Loss: 0.5190086136368985; l2 norm of gradients: 0.21222358936046384; l2 norm of weights: 0.4875367784179473\n","Iteration #3235  Loss: 0.5189575981579337; l2 norm of gradients: 0.21199067045137124; l2 norm of weights: 0.48742699275382184\n","Iteration #3236  Loss: 0.5189066828117895; l2 norm of gradients: 0.21175800922896804; l2 norm of weights: 0.48731739753342557\n","Iteration #3237  Loss: 0.5188558673815582; l2 norm of gradients: 0.21152560540671372; l2 norm of weights: 0.48720799243646534\n","Iteration #3238  Loss: 0.5188051516508112; l2 norm of gradients: 0.21129345869838798; l2 norm of weights: 0.48709877714309535\n","Iteration #3239  Loss: 0.5187545354035968; l2 norm of gradients: 0.21106156881808963; l2 norm of weights: 0.48698975133391664\n","Iteration #3240  Loss: 0.5187040184244402; l2 norm of gradients: 0.21082993548023687; l2 norm of weights: 0.48688091468997663\n","Iteration #3241  Loss: 0.518653600498342; l2 norm of gradients: 0.21059855839956654; l2 norm of weights: 0.486772266892769\n","Iteration #3242  Loss: 0.5186032814107766; l2 norm of gradients: 0.2103674372911341; l2 norm of weights: 0.48666380762423317\n","Iteration #3243  Loss: 0.5185530609476923; l2 norm of gradients: 0.21013657187031293; l2 norm of weights: 0.48655553656675443\n","Iteration #3244  Loss: 0.5185029388955094; l2 norm of gradients: 0.20990596185279425; l2 norm of weights: 0.4864474534031629\n","Iteration #3245  Loss: 0.5184529150411192; l2 norm of gradients: 0.20967560695458665; l2 norm of weights: 0.48633955781673394\n","Iteration #3246  Loss: 0.5184029891718834; l2 norm of gradients: 0.20944550689201574; l2 norm of weights: 0.4862318494911872\n","Iteration #3247  Loss: 0.5183531610756327; l2 norm of gradients: 0.20921566138172384; l2 norm of weights: 0.48612432811068673\n","Iteration #3248  Loss: 0.5183034305406657; l2 norm of gradients: 0.20898607014066967; l2 norm of weights: 0.48601699335984055\n","Iteration #3249  Loss: 0.5182537973557485; l2 norm of gradients: 0.20875673288612773; l2 norm of weights: 0.4859098449237001\n","Iteration #3250  Loss: 0.5182042613101124; l2 norm of gradients: 0.20852764933568846; l2 norm of weights: 0.4858028824877605\n","Iteration #3251  Loss: 0.5181548221934545; l2 norm of gradients: 0.20829881920725735; l2 norm of weights: 0.4856961057379592\n","Iteration #3252  Loss: 0.5181054797959355; l2 norm of gradients: 0.20807024221905496; l2 norm of weights: 0.485589514360677\n","Iteration #3253  Loss: 0.5180562339081786; l2 norm of gradients: 0.2078419180896164; l2 norm of weights: 0.4854831080427364\n","Iteration #3254  Loss: 0.5180070843212691; l2 norm of gradients: 0.20761384653779105; l2 norm of weights: 0.4853768864714022\n","Iteration #3255  Loss: 0.5179580308267535; l2 norm of gradients: 0.20738602728274214; l2 norm of weights: 0.48527084933438064\n","Iteration #3256  Loss: 0.517909073216638; l2 norm of gradients: 0.20715846004394653; l2 norm of weights: 0.48516499631981946\n","Iteration #3257  Loss: 0.517860211283387; l2 norm of gradients: 0.20693114454119424; l2 norm of weights: 0.4850593271163074\n","Iteration #3258  Loss: 0.5178114448199236; l2 norm of gradients: 0.20670408049458813; l2 norm of weights: 0.48495384141287373\n","Iteration #3259  Loss: 0.5177627736196269; l2 norm of gradients: 0.2064772676245437; l2 norm of weights: 0.484848538898988\n","Iteration #3260  Loss: 0.517714197476332; l2 norm of gradients: 0.2062507056517884; l2 norm of weights: 0.4847434192645599\n","Iteration #3261  Loss: 0.5176657161843289; l2 norm of gradients: 0.2060243942973617; l2 norm of weights: 0.4846384821999387\n","Iteration #3262  Loss: 0.5176173295383615; l2 norm of gradients: 0.20579833328261452; l2 norm of weights: 0.484533727395913\n","Iteration #3263  Loss: 0.5175690373336258; l2 norm of gradients: 0.20557252232920875; l2 norm of weights: 0.48442915454371055\n","Iteration #3264  Loss: 0.5175208393657701; l2 norm of gradients: 0.2053469611591173; l2 norm of weights: 0.48432476333499747\n","Iteration #3265  Loss: 0.5174727354308932; l2 norm of gradients: 0.20512164949462336; l2 norm of weights: 0.4842205534618784\n","Iteration #3266  Loss: 0.5174247253255436; l2 norm of gradients: 0.2048965870583203; l2 norm of weights: 0.48411652461689597\n","Iteration #3267  Loss: 0.5173768088467187; l2 norm of gradients: 0.20467177357311128; l2 norm of weights: 0.4840126764930305\n","Iteration #3268  Loss: 0.5173289857918634; l2 norm of gradients: 0.2044472087622088; l2 norm of weights: 0.4839090087836993\n","Iteration #3269  Loss: 0.5172812559588696; l2 norm of gradients: 0.2042228923491344; l2 norm of weights: 0.4838055211827572\n","Iteration #3270  Loss: 0.5172336191460747; l2 norm of gradients: 0.20399882405771858; l2 norm of weights: 0.48370221338449537\n","Iteration #3271  Loss: 0.5171860751522612; l2 norm of gradients: 0.2037750036120999; l2 norm of weights: 0.4835990850836412\n","Iteration #3272  Loss: 0.517138623776655; l2 norm of gradients: 0.20355143073672527; l2 norm of weights: 0.4834961359753582\n","Iteration #3273  Loss: 0.5170912648189248; l2 norm of gradients: 0.20332810515634905; l2 norm of weights: 0.4833933657552455\n","Iteration #3274  Loss: 0.5170439980791817; l2 norm of gradients: 0.20310502659603322; l2 norm of weights: 0.4832907741193375\n","Iteration #3275  Loss: 0.5169968233579769; l2 norm of gradients: 0.2028821947811466; l2 norm of weights: 0.48318836076410354\n","Iteration #3276  Loss: 0.5169497404563018; l2 norm of gradients: 0.2026596094373648; l2 norm of weights: 0.48308612538644763\n","Iteration #3277  Loss: 0.5169027491755868; l2 norm of gradients: 0.20243727029066974; l2 norm of weights: 0.4829840676837079\n","Iteration #3278  Loss: 0.5168558493177; l2 norm of gradients: 0.20221517706734937; l2 norm of weights: 0.48288218735365657\n","Iteration #3279  Loss: 0.5168090406849466; l2 norm of gradients: 0.20199332949399723; l2 norm of weights: 0.48278048409449936\n","Iteration #3280  Loss: 0.5167623230800674; l2 norm of gradients: 0.2017717272975123; l2 norm of weights: 0.4826789576048753\n","Iteration #3281  Loss: 0.5167156963062387; l2 norm of gradients: 0.20155037020509856; l2 norm of weights: 0.4825776075838562\n","Iteration #3282  Loss: 0.5166691601670707; l2 norm of gradients: 0.20132925794426448; l2 norm of weights: 0.48247643373094645\n","Iteration #3283  Loss: 0.5166227144666063; l2 norm of gradients: 0.2011083902428231; l2 norm of weights: 0.48237543574608277\n","Iteration #3284  Loss: 0.5165763590093213; l2 norm of gradients: 0.20088776682889117; l2 norm of weights: 0.4822746133296336\n","Iteration #3285  Loss: 0.5165300936001216; l2 norm of gradients: 0.20066738743088927; l2 norm of weights: 0.482173966182399\n","Iteration #3286  Loss: 0.5164839180443442; l2 norm of gradients: 0.20044725177754133; l2 norm of weights: 0.48207349400561006\n","Iteration #3287  Loss: 0.5164378321477553; l2 norm of gradients: 0.20022735959787405; l2 norm of weights: 0.48197319650092896\n","Iteration #3288  Loss: 0.5163918357165486; l2 norm of gradients: 0.20000771062121694; l2 norm of weights: 0.481873073370448\n","Iteration #3289  Loss: 0.516345928557346; l2 norm of gradients: 0.19978830457720173; l2 norm of weights: 0.48177312431668984\n","Iteration #3290  Loss: 0.5163001104771956; l2 norm of gradients: 0.19956914119576208; l2 norm of weights: 0.481673349042607\n","Iteration #3291  Loss: 0.5162543812835705; l2 norm of gradients: 0.19935022020713344; l2 norm of weights: 0.4815737472515811\n","Iteration #3292  Loss: 0.5162087407843688; l2 norm of gradients: 0.1991315413418524; l2 norm of weights: 0.4814743186474232\n","Iteration #3293  Loss: 0.5161631887879121; l2 norm of gradients: 0.1989131043307566; l2 norm of weights: 0.48137506293437277\n","Iteration #3294  Loss: 0.5161177251029448; l2 norm of gradients: 0.19869490890498423; l2 norm of weights: 0.481275979817098\n","Iteration #3295  Loss: 0.5160723495386323; l2 norm of gradients: 0.1984769547959738; l2 norm of weights: 0.48117706900069485\n","Iteration #3296  Loss: 0.5160270619045617; l2 norm of gradients: 0.19825924173546391; l2 norm of weights: 0.4810783301906869\n","Iteration #3297  Loss: 0.515981862010739; l2 norm of gradients: 0.19804176945549262; l2 norm of weights: 0.48097976309302537\n","Iteration #3298  Loss: 0.5159367496675901; l2 norm of gradients: 0.1978245376883973; l2 norm of weights: 0.48088136741408827\n","Iteration #3299  Loss: 0.5158917246859579; l2 norm of gradients: 0.1976075461668145; l2 norm of weights: 0.48078314286068013\n","Iteration #3300  Loss: 0.515846786877103; l2 norm of gradients: 0.1973907946236791; l2 norm of weights: 0.4806850891400319\n","Iteration #3301  Loss: 0.515801936052702; l2 norm of gradients: 0.1971742827922245; l2 norm of weights: 0.48058720595980037\n","Iteration #3302  Loss: 0.5157571720248463; l2 norm of gradients: 0.19695801040598196; l2 norm of weights: 0.4804894930280678\n","Iteration #3303  Loss: 0.515712494606042; l2 norm of gradients: 0.19674197719878045; l2 norm of weights: 0.48039195005334195\n","Iteration #3304  Loss: 0.5156679036092088; l2 norm of gradients: 0.1965261829047463; l2 norm of weights: 0.4802945767445551\n","Iteration #3305  Loss: 0.5156233988476779; l2 norm of gradients: 0.1963106272583026; l2 norm of weights: 0.4801973728110641\n","Iteration #3306  Loss: 0.5155789801351933; l2 norm of gradients: 0.19609530999416935; l2 norm of weights: 0.48010033796265017\n","Iteration #3307  Loss: 0.5155346472859086; l2 norm of gradients: 0.19588023084736275; l2 norm of weights: 0.48000347190951786\n","Iteration #3308  Loss: 0.5154904001143873; l2 norm of gradients: 0.19566538955319507; l2 norm of weights: 0.4799067743622955\n","Iteration #3309  Loss: 0.5154462384356021; l2 norm of gradients: 0.19545078584727404; l2 norm of weights: 0.47981024503203423\n","Iteration #3310  Loss: 0.5154021620649333; l2 norm of gradients: 0.19523641946550302; l2 norm of weights: 0.4797138836302083\n","Iteration #3311  Loss: 0.5153581708181683; l2 norm of gradients: 0.1950222901440802; l2 norm of weights: 0.4796176898687137\n","Iteration #3312  Loss: 0.5153142645115004; l2 norm of gradients: 0.19480839761949845; l2 norm of weights: 0.479521663459869\n","Iteration #3313  Loss: 0.5152704429615284; l2 norm of gradients: 0.19459474162854504; l2 norm of weights: 0.47942580411641394\n","Iteration #3314  Loss: 0.5152267059852549; l2 norm of gradients: 0.1943813219083013; l2 norm of weights: 0.4793301115515096\n","Iteration #3315  Loss: 0.5151830534000866; l2 norm of gradients: 0.1941681381961422; l2 norm of weights: 0.4792345854787383\n","Iteration #3316  Loss: 0.5151394850238321; l2 norm of gradients: 0.1939551902297362; l2 norm of weights: 0.4791392256121024\n","Iteration #3317  Loss: 0.5150960006747018; l2 norm of gradients: 0.1937424777470446; l2 norm of weights: 0.4790440316660248\n","Iteration #3318  Loss: 0.5150526001713069; l2 norm of gradients: 0.19353000048632168; l2 norm of weights: 0.47894900335534796\n","Iteration #3319  Loss: 0.5150092833326585; l2 norm of gradients: 0.1933177581861139; l2 norm of weights: 0.478854140395334\n","Iteration #3320  Loss: 0.5149660499781663; l2 norm of gradients: 0.19310575058525992; l2 norm of weights: 0.4787594425016641\n","Iteration #3321  Loss: 0.5149228999276384; l2 norm of gradients: 0.19289397742289016; l2 norm of weights: 0.47866490939043793\n","Iteration #3322  Loss: 0.5148798330012799; l2 norm of gradients: 0.19268243843842645; l2 norm of weights: 0.4785705407781738\n","Iteration #3323  Loss: 0.5148368490196923; l2 norm of gradients: 0.1924711333715816; l2 norm of weights: 0.4784763363818078\n","Iteration #3324  Loss: 0.5147939478038723; l2 norm of gradients: 0.1922600619623593; l2 norm of weights: 0.4783822959186938\n","Iteration #3325  Loss: 0.5147511291752117; l2 norm of gradients: 0.1920492239510539; l2 norm of weights: 0.4782884191066028\n","Iteration #3326  Loss: 0.5147083929554952; l2 norm of gradients: 0.19183861907824948; l2 norm of weights: 0.47819470566372285\n","Iteration #3327  Loss: 0.5146657389669009; l2 norm of gradients: 0.19162824708482018; l2 norm of weights: 0.4781011553086584\n","Iteration #3328  Loss: 0.5146231670319986; l2 norm of gradients: 0.1914181077119297; l2 norm of weights: 0.47800776776043\n","Iteration #3329  Loss: 0.5145806769737492; l2 norm of gradients: 0.19120820070103076; l2 norm of weights: 0.4779145427384742\n","Iteration #3330  Loss: 0.5145382686155036; l2 norm of gradients: 0.1909985257938649; l2 norm of weights: 0.47782147996264285\n","Iteration #3331  Loss: 0.5144959417810023; l2 norm of gradients: 0.1907890827324623; l2 norm of weights: 0.4777285791532027\n","Iteration #3332  Loss: 0.5144536962943742; l2 norm of gradients: 0.19057987125914136; l2 norm of weights: 0.4776358400308355\n","Iteration #3333  Loss: 0.5144115319801357; l2 norm of gradients: 0.19037089111650823; l2 norm of weights: 0.477543262316637\n","Iteration #3334  Loss: 0.5143694486631899; l2 norm of gradients: 0.19016214204745677; l2 norm of weights: 0.4774508457321173\n","Iteration #3335  Loss: 0.5143274461688261; l2 norm of gradients: 0.18995362379516795; l2 norm of weights: 0.4773585899991997\n","Iteration #3336  Loss: 0.5142855243227185; l2 norm of gradients: 0.1897453361031098; l2 norm of weights: 0.4772664948402209\n","Iteration #3337  Loss: 0.514243682950925; l2 norm of gradients: 0.18953727871503692; l2 norm of weights: 0.4771745599779302\n","Iteration #3338  Loss: 0.5142019218798877; l2 norm of gradients: 0.1893294513749901; l2 norm of weights: 0.47708278513548963\n","Iteration #3339  Loss: 0.5141602409364303; l2 norm of gradients: 0.1891218538272963; l2 norm of weights: 0.47699117003647334\n","Iteration #3340  Loss: 0.5141186399477587; l2 norm of gradients: 0.18891448581656786; l2 norm of weights: 0.47689971440486684\n","Iteration #3341  Loss: 0.5140771187414596; l2 norm of gradients: 0.1887073470877027; l2 norm of weights: 0.4768084179650674\n","Iteration #3342  Loss: 0.5140356771454992; l2 norm of gradients: 0.18850043738588368; l2 norm of weights: 0.476717280441883\n","Iteration #3343  Loss: 0.5139943149882233; l2 norm of gradients: 0.18829375645657834; l2 norm of weights: 0.4766263015605323\n","Iteration #3344  Loss: 0.5139530320983555; l2 norm of gradients: 0.1880873040455386; l2 norm of weights: 0.47653548104664417\n","Iteration #3345  Loss: 0.5139118283049975; l2 norm of gradients: 0.1878810798988005; l2 norm of weights: 0.4764448186262574\n","Iteration #3346  Loss: 0.5138707034376266; l2 norm of gradients: 0.1876750837626838; l2 norm of weights: 0.47635431402582024\n","Iteration #3347  Loss: 0.5138296573260966; l2 norm of gradients: 0.1874693153837917; l2 norm of weights: 0.47626396697218976\n","Iteration #3348  Loss: 0.5137886898006361; l2 norm of gradients: 0.18726377450901036; l2 norm of weights: 0.4761737771926322\n","Iteration #3349  Loss: 0.5137478006918477; l2 norm of gradients: 0.18705846088550918; l2 norm of weights: 0.47608374441482193\n","Iteration #3350  Loss: 0.5137069898307071; l2 norm of gradients: 0.18685337426073964; l2 norm of weights: 0.4759938683668412\n","Iteration #3351  Loss: 0.5136662570485626; l2 norm of gradients: 0.18664851438243552; l2 norm of weights: 0.47590414877718\n","Iteration #3352  Loss: 0.5136256021771342; l2 norm of gradients: 0.18644388099861253; l2 norm of weights: 0.47581458537473553\n","Iteration #3353  Loss: 0.5135850250485127; l2 norm of gradients: 0.18623947385756787; l2 norm of weights: 0.47572517788881175\n","Iteration #3354  Loss: 0.5135445254951583; l2 norm of gradients: 0.18603529270788005; l2 norm of weights: 0.47563592604911903\n","Iteration #3355  Loss: 0.5135041033499012; l2 norm of gradients: 0.18583133729840837; l2 norm of weights: 0.47554682958577404\n","Iteration #3356  Loss: 0.5134637584459394; l2 norm of gradients: 0.18562760737829284; l2 norm of weights: 0.47545788822929896\n","Iteration #3357  Loss: 0.5134234906168383; l2 norm of gradients: 0.18542410269695375; l2 norm of weights: 0.4753691017106214\n","Iteration #3358  Loss: 0.5133832996965305; l2 norm of gradients: 0.18522082300409146; l2 norm of weights: 0.4752804697610739\n","Iteration #3359  Loss: 0.5133431855193142; l2 norm of gradients: 0.1850177680496858; l2 norm of weights: 0.4751919921123935\n","Iteration #3360  Loss: 0.5133031479198524; l2 norm of gradients: 0.18481493758399622; l2 norm of weights: 0.4751036684967214\n","Iteration #3361  Loss: 0.5132631867331727; l2 norm of gradients: 0.18461233135756105; l2 norm of weights: 0.4750154986466028\n","Iteration #3362  Loss: 0.5132233017946664; l2 norm of gradients: 0.18440994912119738; l2 norm of weights: 0.47492748229498616\n","Iteration #3363  Loss: 0.5131834929400866; l2 norm of gradients: 0.18420779062600084; l2 norm of weights: 0.47483961917522305\n","Iteration #3364  Loss: 0.5131437600055491; l2 norm of gradients: 0.184005855623345; l2 norm of weights: 0.47475190902106756\n","Iteration #3365  Loss: 0.5131041028275304; l2 norm of gradients: 0.1838041438648815; l2 norm of weights: 0.4746643515666762\n","Iteration #3366  Loss: 0.5130645212428672; l2 norm of gradients: 0.18360265510253918; l2 norm of weights: 0.4745769465466074\n","Iteration #3367  Loss: 0.5130250150887559; l2 norm of gradients: 0.18340138908852438; l2 norm of weights: 0.4744896936958211\n","Iteration #3368  Loss: 0.5129855842027508; l2 norm of gradients: 0.1832003455753201; l2 norm of weights: 0.4744025927496781\n","Iteration #3369  Loss: 0.5129462284227652; l2 norm of gradients: 0.18299952431568603; l2 norm of weights: 0.47431564344394034\n","Iteration #3370  Loss: 0.5129069475870687; l2 norm of gradients: 0.18279892506265813; l2 norm of weights: 0.4742288455147699\n","Iteration #3371  Loss: 0.5128677415342874; l2 norm of gradients: 0.18259854756954821; l2 norm of weights: 0.474142198698729\n","Iteration #3372  Loss: 0.5128286101034025; l2 norm of gradients: 0.18239839158994398; l2 norm of weights: 0.474055702732779\n","Iteration #3373  Loss: 0.5127895531337507; l2 norm of gradients: 0.18219845687770828; l2 norm of weights: 0.4739693573542812\n","Iteration #3374  Loss: 0.5127505704650218; l2 norm of gradients: 0.18199874318697895; l2 norm of weights: 0.47388316230099503\n","Iteration #3375  Loss: 0.5127116619372593; l2 norm of gradients: 0.18179925027216878; l2 norm of weights: 0.4737971173110788\n","Iteration #3376  Loss: 0.5126728273908584; l2 norm of gradients: 0.1815999778879649; l2 norm of weights: 0.47371122212308875\n","Iteration #3377  Loss: 0.5126340666665669; l2 norm of gradients: 0.18140092578932848; l2 norm of weights: 0.47362547647597875\n","Iteration #3378  Loss: 0.5125953796054823; l2 norm of gradients: 0.18120209373149462; l2 norm of weights: 0.4735398801090998\n","Iteration #3379  Loss: 0.5125567660490525; l2 norm of gradients: 0.18100348146997194; l2 norm of weights: 0.47345443276220023\n","Iteration #3380  Loss: 0.512518225839075; l2 norm of gradients: 0.18080508876054213; l2 norm of weights: 0.47336913417542437\n","Iteration #3381  Loss: 0.5124797588176951; l2 norm of gradients: 0.18060691535925996; l2 norm of weights: 0.473283984089313\n","Iteration #3382  Loss: 0.5124413648274065; l2 norm of gradients: 0.1804089610224527; l2 norm of weights: 0.47319898224480256\n","Iteration #3383  Loss: 0.512403043711049; l2 norm of gradients: 0.1802112255067199; l2 norm of weights: 0.47311412838322464\n","Iteration #3384  Loss: 0.5123647953118095; l2 norm of gradients: 0.1800137085689332; l2 norm of weights: 0.4730294222463062\n","Iteration #3385  Loss: 0.5123266194732193; l2 norm of gradients: 0.1798164099662358; l2 norm of weights: 0.4729448635761682\n","Iteration #3386  Loss: 0.5122885160391552; l2 norm of gradients: 0.17961932945604248; l2 norm of weights: 0.4728604521153263\n","Iteration #3387  Loss: 0.5122504848538371; l2 norm of gradients: 0.17942246679603888; l2 norm of weights: 0.4727761876066897\n","Iteration #3388  Loss: 0.5122125257618284; l2 norm of gradients: 0.17922582174418158; l2 norm of weights: 0.47269206979356093\n","Iteration #3389  Loss: 0.5121746386080348; l2 norm of gradients: 0.1790293940586976; l2 norm of weights: 0.4726080984196357\n","Iteration #3390  Loss: 0.5121368232377035; l2 norm of gradients: 0.1788331834980841; l2 norm of weights: 0.4725242732290022\n","Iteration #3391  Loss: 0.5120990794964226; l2 norm of gradients: 0.17863718982110824; l2 norm of weights: 0.4724405939661409\n","Iteration #3392  Loss: 0.5120614072301203; l2 norm of gradients: 0.1784414127868066; l2 norm of weights: 0.4723570603759242\n","Iteration #3393  Loss: 0.5120238062850637; l2 norm of gradients: 0.1782458521544851; l2 norm of weights: 0.47227367220361577\n","Iteration #3394  Loss: 0.511986276507859; l2 norm of gradients: 0.17805050768371877; l2 norm of weights: 0.47219042919487036\n","Iteration #3395  Loss: 0.5119488177454499; l2 norm of gradients: 0.17785537913435107; l2 norm of weights: 0.47210733109573344\n","Iteration #3396  Loss: 0.5119114298451172; l2 norm of gradients: 0.177660466266494; l2 norm of weights: 0.4720243776526409\n","Iteration #3397  Loss: 0.511874112654478; l2 norm of gradients: 0.17746576884052767; l2 norm of weights: 0.4719415686124181\n","Iteration #3398  Loss: 0.5118368660214851; l2 norm of gradients: 0.17727128661709987; l2 norm of weights: 0.4718589037222803\n","Iteration #3399  Loss: 0.5117996897944259; l2 norm of gradients: 0.17707701935712591; l2 norm of weights: 0.4717763827298316\n","Iteration #3400  Loss: 0.5117625838219221; l2 norm of gradients: 0.1768829668217883; l2 norm of weights: 0.4716940053830648\n","Iteration #3401  Loss: 0.5117255479529286; l2 norm of gradients: 0.17668912877253645; l2 norm of weights: 0.4716117714303611\n","Iteration #3402  Loss: 0.5116885820367328; l2 norm of gradients: 0.17649550497108626; l2 norm of weights: 0.4715296806204896\n","Iteration #3403  Loss: 0.5116516859229544; l2 norm of gradients: 0.17630209517942005; l2 norm of weights: 0.47144773270260704\n","Iteration #3404  Loss: 0.5116148594615436; l2 norm of gradients: 0.1761088991597861; l2 norm of weights: 0.47136592742625694\n","Iteration #3405  Loss: 0.5115781025027816; l2 norm of gradients: 0.1759159166746983; l2 norm of weights: 0.47128426454136985\n","Iteration #3406  Loss: 0.511541414897279; l2 norm of gradients: 0.17572314748693607; l2 norm of weights: 0.47120274379826244\n","Iteration #3407  Loss: 0.5115047964959749; l2 norm of gradients: 0.17553059135954382; l2 norm of weights: 0.47112136494763746\n","Iteration #3408  Loss: 0.5114682471501376; l2 norm of gradients: 0.1753382480558309; l2 norm of weights: 0.47104012774058324\n","Iteration #3409  Loss: 0.5114317667113621; l2 norm of gradients: 0.175146117339371; l2 norm of weights: 0.470959031928573\n","Iteration #3410  Loss: 0.5113953550315703; l2 norm of gradients: 0.1749541989740022; l2 norm of weights: 0.47087807726346514\n","Iteration #3411  Loss: 0.5113590119630101; l2 norm of gradients: 0.17476249272382638; l2 norm of weights: 0.47079726349750195\n","Iteration #3412  Loss: 0.511322737358255; l2 norm of gradients: 0.17457099835320916; l2 norm of weights: 0.47071659038330993\n","Iteration #3413  Loss: 0.5112865310702028; l2 norm of gradients: 0.1743797156267794; l2 norm of weights: 0.4706360576738992\n","Iteration #3414  Loss: 0.5112503929520754; l2 norm of gradients: 0.17418864430942907; l2 norm of weights: 0.4705556651226629\n","Iteration #3415  Loss: 0.5112143228574174; l2 norm of gradients: 0.17399778416631284; l2 norm of weights: 0.4704754124833767\n","Iteration #3416  Loss: 0.5111783206400965; l2 norm of gradients: 0.17380713496284786; l2 norm of weights: 0.47039529951019937\n","Iteration #3417  Loss: 0.5111423861543015; l2 norm of gradients: 0.17361669646471345; l2 norm of weights: 0.4703153259576709\n","Iteration #3418  Loss: 0.5111065192545426; l2 norm of gradients: 0.17342646843785076; l2 norm of weights: 0.47023549158071326\n","Iteration #3419  Loss: 0.5110707197956502; l2 norm of gradients: 0.1732364506484626; l2 norm of weights: 0.4701557961346295\n","Iteration #3420  Loss: 0.5110349876327742; l2 norm of gradients: 0.17304664286301297; l2 norm of weights: 0.4700762393751034\n","Iteration #3421  Loss: 0.5109993226213835; l2 norm of gradients: 0.17285704484822692; l2 norm of weights: 0.4699968210581991\n","Iteration #3422  Loss: 0.5109637246172649; l2 norm of gradients: 0.17266765637109024; l2 norm of weights: 0.46991754094036087\n","Iteration #3423  Loss: 0.5109281934765232; l2 norm of gradients: 0.172478477198849; l2 norm of weights: 0.46983839877841255\n","Iteration #3424  Loss: 0.5108927290555796; l2 norm of gradients: 0.17228950709900961; l2 norm of weights: 0.469759394329557\n","Iteration #3425  Loss: 0.5108573312111712; l2 norm of gradients: 0.17210074583933804; l2 norm of weights: 0.4696805273513757\n","Iteration #3426  Loss: 0.5108219998003507; l2 norm of gradients: 0.17191219318786002; l2 norm of weights: 0.4696017976018292\n","Iteration #3427  Loss: 0.5107867346804856; l2 norm of gradients: 0.1717238489128604; l2 norm of weights: 0.46952320483925525\n","Iteration #3428  Loss: 0.5107515357092574; l2 norm of gradients: 0.17153571278288315; l2 norm of weights: 0.46944474882236975\n","Iteration #3429  Loss: 0.5107164027446601; l2 norm of gradients: 0.17134778456673072; l2 norm of weights: 0.46936642931026545\n","Iteration #3430  Loss: 0.5106813356450015; l2 norm of gradients: 0.17116006403346404; l2 norm of weights: 0.46928824606241204\n","Iteration #3431  Loss: 0.5106463342689003; l2 norm of gradients: 0.17097255095240219; l2 norm of weights: 0.4692101988386555\n","Iteration #3432  Loss: 0.5106113984752869; l2 norm of gradients: 0.17078524509312196; l2 norm of weights: 0.46913228739921786\n","Iteration #3433  Loss: 0.5105765281234019; l2 norm of gradients: 0.1705981462254577; l2 norm of weights: 0.4690545115046967\n","Iteration #3434  Loss: 0.5105417230727961; l2 norm of gradients: 0.170411254119501; l2 norm of weights: 0.4689768709160649\n","Iteration #3435  Loss: 0.510506983183329; l2 norm of gradients: 0.17022456854560042; l2 norm of weights: 0.4688993653946698\n","Iteration #3436  Loss: 0.510472308315169; l2 norm of gradients: 0.17003808927436112; l2 norm of weights: 0.4688219947022336\n","Iteration #3437  Loss: 0.5104376983287919; l2 norm of gradients: 0.16985181607664468; l2 norm of weights: 0.46874475860085196\n","Iteration #3438  Loss: 0.5104031530849807; l2 norm of gradients: 0.1696657487235687; l2 norm of weights: 0.46866765685299455\n","Iteration #3439  Loss: 0.5103686724448251; l2 norm of gradients: 0.1694798869865067; l2 norm of weights: 0.4685906892215039\n","Iteration #3440  Loss: 0.5103342562697197; l2 norm of gradients: 0.16929423063708762; l2 norm of weights: 0.4685138554695954\n","Iteration #3441  Loss: 0.5102999044213653; l2 norm of gradients: 0.16910877944719568; l2 norm of weights: 0.4684371553608569\n","Iteration #3442  Loss: 0.5102656167617662; l2 norm of gradients: 0.1689235331889699; l2 norm of weights: 0.46836058865924823\n","Iteration #3443  Loss: 0.5102313931532305; l2 norm of gradients: 0.16873849163480426; l2 norm of weights: 0.4682841551291005\n","Iteration #3444  Loss: 0.5101972334583699; l2 norm of gradients: 0.16855365455734675; l2 norm of weights: 0.4682078545351163\n","Iteration #3445  Loss: 0.5101631375400978; l2 norm of gradients: 0.1683690217294998; l2 norm of weights: 0.4681316866423688\n","Iteration #3446  Loss: 0.5101291052616297; l2 norm of gradients: 0.16818459292441934; l2 norm of weights: 0.4680556512163016\n","Iteration #3447  Loss: 0.5100951364864817; l2 norm of gradients: 0.16800036791551504; l2 norm of weights: 0.46797974802272824\n","Iteration #3448  Loss: 0.5100612310784711; l2 norm of gradients: 0.16781634647644972; l2 norm of weights: 0.46790397682783175\n","Iteration #3449  Loss: 0.5100273889017136; l2 norm of gradients: 0.16763252838113918; l2 norm of weights: 0.46782833739816426\n","Iteration #3450  Loss: 0.5099936098206251; l2 norm of gradients: 0.16744891340375184; l2 norm of weights: 0.46775282950064684\n","Iteration #3451  Loss: 0.5099598936999192; l2 norm of gradients: 0.16726550131870865; l2 norm of weights: 0.4676774529025686\n","Iteration #3452  Loss: 0.5099262404046075; l2 norm of gradients: 0.16708229190068255; l2 norm of weights: 0.46760220737158686\n","Iteration #3453  Loss: 0.5098926497999985; l2 norm of gradients: 0.1668992849245983; l2 norm of weights: 0.4675270926757263\n","Iteration #3454  Loss: 0.5098591217516971; l2 norm of gradients: 0.16671648016563229; l2 norm of weights: 0.4674521085833787\n","Iteration #3455  Loss: 0.5098256561256039; l2 norm of gradients: 0.1665338773992121; l2 norm of weights: 0.46737725486330267\n","Iteration #3456  Loss: 0.5097922527879148; l2 norm of gradients: 0.16635147640101644; l2 norm of weights: 0.46730253128462307\n","Iteration #3457  Loss: 0.50975891160512; l2 norm of gradients: 0.16616927694697445; l2 norm of weights: 0.4672279376168306\n","Iteration #3458  Loss: 0.5097256324440032; l2 norm of gradients: 0.165987278813266; l2 norm of weights: 0.4671534736297816\n","Iteration #3459  Loss: 0.5096924151716413; l2 norm of gradients: 0.1658054817763209; l2 norm of weights: 0.46707913909369725\n","Iteration #3460  Loss: 0.509659259655404; l2 norm of gradients: 0.16562388561281893; l2 norm of weights: 0.4670049337791638\n","Iteration #3461  Loss: 0.5096261657629528; l2 norm of gradients: 0.16544249009968945; l2 norm of weights: 0.4669308574571314\n","Iteration #3462  Loss: 0.50959313336224; l2 norm of gradients: 0.16526129501411102; l2 norm of weights: 0.4668569098989143\n","Iteration #3463  Loss: 0.5095601623215085; l2 norm of gradients: 0.16508030013351135; l2 norm of weights: 0.4667830908761901\n","Iteration #3464  Loss: 0.5095272525092914; l2 norm of gradients: 0.16489950523556693; l2 norm of weights: 0.46670940016099965\n","Iteration #3465  Loss: 0.5094944037944107; l2 norm of gradients: 0.16471891009820258; l2 norm of weights: 0.4666358375257461\n","Iteration #3466  Loss: 0.5094616160459774; l2 norm of gradients: 0.1645385144995914; l2 norm of weights: 0.46656240274319527\n","Iteration #3467  Loss: 0.5094288891333901; l2 norm of gradients: 0.16435831821815444; l2 norm of weights: 0.4664890955864746\n","Iteration #3468  Loss: 0.5093962229263347; l2 norm of gradients: 0.1641783210325603; l2 norm of weights: 0.46641591582907294\n","Iteration #3469  Loss: 0.5093636172947843; l2 norm of gradients: 0.16399852272172508; l2 norm of weights: 0.46634286324484026\n","Iteration #3470  Loss: 0.5093310721089974; l2 norm of gradients: 0.1638189230648118; l2 norm of weights: 0.466269937607987\n","Iteration #3471  Loss: 0.5092985872395183; l2 norm of gradients: 0.1636395218412304; l2 norm of weights: 0.46619713869308416\n","Iteration #3472  Loss: 0.5092661625571759; l2 norm of gradients: 0.16346031883063736; l2 norm of weights: 0.46612446627506215\n","Iteration #3473  Loss: 0.5092337979330838; l2 norm of gradients: 0.16328131381293537; l2 norm of weights: 0.466051920129211\n","Iteration #3474  Loss: 0.5092014932386383; l2 norm of gradients: 0.16310250656827313; l2 norm of weights: 0.4659795000311798\n","Iteration #3475  Loss: 0.509169248345519; l2 norm of gradients: 0.1629238968770451; l2 norm of weights: 0.4659072057569759\n","Iteration #3476  Loss: 0.5091370631256882; l2 norm of gradients: 0.16274548451989101; l2 norm of weights: 0.46583503708296514\n","Iteration #3477  Loss: 0.5091049374513891; l2 norm of gradients: 0.162567269277696; l2 norm of weights: 0.4657629937858712\n","Iteration #3478  Loss: 0.5090728711951464; l2 norm of gradients: 0.16238925093158982; l2 norm of weights: 0.46569107564277473\n","Iteration #3479  Loss: 0.509040864229765; l2 norm of gradients: 0.16221142926294713; l2 norm of weights: 0.4656192824311138\n","Iteration #3480  Loss: 0.5090089164283298; l2 norm of gradients: 0.16203380405338663; l2 norm of weights: 0.46554761392868255\n","Iteration #3481  Loss: 0.5089770276642047; l2 norm of gradients: 0.1618563750847713; l2 norm of weights: 0.4654760699136316\n","Iteration #3482  Loss: 0.5089451978110322; l2 norm of gradients: 0.16167914213920787; l2 norm of weights: 0.46540465016446736\n","Iteration #3483  Loss: 0.5089134267427325; l2 norm of gradients: 0.16150210499904652; l2 norm of weights: 0.46533335446005125\n","Iteration #3484  Loss: 0.5088817143335037; l2 norm of gradients: 0.16132526344688075; l2 norm of weights: 0.46526218257959984\n","Iteration #3485  Loss: 0.50885006045782; l2 norm of gradients: 0.161148617265547; l2 norm of weights: 0.46519113430268416\n","Iteration #3486  Loss: 0.508818464990432; l2 norm of gradients: 0.1609721662381244; l2 norm of weights: 0.4651202094092292\n","Iteration #3487  Loss: 0.5087869278063658; l2 norm of gradients: 0.16079591014793457; l2 norm of weights: 0.4650494076795139\n","Iteration #3488  Loss: 0.5087554487809222; l2 norm of gradients: 0.1606198487785413; l2 norm of weights: 0.4649787288941703\n","Iteration #3489  Loss: 0.5087240277896764; l2 norm of gradients: 0.16044398191375014; l2 norm of weights: 0.46490817283418334\n","Iteration #3490  Loss: 0.5086926647084771; l2 norm of gradients: 0.16026830933760844; l2 norm of weights: 0.46483773928089045\n","Iteration #3491  Loss: 0.5086613594134461; l2 norm of gradients: 0.16009283083440476; l2 norm of weights: 0.46476742801598114\n","Iteration #3492  Loss: 0.5086301117809778; l2 norm of gradients: 0.15991754618866882; l2 norm of weights: 0.46469723882149644\n","Iteration #3493  Loss: 0.5085989216877386; l2 norm of gradients: 0.15974245518517113; l2 norm of weights: 0.4646271714798288\n","Iteration #3494  Loss: 0.5085677890106653; l2 norm of gradients: 0.15956755760892272; l2 norm of weights: 0.4645572257737213\n","Iteration #3495  Loss: 0.5085367136269665; l2 norm of gradients: 0.15939285324517494; l2 norm of weights: 0.46448740148626777\n","Iteration #3496  Loss: 0.5085056954141197; l2 norm of gradients: 0.15921834187941908; l2 norm of weights: 0.46441769840091157\n","Iteration #3497  Loss: 0.5084747342498728; l2 norm of gradients: 0.15904402329738626; l2 norm of weights: 0.4643481163014461\n","Iteration #3498  Loss: 0.5084438300122419; l2 norm of gradients: 0.15886989728504694; l2 norm of weights: 0.46427865497201365\n","Iteration #3499  Loss: 0.5084129825795118; l2 norm of gradients: 0.1586959636286109; l2 norm of weights: 0.46420931419710554\n","Iteration #3500  Loss: 0.5083821918302346; l2 norm of gradients: 0.15852222211452677; l2 norm of weights: 0.4641400937615613\n","Iteration #3501  Loss: 0.5083514576432296; l2 norm of gradients: 0.1583486725294819; l2 norm of weights: 0.46407099345056835\n","Iteration #3502  Loss: 0.5083207798975826; l2 norm of gradients: 0.158175314660402; l2 norm of weights: 0.46400201304966177\n","Iteration #3503  Loss: 0.5082901584726455; l2 norm of gradients: 0.15800214829445092; l2 norm of weights: 0.46393315234472404\n","Iteration #3504  Loss: 0.5082595932480347; l2 norm of gradients: 0.1578291732190304; l2 norm of weights: 0.46386441112198384\n","Iteration #3505  Loss: 0.5082290841036325; l2 norm of gradients: 0.15765638922177977; l2 norm of weights: 0.46379578916801667\n","Iteration #3506  Loss: 0.5081986309195844; l2 norm of gradients: 0.1574837960905756; l2 norm of weights: 0.4637272862697436\n","Iteration #3507  Loss: 0.5081682335762998; l2 norm of gradients: 0.1573113936135317; l2 norm of weights: 0.46365890221443135\n","Iteration #3508  Loss: 0.5081378919544508; l2 norm of gradients: 0.1571391815789985; l2 norm of weights: 0.46359063678969176\n","Iteration #3509  Loss: 0.5081076059349722; l2 norm of gradients: 0.1569671597755632; l2 norm of weights: 0.4635224897834812\n","Iteration #3510  Loss: 0.5080773753990604; l2 norm of gradients: 0.15679532799204898; l2 norm of weights: 0.4634544609841006\n","Iteration #3511  Loss: 0.5080472002281731; l2 norm of gradients: 0.15662368601751525; l2 norm of weights: 0.4633865501801946\n","Iteration #3512  Loss: 0.5080170803040286; l2 norm of gradients: 0.15645223364125707; l2 norm of weights: 0.4633187571607511\n","Iteration #3513  Loss: 0.5079870155086051; l2 norm of gradients: 0.156280970652805; l2 norm of weights: 0.46325108171510143\n","Iteration #3514  Loss: 0.5079570057241407; l2 norm of gradients: 0.15610989684192486; l2 norm of weights: 0.4631835236329192\n","Iteration #3515  Loss: 0.5079270508331318; l2 norm of gradients: 0.15593901199861734; l2 norm of weights: 0.46311608270422056\n","Iteration #3516  Loss: 0.5078971507183336; l2 norm of gradients: 0.15576831591311782; l2 norm of weights: 0.46304875871936313\n","Iteration #3517  Loss: 0.5078673052627591; l2 norm of gradients: 0.15559780837589623; l2 norm of weights: 0.46298155146904635\n","Iteration #3518  Loss: 0.5078375143496783; l2 norm of gradients: 0.15542748917765653; l2 norm of weights: 0.4629144607443101\n","Iteration #3519  Loss: 0.5078077778626175; l2 norm of gradients: 0.1552573581093367; l2 norm of weights: 0.4628474863365356\n","Iteration #3520  Loss: 0.5077780956853597; l2 norm of gradients: 0.1550874149621082; l2 norm of weights: 0.46278062803744346\n","Iteration #3521  Loss: 0.5077484677019429; l2 norm of gradients: 0.15491765952737607; l2 norm of weights: 0.46271388563909466\n","Iteration #3522  Loss: 0.5077188937966605; l2 norm of gradients: 0.15474809159677827; l2 norm of weights: 0.46264725893388914\n","Iteration #3523  Loss: 0.5076893738540595; l2 norm of gradients: 0.15457871096218578; l2 norm of weights: 0.462580747714566\n","Iteration #3524  Loss: 0.5076599077589418; l2 norm of gradients: 0.15440951741570214; l2 norm of weights: 0.46251435177420275\n","Iteration #3525  Loss: 0.5076304953963613; l2 norm of gradients: 0.1542405107496631; l2 norm of weights: 0.4624480709062152\n","Iteration #3526  Loss: 0.5076011366516253; l2 norm of gradients: 0.15407169075663668; l2 norm of weights: 0.46238190490435666\n","Iteration #3527  Loss: 0.5075718314102933; l2 norm of gradients: 0.15390305722942255; l2 norm of weights: 0.4623158535627178\n","Iteration #3528  Loss: 0.5075425795581758; l2 norm of gradients: 0.15373460996105207; l2 norm of weights: 0.46224991667572635\n","Iteration #3529  Loss: 0.507513380981335; l2 norm of gradients: 0.1535663487447878; l2 norm of weights: 0.4621840940381463\n","Iteration #3530  Loss: 0.5074842355660829; l2 norm of gradients: 0.15339827337412337; l2 norm of weights: 0.46211838544507783\n","Iteration #3531  Loss: 0.5074551431989818; l2 norm of gradients: 0.15323038364278327; l2 norm of weights: 0.46205279069195687\n","Iteration #3532  Loss: 0.507426103766843; l2 norm of gradients: 0.15306267934472237; l2 norm of weights: 0.4619873095745543\n","Iteration #3533  Loss: 0.5073971171567272; l2 norm of gradients: 0.1528951602741259; l2 norm of weights: 0.4619219418889762\n","Iteration #3534  Loss: 0.5073681832559426; l2 norm of gradients: 0.1527278262254091; l2 norm of weights: 0.4618566874316629\n","Iteration #3535  Loss: 0.5073393019520454; l2 norm of gradients: 0.15256067699321685; l2 norm of weights: 0.4617915459993887\n","Iteration #3536  Loss: 0.507310473132839; l2 norm of gradients: 0.1523937123724237; l2 norm of weights: 0.4617265173892617\n","Iteration #3537  Loss: 0.5072816966863732; l2 norm of gradients: 0.15222693215813327; l2 norm of weights: 0.461661601398723\n","Iteration #3538  Loss: 0.5072529725009444; l2 norm of gradients: 0.1520603361456782; l2 norm of weights: 0.4615967978255465\n","Iteration #3539  Loss: 0.5072243004650936; l2 norm of gradients: 0.15189392413062; l2 norm of weights: 0.4615321064678387\n","Iteration #3540  Loss: 0.5071956804676074; l2 norm of gradients: 0.15172769590874843; l2 norm of weights: 0.46146752712403794\n","Iteration #3541  Loss: 0.5071671123975168; l2 norm of gradients: 0.15156165127608157; l2 norm of weights: 0.461403059592914\n","Iteration #3542  Loss: 0.5071385961440961; l2 norm of gradients: 0.1513957900288654; l2 norm of weights: 0.46133870367356794\n","Iteration #3543  Loss: 0.5071101315968637; l2 norm of gradients: 0.15123011196357375; l2 norm of weights: 0.4612744591654317\n","Iteration #3544  Loss: 0.5070817186455804; l2 norm of gradients: 0.15106461687690767; l2 norm of weights: 0.46121032586826727\n","Iteration #3545  Loss: 0.5070533571802492; l2 norm of gradients: 0.15089930456579553; l2 norm of weights: 0.46114630358216663\n","Iteration #3546  Loss: 0.507025047091115; l2 norm of gradients: 0.15073417482739268; l2 norm of weights: 0.4610823921075515\n","Iteration #3547  Loss: 0.5069967882686639; l2 norm of gradients: 0.15056922745908102; l2 norm of weights: 0.46101859124517236\n","Iteration #3548  Loss: 0.5069685806036223; l2 norm of gradients: 0.15040446225846896; l2 norm of weights: 0.46095490079610857\n","Iteration #3549  Loss: 0.506940423986957; l2 norm of gradients: 0.15023987902339106; l2 norm of weights: 0.4608913205617679\n","Iteration #3550  Loss: 0.5069123183098748; l2 norm of gradients: 0.15007547755190784; l2 norm of weights: 0.46082785034388574\n","Iteration #3551  Loss: 0.5068842634638209; l2 norm of gradients: 0.14991125764230542; l2 norm of weights: 0.4607644899445251\n","Iteration #3552  Loss: 0.506856259340479; l2 norm of gradients: 0.14974721909309538; l2 norm of weights: 0.46070123916607597\n","Iteration #3553  Loss: 0.5068283058317713; l2 norm of gradients: 0.14958336170301448; l2 norm of weights: 0.460638097811255\n","Iteration #3554  Loss: 0.5068004028298573; l2 norm of gradients: 0.1494196852710243; l2 norm of weights: 0.460575065683105\n","Iteration #3555  Loss: 0.5067725502271331; l2 norm of gradients: 0.14925618959631118; l2 norm of weights: 0.4605121425849948\n","Iteration #3556  Loss: 0.5067447479162319; l2 norm of gradients: 0.14909287447828584; l2 norm of weights: 0.46044932832061847\n","Iteration #3557  Loss: 0.5067169957900223; l2 norm of gradients: 0.14892973971658308; l2 norm of weights: 0.46038662269399516\n","Iteration #3558  Loss: 0.5066892937416082; l2 norm of gradients: 0.1487667851110617; l2 norm of weights: 0.4603240255094686\n","Iteration #3559  Loss: 0.5066616416643288; l2 norm of gradients: 0.14860401046180416; l2 norm of weights: 0.4602615365717067\n","Iteration #3560  Loss: 0.5066340394517572; l2 norm of gradients: 0.14844141556911616; l2 norm of weights: 0.46019915568570113\n","Iteration #3561  Loss: 0.5066064869977007; l2 norm of gradients: 0.14827900023352683; l2 norm of weights: 0.46013688265676694\n","Iteration #3562  Loss: 0.5065789841961998; l2 norm of gradients: 0.14811676425578793; l2 norm of weights: 0.46007471729054217\n","Iteration #3563  Loss: 0.5065515309415275; l2 norm of gradients: 0.14795470743687403; l2 norm of weights: 0.4600126593929874\n","Iteration #3564  Loss: 0.5065241271281893; l2 norm of gradients: 0.14779282957798204; l2 norm of weights: 0.45995070877038524\n","Iteration #3565  Loss: 0.5064967726509226; l2 norm of gradients: 0.14763113048053106; l2 norm of weights: 0.4598888652293403\n","Iteration #3566  Loss: 0.5064694674046959; l2 norm of gradients: 0.14746960994616204; l2 norm of weights: 0.45982712857677815\n","Iteration #3567  Loss: 0.5064422112847082; l2 norm of gradients: 0.14730826777673767; l2 norm of weights: 0.4597654986199455\n","Iteration #3568  Loss: 0.5064150041863893; l2 norm of gradients: 0.147147103774342; l2 norm of weights: 0.4597039751664097\n","Iteration #3569  Loss: 0.5063878460053981; l2 norm of gradients: 0.14698611774128023; l2 norm of weights: 0.45964255802405773\n","Iteration #3570  Loss: 0.5063607366376232; l2 norm of gradients: 0.1468253094800785; l2 norm of weights: 0.4595812470010968\n","Iteration #3571  Loss: 0.5063336759791814; l2 norm of gradients: 0.1466646787934836; l2 norm of weights: 0.459520041906053\n","Iteration #3572  Loss: 0.5063066639264183; l2 norm of gradients: 0.14650422548446274; l2 norm of weights: 0.45945894254777153\n","Iteration #3573  Loss: 0.5062797003759065; l2 norm of gradients: 0.1463439493562033; l2 norm of weights: 0.45939794873541595\n","Iteration #3574  Loss: 0.5062527852244466; l2 norm of gradients: 0.14618385021211255; l2 norm of weights: 0.45933706027846777\n","Iteration #3575  Loss: 0.506225918369065; l2 norm of gradients: 0.14602392785581753; l2 norm of weights: 0.45927627698672635\n","Iteration #3576  Loss: 0.5061990997070152; l2 norm of gradients: 0.14586418209116458; l2 norm of weights: 0.45921559867030814\n","Iteration #3577  Loss: 0.5061723291357754; l2 norm of gradients: 0.1457046127222194; l2 norm of weights: 0.4591550251396465\n","Iteration #3578  Loss: 0.5061456065530497; l2 norm of gradients: 0.1455452195532665; l2 norm of weights: 0.45909455620549133\n","Iteration #3579  Loss: 0.5061189318567666; l2 norm of gradients: 0.14538600238880903; l2 norm of weights: 0.45903419167890813\n","Iteration #3580  Loss: 0.5060923049450791; l2 norm of gradients: 0.14522696103356875; l2 norm of weights: 0.45897393137127845\n","Iteration #3581  Loss: 0.5060657257163633; l2 norm of gradients: 0.14506809529248557; l2 norm of weights: 0.45891377509429876\n","Iteration #3582  Loss: 0.506039194069219; l2 norm of gradients: 0.14490940497071728; l2 norm of weights: 0.45885372265998053\n","Iteration #3583  Loss: 0.5060127099024686; l2 norm of gradients: 0.1447508898736395; l2 norm of weights: 0.45879377388064946\n","Iteration #3584  Loss: 0.5059862731151564; l2 norm of gradients: 0.14459254980684524; l2 norm of weights: 0.4587339285689452\n","Iteration #3585  Loss: 0.5059598836065491; l2 norm of gradients: 0.14443438457614474; l2 norm of weights: 0.45867418653782105\n","Iteration #3586  Loss: 0.5059335412761338; l2 norm of gradients: 0.14427639398756537; l2 norm of weights: 0.4586145476005435\n","Iteration #3587  Loss: 0.5059072460236188; l2 norm of gradients: 0.14411857784735096; l2 norm of weights: 0.4585550115706918\n","Iteration #3588  Loss: 0.5058809977489326; l2 norm of gradients: 0.14396093596196216; l2 norm of weights: 0.45849557826215726\n","Iteration #3589  Loss: 0.5058547963522237; l2 norm of gradients: 0.1438034681380756; l2 norm of weights: 0.4584362474891437\n","Iteration #3590  Loss: 0.5058286417338593; l2 norm of gradients: 0.14364617418258413; l2 norm of weights: 0.45837701906616585\n","Iteration #3591  Loss: 0.5058025337944259; l2 norm of gradients: 0.14348905390259623; l2 norm of weights: 0.45831789280804996\n","Iteration #3592  Loss: 0.505776472434728; l2 norm of gradients: 0.14333210710543595; l2 norm of weights: 0.4582588685299329\n","Iteration #3593  Loss: 0.5057504575557882; l2 norm of gradients: 0.14317533359864276; l2 norm of weights: 0.45819994604726183\n","Iteration #3594  Loss: 0.505724489058846; l2 norm of gradients: 0.14301873318997094; l2 norm of weights: 0.45814112517579386\n","Iteration #3595  Loss: 0.5056985668453583; l2 norm of gradients: 0.14286230568738978; l2 norm of weights: 0.45808240573159553\n","Iteration #3596  Loss: 0.5056726908169982; l2 norm of gradients: 0.14270605089908303; l2 norm of weights: 0.45802378753104245\n","Iteration #3597  Loss: 0.5056468608756544; l2 norm of gradients: 0.14254996863344876; l2 norm of weights: 0.4579652703908191\n","Iteration #3598  Loss: 0.5056210769234313; l2 norm of gradients: 0.14239405869909919; l2 norm of weights: 0.45790685412791804\n","Iteration #3599  Loss: 0.5055953388626484; l2 norm of gradients: 0.14223832090486035; l2 norm of weights: 0.4578485385596398\n","Iteration #3600  Loss: 0.5055696465958394; l2 norm of gradients: 0.14208275505977191; l2 norm of weights: 0.4577903235035924\n","Iteration #3601  Loss: 0.5055440000257518; l2 norm of gradients: 0.14192736097308678; l2 norm of weights: 0.45773220877769083\n","Iteration #3602  Loss: 0.5055183990553473; l2 norm of gradients: 0.1417721384542712; l2 norm of weights: 0.45767419420015687\n","Iteration #3603  Loss: 0.5054928435878003; l2 norm of gradients: 0.1416170873130041; l2 norm of weights: 0.45761627958951845\n","Iteration #3604  Loss: 0.5054673335264976; l2 norm of gradients: 0.1414622073591772; l2 norm of weights: 0.45755846476460943\n","Iteration #3605  Loss: 0.505441868775038; l2 norm of gradients: 0.14130749840289455; l2 norm of weights: 0.45750074954456893\n","Iteration #3606  Loss: 0.5054164492372326; l2 norm of gradients: 0.14115296025447244; l2 norm of weights: 0.4574431337488414\n","Iteration #3607  Loss: 0.5053910748171031; l2 norm of gradients: 0.14099859272443901; l2 norm of weights: 0.4573856171971757\n","Iteration #3608  Loss: 0.5053657454188822; l2 norm of gradients: 0.1408443956235341; l2 norm of weights: 0.457328199709625\n","Iteration #3609  Loss: 0.5053404609470127; l2 norm of gradients: 0.14069036876270918; l2 norm of weights: 0.45727088110654635\n","Iteration #3610  Loss: 0.5053152213061471; l2 norm of gradients: 0.1405365119531267; l2 norm of weights: 0.45721366120859996\n","Iteration #3611  Loss: 0.5052900264011474; l2 norm of gradients: 0.14038282500616028; l2 norm of weights: 0.4571565398367495\n","Iteration #3612  Loss: 0.5052648761370845; l2 norm of gradients: 0.1402293077333942; l2 norm of weights: 0.45709951681226085\n","Iteration #3613  Loss: 0.5052397704192373; l2 norm of gradients: 0.1400759599466233; l2 norm of weights: 0.45704259195670227\n","Iteration #3614  Loss: 0.5052147091530931; l2 norm of gradients: 0.13992278145785264; l2 norm of weights: 0.45698576509194383\n","Iteration #3615  Loss: 0.5051896922443464; l2 norm of gradients: 0.1397697720792974; l2 norm of weights: 0.45692903604015705\n","Iteration #3616  Loss: 0.5051647195988987; l2 norm of gradients: 0.13961693162338243; l2 norm of weights: 0.45687240462381423\n","Iteration #3617  Loss: 0.5051397911228583; l2 norm of gradients: 0.1394642599027423; l2 norm of weights: 0.45681587066568846\n","Iteration #3618  Loss: 0.5051149067225393; l2 norm of gradients: 0.13931175673022092; l2 norm of weights: 0.45675943398885305\n","Iteration #3619  Loss: 0.5050900663044616; l2 norm of gradients: 0.13915942191887115; l2 norm of weights: 0.45670309441668094\n","Iteration #3620  Loss: 0.5050652697753502; l2 norm of gradients: 0.13900725528195484; l2 norm of weights: 0.4566468517728446\n","Iteration #3621  Loss: 0.5050405170421347; l2 norm of gradients: 0.1388552566329424; l2 norm of weights: 0.45659070588131523\n","Iteration #3622  Loss: 0.5050158080119495; l2 norm of gradients: 0.13870342578551276; l2 norm of weights: 0.45653465656636305\n","Iteration #3623  Loss: 0.5049911425921322; l2 norm of gradients: 0.1385517625535529; l2 norm of weights: 0.456478703652556\n","Iteration #3624  Loss: 0.5049665206902242; l2 norm of gradients: 0.13840026675115774; l2 norm of weights: 0.45642284696476015\n","Iteration #3625  Loss: 0.5049419422139696; l2 norm of gradients: 0.1382489381926299; l2 norm of weights: 0.45636708632813866\n","Iteration #3626  Loss: 0.504917407071315; l2 norm of gradients: 0.1380977766924796; l2 norm of weights: 0.4563114215681518\n","Iteration #3627  Loss: 0.5048929151704091; l2 norm of gradients: 0.13794678206542407; l2 norm of weights: 0.45625585251055645\n","Iteration #3628  Loss: 0.5048684664196023; l2 norm of gradients: 0.13779595412638773; l2 norm of weights: 0.45620037898140564\n","Iteration #3629  Loss: 0.5048440607274459; l2 norm of gradients: 0.13764529269050169; l2 norm of weights: 0.4561450008070481\n","Iteration #3630  Loss: 0.5048196980026919; l2 norm of gradients: 0.1374947975731035; l2 norm of weights: 0.4560897178141279\n","Iteration #3631  Loss: 0.5047953781542929; l2 norm of gradients: 0.13734446858973728; l2 norm of weights: 0.45603452982958415\n","Iteration #3632  Loss: 0.5047711010914006; l2 norm of gradients: 0.13719430555615295; l2 norm of weights: 0.4559794366806505\n","Iteration #3633  Loss: 0.5047468667233669; l2 norm of gradients: 0.13704430828830638; l2 norm of weights: 0.4559244381948548\n","Iteration #3634  Loss: 0.504722674959742; l2 norm of gradients: 0.13689447660235912; l2 norm of weights: 0.4558695342000185\n","Iteration #3635  Loss: 0.5046985257102751; l2 norm of gradients: 0.136744810314678; l2 norm of weights: 0.4558147245242567\n","Iteration #3636  Loss: 0.5046744188849127; l2 norm of gradients: 0.13659530924183502; l2 norm of weights: 0.45576000899597713\n","Iteration #3637  Loss: 0.5046503543937997; l2 norm of gradients: 0.13644597320060714; l2 norm of weights: 0.4557053874438804\n","Iteration #3638  Loss: 0.5046263321472777; l2 norm of gradients: 0.136296802007976; l2 norm of weights: 0.45565085969695895\n","Iteration #3639  Loss: 0.5046023520558852; l2 norm of gradients: 0.13614779548112768; l2 norm of weights: 0.45559642558449726\n","Iteration #3640  Loss: 0.504578414030357; l2 norm of gradients: 0.13599895343745255; l2 norm of weights: 0.45554208493607107\n","Iteration #3641  Loss: 0.5045545179816235; l2 norm of gradients: 0.13585027569454483; l2 norm of weights: 0.45548783758154704\n","Iteration #3642  Loss: 0.504530663820811; l2 norm of gradients: 0.13570176207020276; l2 norm of weights: 0.4554336833510824\n","Iteration #3643  Loss: 0.5045068514592405; l2 norm of gradients: 0.13555341238242788; l2 norm of weights: 0.4553796220751245\n","Iteration #3644  Loss: 0.5044830808084275; l2 norm of gradients: 0.13540522644942524; l2 norm of weights: 0.4553256535844107\n","Iteration #3645  Loss: 0.5044593517800818; l2 norm of gradients: 0.13525720408960287; l2 norm of weights: 0.45527177770996735\n","Iteration #3646  Loss: 0.5044356642861069; l2 norm of gradients: 0.1351093451215717; l2 norm of weights: 0.4552179942831101\n","Iteration #3647  Loss: 0.5044120182385996; l2 norm of gradients: 0.1349616493641452; l2 norm of weights: 0.45516430313544304\n","Iteration #3648  Loss: 0.5043884135498492; l2 norm of gradients: 0.1348141166363394; l2 norm of weights: 0.4551107040988583\n","Iteration #3649  Loss: 0.504364850132338; l2 norm of gradients: 0.13466674675737253; l2 norm of weights: 0.45505719700553593\n","Iteration #3650  Loss: 0.5043413278987401; l2 norm of gradients: 0.1345195395466646; l2 norm of weights: 0.45500378168794336\n","Iteration #3651  Loss: 0.5043178467619208; l2 norm of gradients: 0.1343724948238374; l2 norm of weights: 0.454950457978835\n","Iteration #3652  Loss: 0.5042944066349369; l2 norm of gradients: 0.13422561240871433; l2 norm of weights: 0.4548972257112517\n","Iteration #3653  Loss: 0.504271007431036; l2 norm of gradients: 0.13407889212131996; l2 norm of weights: 0.45484408471852067\n","Iteration #3654  Loss: 0.5042476490636559; l2 norm of gradients: 0.1339323337818799; l2 norm of weights: 0.4547910348342548\n","Iteration #3655  Loss: 0.5042243314464241; l2 norm of gradients: 0.13378593721082072; l2 norm of weights: 0.45473807589235243\n","Iteration #3656  Loss: 0.5042010544931579; l2 norm of gradients: 0.1336397022287694; l2 norm of weights: 0.4546852077269968\n","Iteration #3657  Loss: 0.5041778181178633; l2 norm of gradients: 0.13349362865655348; l2 norm of weights: 0.45463243017265587\n","Iteration #3658  Loss: 0.5041546222347354; l2 norm of gradients: 0.13334771631520045; l2 norm of weights: 0.45457974306408166\n","Iteration #3659  Loss: 0.504131466758157; l2 norm of gradients: 0.13320196502593795; l2 norm of weights: 0.45452714623631024\n","Iteration #3660  Loss: 0.504108351602699; l2 norm of gradients: 0.1330563746101931; l2 norm of weights: 0.4544746395246607\n","Iteration #3661  Loss: 0.5040852766831194; l2 norm of gradients: 0.13291094488959265; l2 norm of weights: 0.4544222227647355\n","Iteration #3662  Loss: 0.5040622419143638; l2 norm of gradients: 0.1327656756859625; l2 norm of weights: 0.45436989579241954\n","Iteration #3663  Loss: 0.5040392472115637; l2 norm of gradients: 0.13262056682132767; l2 norm of weights: 0.45431765844387995\n","Iteration #3664  Loss: 0.5040162924900368; l2 norm of gradients: 0.13247561811791195; l2 norm of weights: 0.45426551055556574\n","Iteration #3665  Loss: 0.503993377665287; l2 norm of gradients: 0.13233082939813764; l2 norm of weights: 0.45421345196420726\n","Iteration #3666  Loss: 0.503970502653003; l2 norm of gradients: 0.13218620048462562; l2 norm of weights: 0.45416148250681615\n","Iteration #3667  Loss: 0.5039476673690587; l2 norm of gradients: 0.13204173120019458; l2 norm of weights: 0.4541096020206843\n","Iteration #3668  Loss: 0.5039248717295126; l2 norm of gradients: 0.13189742136786142; l2 norm of weights: 0.45405781034338427\n","Iteration #3669  Loss: 0.5039021156506069; l2 norm of gradients: 0.13175327081084054; l2 norm of weights: 0.4540061073127681\n","Iteration #3670  Loss: 0.5038793990487679; l2 norm of gradients: 0.1316092793525439; l2 norm of weights: 0.45395449276696753\n","Iteration #3671  Loss: 0.5038567218406048; l2 norm of gradients: 0.1314654468165807; l2 norm of weights: 0.45390296654439327\n","Iteration #3672  Loss: 0.5038340839429101; l2 norm of gradients: 0.13132177302675715; l2 norm of weights: 0.4538515284837347\n","Iteration #3673  Loss: 0.5038114852726582; l2 norm of gradients: 0.13117825780707637; l2 norm of weights: 0.45380017842395953\n","Iteration #3674  Loss: 0.5037889257470063; l2 norm of gradients: 0.13103490098173784; l2 norm of weights: 0.45374891620431346\n","Iteration #3675  Loss: 0.5037664052832925; l2 norm of gradients: 0.13089170237513761; l2 norm of weights: 0.4536977416643194\n","Iteration #3676  Loss: 0.5037439237990369; l2 norm of gradients: 0.13074866181186784; l2 norm of weights: 0.4536466546437777\n","Iteration #3677  Loss: 0.5037214812119399; l2 norm of gradients: 0.13060577911671656; l2 norm of weights: 0.45359565498276516\n","Iteration #3678  Loss: 0.5036990774398826; l2 norm of gradients: 0.13046305411466755; l2 norm of weights: 0.45354474252163496\n","Iteration #3679  Loss: 0.5036767124009262; l2 norm of gradients: 0.1303204866309; l2 norm of weights: 0.4534939171010164\n","Iteration #3680  Loss: 0.5036543860133113; l2 norm of gradients: 0.13017807649078847; l2 norm of weights: 0.45344317856181393\n","Iteration #3681  Loss: 0.5036320981954581; l2 norm of gradients: 0.13003582351990253; l2 norm of weights: 0.45339252674520764\n","Iteration #3682  Loss: 0.5036098488659656; l2 norm of gradients: 0.12989372754400658; l2 norm of weights: 0.453341961492652\n","Iteration #3683  Loss: 0.5035876379436112; l2 norm of gradients: 0.12975178838905957; l2 norm of weights: 0.45329148264587604\n","Iteration #3684  Loss: 0.5035654653473501; l2 norm of gradients: 0.12961000588121496; l2 norm of weights: 0.45324109004688257\n","Iteration #3685  Loss: 0.503543330996316; l2 norm of gradients: 0.12946837984682033; l2 norm of weights: 0.45319078353794817\n","Iteration #3686  Loss: 0.503521234809819; l2 norm of gradients: 0.12932691011241715; l2 norm of weights: 0.4531405629616225\n","Iteration #3687  Loss: 0.5034991767073467; l2 norm of gradients: 0.12918559650474074; l2 norm of weights: 0.45309042816072814\n","Iteration #3688  Loss: 0.5034771566085627; l2 norm of gradients: 0.1290444388507199; l2 norm of weights: 0.45304037897835997\n","Iteration #3689  Loss: 0.5034551744333073; l2 norm of gradients: 0.1289034369774767; l2 norm of weights: 0.4529904152578851\n","Iteration #3690  Loss: 0.5034332301015965; l2 norm of gradients: 0.1287625907123263; l2 norm of weights: 0.4529405368429419\n","Iteration #3691  Loss: 0.5034113235336207; l2 norm of gradients: 0.12862189988277678; l2 norm of weights: 0.4528907435774403\n","Iteration #3692  Loss: 0.5033894546497466; l2 norm of gradients: 0.12848136431652887; l2 norm of weights: 0.45284103530556113\n","Iteration #3693  Loss: 0.5033676233705147; l2 norm of gradients: 0.12834098384147571; l2 norm of weights: 0.4527914118717553\n","Iteration #3694  Loss: 0.5033458296166397; l2 norm of gradients: 0.12820075828570265; l2 norm of weights: 0.45274187312074426\n","Iteration #3695  Loss: 0.5033240733090101; l2 norm of gradients: 0.1280606874774871; l2 norm of weights: 0.452692418897519\n","Iteration #3696  Loss: 0.5033023543686882; l2 norm of gradients: 0.12792077124529821; l2 norm of weights: 0.45264304904733954\n","Iteration #3697  Loss: 0.5032806727169089; l2 norm of gradients: 0.12778100941779674; l2 norm of weights: 0.45259376341573526\n","Iteration #3698  Loss: 0.5032590282750802; l2 norm of gradients: 0.1276414018238348; l2 norm of weights: 0.4525445618485038\n","Iteration #3699  Loss: 0.5032374209647817; l2 norm of gradients: 0.12750194829245562; l2 norm of weights: 0.45249544419171106\n","Iteration #3700  Loss: 0.5032158507077655; l2 norm of gradients: 0.12736264865289346; l2 norm of weights: 0.4524464102916906\n","Iteration #3701  Loss: 0.5031943174259552; l2 norm of gradients: 0.12722350273457309; l2 norm of weights: 0.45239745999504344\n","Iteration #3702  Loss: 0.5031728210414449; l2 norm of gradients: 0.12708451036711005; l2 norm of weights: 0.4523485931486375\n","Iteration #3703  Loss: 0.5031513614765002; l2 norm of gradients: 0.12694567138031002; l2 norm of weights: 0.4522998095996075\n","Iteration #3704  Loss: 0.5031299386535567; l2 norm of gradients: 0.12680698560416875; l2 norm of weights: 0.45225110919535416\n","Iteration #3705  Loss: 0.5031085524952197; l2 norm of gradients: 0.1266684528688719; l2 norm of weights: 0.452202491783544\n","Iteration #3706  Loss: 0.503087202924265; l2 norm of gradients: 0.12653007300479477; l2 norm of weights: 0.45215395721210927\n","Iteration #3707  Loss: 0.5030658898636367; l2 norm of gradients: 0.12639184584250207; l2 norm of weights: 0.4521055053292469\n","Iteration #3708  Loss: 0.5030446132364484; l2 norm of gradients: 0.12625377121274778; l2 norm of weights: 0.452057135983419\n","Iteration #3709  Loss: 0.5030233729659821; l2 norm of gradients: 0.12611584894647482; l2 norm of weights: 0.4520088490233514\n","Iteration #3710  Loss: 0.5030021689756874; l2 norm of gradients: 0.12597807887481505; l2 norm of weights: 0.4519606442980342\n","Iteration #3711  Loss: 0.5029810011891824; l2 norm of gradients: 0.12584046082908873; l2 norm of weights: 0.4519125216567208\n","Iteration #3712  Loss: 0.5029598695302523; l2 norm of gradients: 0.1257029946408047; l2 norm of weights: 0.45186448094892817\n","Iteration #3713  Loss: 0.5029387739228492; l2 norm of gradients: 0.1255656801416598; l2 norm of weights: 0.45181652202443545\n","Iteration #3714  Loss: 0.5029177142910919; l2 norm of gradients: 0.12542851716353892; l2 norm of weights: 0.45176864473328454\n","Iteration #3715  Loss: 0.5028966905592653; l2 norm of gradients: 0.12529150553851465; l2 norm of weights: 0.45172084892577924\n","Iteration #3716  Loss: 0.5028757026518207; l2 norm of gradients: 0.12515464509884713; l2 norm of weights: 0.451673134452485\n","Iteration #3717  Loss: 0.5028547504933745; l2 norm of gradients: 0.1250179356769838; l2 norm of weights: 0.45162550116422845\n","Iteration #3718  Loss: 0.5028338340087084; l2 norm of gradients: 0.1248813771055593; l2 norm of weights: 0.45157794891209696\n","Iteration #3719  Loss: 0.5028129531227692; l2 norm of gradients: 0.124744969217395; l2 norm of weights: 0.4515304775474386\n","Iteration #3720  Loss: 0.5027921077606675; l2 norm of gradients: 0.1246087118454992; l2 norm of weights: 0.45148308692186123\n","Iteration #3721  Loss: 0.5027712978476788; l2 norm of gradients: 0.12447260482306648; l2 norm of weights: 0.4514357768872327\n","Iteration #3722  Loss: 0.5027505233092417; l2 norm of gradients: 0.12433664798347775; l2 norm of weights: 0.45138854729567995\n","Iteration #3723  Loss: 0.5027297840709583; l2 norm of gradients: 0.12420084116030006; l2 norm of weights: 0.451341397999589\n","Iteration #3724  Loss: 0.5027090800585938; l2 norm of gradients: 0.12406518418728621; l2 norm of weights: 0.4512943288516045\n","Iteration #3725  Loss: 0.5026884111980761; l2 norm of gradients: 0.12392967689837475; l2 norm of weights: 0.45124733970462894\n","Iteration #3726  Loss: 0.5026677774154953; l2 norm of gradients: 0.1237943191276896; l2 norm of weights: 0.4512004304118231\n","Iteration #3727  Loss: 0.5026471786371031; l2 norm of gradients: 0.12365911070953989; l2 norm of weights: 0.45115360082660466\n","Iteration #3728  Loss: 0.5026266147893135; l2 norm of gradients: 0.12352405147841987; l2 norm of weights: 0.45110685080264873\n","Iteration #3729  Loss: 0.5026060857987009; l2 norm of gradients: 0.12338914126900853; l2 norm of weights: 0.45106018019388694\n","Iteration #3730  Loss: 0.5025855915920011; l2 norm of gradients: 0.1232543799161695; l2 norm of weights: 0.45101358885450715\n","Iteration #3731  Loss: 0.5025651320961104; l2 norm of gradients: 0.12311976725495079; l2 norm of weights: 0.45096707663895336\n","Iteration #3732  Loss: 0.5025447072380849; l2 norm of gradients: 0.12298530312058462; l2 norm of weights: 0.4509206434019246\n","Iteration #3733  Loss: 0.5025243169451404; l2 norm of gradients: 0.12285098734848722; l2 norm of weights: 0.4508742889983756\n","Iteration #3734  Loss: 0.5025039611446529; l2 norm of gradients: 0.12271681977425854; l2 norm of weights: 0.4508280132835156\n","Iteration #3735  Loss: 0.5024836397641566; l2 norm of gradients: 0.12258280023368218; l2 norm of weights: 0.45078181611280826\n","Iteration #3736  Loss: 0.502463352731345; l2 norm of gradients: 0.12244892856272502; l2 norm of weights: 0.45073569734197116\n","Iteration #3737  Loss: 0.5024430999740697; l2 norm of gradients: 0.12231520459753722; l2 norm of weights: 0.4506896568269758\n","Iteration #3738  Loss: 0.5024228814203404; l2 norm of gradients: 0.12218162817445177; l2 norm of weights: 0.45064369442404667\n","Iteration #3739  Loss: 0.5024026969983244; l2 norm of gradients: 0.12204819912998453; l2 norm of weights: 0.45059780998966115\n","Iteration #3740  Loss: 0.5023825466363467; l2 norm of gradients: 0.12191491730083386; l2 norm of weights: 0.45055200338054935\n","Iteration #3741  Loss: 0.5023624302628886; l2 norm of gradients: 0.12178178252388039; l2 norm of weights: 0.4505062744536933\n","Iteration #3742  Loss: 0.502342347806589; l2 norm of gradients: 0.12164879463618704; l2 norm of weights: 0.450460623066327\n","Iteration #3743  Loss: 0.5023222991962422; l2 norm of gradients: 0.12151595347499848; l2 norm of weights: 0.45041504907593555\n","Iteration #3744  Loss: 0.5023022843607986; l2 norm of gradients: 0.12138325887774132; l2 norm of weights: 0.4503695523402553\n","Iteration #3745  Loss: 0.5022823032293648; l2 norm of gradients: 0.12125071068202352; l2 norm of weights: 0.45032413271727323\n","Iteration #3746  Loss: 0.5022623557312019; l2 norm of gradients: 0.12111830872563442; l2 norm of weights: 0.45027879006522636\n","Iteration #3747  Loss: 0.5022424417957265; l2 norm of gradients: 0.12098605284654453; l2 norm of weights: 0.45023352424260193\n","Iteration #3748  Loss: 0.5022225613525095; l2 norm of gradients: 0.12085394288290519; l2 norm of weights: 0.4501883351081364\n","Iteration #3749  Loss: 0.5022027143312762; l2 norm of gradients: 0.12072197867304854; l2 norm of weights: 0.45014322252081557\n","Iteration #3750  Loss: 0.5021829006619052; l2 norm of gradients: 0.12059016005548713; l2 norm of weights: 0.45009818633987386\n","Iteration #3751  Loss: 0.5021631202744294; l2 norm of gradients: 0.12045848686891392; l2 norm of weights: 0.4500532264247941\n","Iteration #3752  Loss: 0.5021433730990349; l2 norm of gradients: 0.12032695895220188; l2 norm of weights: 0.4500083426353074\n","Iteration #3753  Loss: 0.5021236590660598; l2 norm of gradients: 0.12019557614440397; l2 norm of weights: 0.4499635348313922\n","Iteration #3754  Loss: 0.5021039781059959; l2 norm of gradients: 0.12006433828475281; l2 norm of weights: 0.4499188028732744\n","Iteration #3755  Loss: 0.5020843301494863; l2 norm of gradients: 0.11993324521266045; l2 norm of weights: 0.4498741466214266\n","Iteration #3756  Loss: 0.5020647151273263; l2 norm of gradients: 0.11980229676771836; l2 norm of weights: 0.44982956593656837\n","Iteration #3757  Loss: 0.5020451329704627; l2 norm of gradients: 0.11967149278969706; l2 norm of weights: 0.44978506067966484\n","Iteration #3758  Loss: 0.5020255836099938; l2 norm of gradients: 0.11954083311854594; l2 norm of weights: 0.44974063071192755\n","Iteration #3759  Loss: 0.5020060669771683; l2 norm of gradients: 0.11941031759439309; l2 norm of weights: 0.44969627589481304\n","Iteration #3760  Loss: 0.5019865830033855; l2 norm of gradients: 0.11927994605754513; l2 norm of weights: 0.4496519960900231\n","Iteration #3761  Loss: 0.5019671316201949; l2 norm of gradients: 0.11914971834848692; l2 norm of weights: 0.4496077911595041\n","Iteration #3762  Loss: 0.5019477127592964; l2 norm of gradients: 0.1190196343078815; l2 norm of weights: 0.44956366096544687\n","Iteration #3763  Loss: 0.5019283263525385; l2 norm of gradients: 0.11888969377656963; l2 norm of weights: 0.44951960537028607\n","Iteration #3764  Loss: 0.5019089723319194; l2 norm of gradients: 0.11875989659557; l2 norm of weights: 0.4494756242366999\n","Iteration #3765  Loss: 0.5018896506295867; l2 norm of gradients: 0.11863024260607861; l2 norm of weights: 0.44943171742760996\n","Iteration #3766  Loss: 0.5018703611778353; l2 norm of gradients: 0.11850073164946887; l2 norm of weights: 0.4493878848061805\n","Iteration #3767  Loss: 0.5018511039091095; l2 norm of gradients: 0.11837136356729117; l2 norm of weights: 0.44934412623581843\n","Iteration #3768  Loss: 0.5018318787560005; l2 norm of gradients: 0.11824213820127291; l2 norm of weights: 0.4493004415801725\n","Iteration #3769  Loss: 0.5018126856512481; l2 norm of gradients: 0.11811305539331814; l2 norm of weights: 0.44925683070313355\n","Iteration #3770  Loss: 0.5017935245277385; l2 norm of gradients: 0.1179841149855074; l2 norm of weights: 0.4492132934688335\n","Iteration #3771  Loss: 0.5017743953185049; l2 norm of gradients: 0.11785531682009755; l2 norm of weights: 0.4491698297416453\n","Iteration #3772  Loss: 0.5017552979567277; l2 norm of gradients: 0.11772666073952157; l2 norm of weights: 0.4491264393861829\n","Iteration #3773  Loss: 0.5017362323757327; l2 norm of gradients: 0.11759814658638833; l2 norm of weights: 0.44908312226729996\n","Iteration #3774  Loss: 0.5017171985089924; l2 norm of gradients: 0.1174697742034824; l2 norm of weights: 0.4490398782500905\n","Iteration #3775  Loss: 0.5016981962901244; l2 norm of gradients: 0.11734154343376385; l2 norm of weights: 0.448996707199888\n","Iteration #3776  Loss: 0.5016792256528918; l2 norm of gradients: 0.11721345412036814; l2 norm of weights: 0.4489536089822649\n","Iteration #3777  Loss: 0.5016602865312026; l2 norm of gradients: 0.11708550610660574; l2 norm of weights: 0.44891058346303264\n","Iteration #3778  Loss: 0.5016413788591096; l2 norm of gradients: 0.11695769923596214; l2 norm of weights: 0.44886763050824113\n","Iteration #3779  Loss: 0.50162250257081; l2 norm of gradients: 0.11683003335209746; l2 norm of weights: 0.44882474998417826\n","Iteration #3780  Loss: 0.5016036576006444; l2 norm of gradients: 0.1167025082988465; l2 norm of weights: 0.44878194175736974\n","Iteration #3781  Loss: 0.5015848438830978; l2 norm of gradients: 0.11657512392021824; l2 norm of weights: 0.44873920569457854\n","Iteration #3782  Loss: 0.5015660613527982; l2 norm of gradients: 0.11644788006039587; l2 norm of weights: 0.44869654166280465\n","Iteration #3783  Loss: 0.501547309944517; l2 norm of gradients: 0.11632077656373649; l2 norm of weights: 0.4486539495292849\n","Iteration #3784  Loss: 0.5015285895931679; l2 norm of gradients: 0.116193813274771; l2 norm of weights: 0.44861142916149205\n","Iteration #3785  Loss: 0.5015099002338073; l2 norm of gradients: 0.11606699003820381; l2 norm of weights: 0.448568980427135\n","Iteration #3786  Loss: 0.5014912418016337; l2 norm of gradients: 0.11594030669891268; l2 norm of weights: 0.44852660319415816\n","Iteration #3787  Loss: 0.5014726142319873; l2 norm of gradients: 0.1158137631019486; l2 norm of weights: 0.448484297330741\n","Iteration #3788  Loss: 0.5014540174603501; l2 norm of gradients: 0.11568735909253547; l2 norm of weights: 0.4484420627052981\n","Iteration #3789  Loss: 0.5014354514223447; l2 norm of gradients: 0.11556109451606994; l2 norm of weights: 0.4483998991864781\n","Iteration #3790  Loss: 0.501416916053735; l2 norm of gradients: 0.11543496921812134; l2 norm of weights: 0.44835780664316416\n","Iteration #3791  Loss: 0.5013984112904255; l2 norm of gradients: 0.11530898304443124; l2 norm of weights: 0.44831578494447283\n","Iteration #3792  Loss: 0.5013799370684606; l2 norm of gradients: 0.11518313584091357; l2 norm of weights: 0.4482738339597543\n","Iteration #3793  Loss: 0.501361493324025; l2 norm of gradients: 0.11505742745365415; l2 norm of weights: 0.44823195355859163\n","Iteration #3794  Loss: 0.5013430799934427; l2 norm of gradients: 0.11493185772891065; l2 norm of weights: 0.4481901436108006\n","Iteration #3795  Loss: 0.5013246970131772; l2 norm of gradients: 0.11480642651311232; l2 norm of weights: 0.44814840398642936\n","Iteration #3796  Loss: 0.501306344319831; l2 norm of gradients: 0.11468113365285988; l2 norm of weights: 0.4481067345557579\n","Iteration #3797  Loss: 0.5012880218501452; l2 norm of gradients: 0.11455597899492526; l2 norm of weights: 0.4480651351892978\n","Iteration #3798  Loss: 0.5012697295409996; l2 norm of gradients: 0.1144309623862514; l2 norm of weights: 0.44802360575779193\n","Iteration #3799  Loss: 0.5012514673294115; l2 norm of gradients: 0.11430608367395213; l2 norm of weights: 0.4479821461322141\n","Iteration #3800  Loss: 0.5012332351525366; l2 norm of gradients: 0.1141813427053119; l2 norm of weights: 0.44794075618376844\n","Iteration #3801  Loss: 0.5012150329476677; l2 norm of gradients: 0.11405673932778564; l2 norm of weights: 0.44789943578388947\n","Iteration #3802  Loss: 0.501196860652235; l2 norm of gradients: 0.11393227338899853; l2 norm of weights: 0.44785818480424133\n","Iteration #3803  Loss: 0.5011787182038054; l2 norm of gradients: 0.11380794473674588; l2 norm of weights: 0.4478170031167176\n","Iteration #3804  Loss: 0.5011606055400825; l2 norm of gradients: 0.11368375321899285; l2 norm of weights: 0.44777589059344114\n","Iteration #3805  Loss: 0.501142522598906; l2 norm of gradients: 0.1135596986838743; l2 norm of weights: 0.4477348471067634\n","Iteration #3806  Loss: 0.5011244693182517; l2 norm of gradients: 0.11343578097969458; l2 norm of weights: 0.4476938725292642\n","Iteration #3807  Loss: 0.5011064456362312; l2 norm of gradients: 0.11331199995492738; l2 norm of weights: 0.44765296673375143\n","Iteration #3808  Loss: 0.5010884514910912; l2 norm of gradients: 0.11318835545821555; l2 norm of weights: 0.4476121295932607\n","Iteration #3809  Loss: 0.5010704868212135; l2 norm of gradients: 0.11306484733837084; l2 norm of weights: 0.4475713609810549\n","Iteration #3810  Loss: 0.5010525515651149; l2 norm of gradients: 0.11294147544437373; l2 norm of weights: 0.4475306607706237\n","Iteration #3811  Loss: 0.5010346456614465; l2 norm of gradients: 0.11281823962537331; l2 norm of weights: 0.4474900288356837\n","Iteration #3812  Loss: 0.5010167690489935; l2 norm of gradients: 0.11269513973068701; l2 norm of weights: 0.4474494650501775\n","Iteration #3813  Loss: 0.5009989216666753; l2 norm of gradients: 0.11257217560980039; l2 norm of weights: 0.44740896928827373\n","Iteration #3814  Loss: 0.5009811034535445; l2 norm of gradients: 0.11244934711236713; l2 norm of weights: 0.4473685414243666\n","Iteration #3815  Loss: 0.5009633143487874; l2 norm of gradients: 0.11232665408820859; l2 norm of weights: 0.4473281813330755\n","Iteration #3816  Loss: 0.5009455542917227; l2 norm of gradients: 0.11220409638731382; l2 norm of weights: 0.44728788888924437\n","Iteration #3817  Loss: 0.5009278232218025; l2 norm of gradients: 0.11208167385983926; l2 norm of weights: 0.44724766396794213\n","Iteration #3818  Loss: 0.5009101210786107; l2 norm of gradients: 0.1119593863561086; l2 norm of weights: 0.44720750644446156\n","Iteration #3819  Loss: 0.5008924478018637; l2 norm of gradients: 0.11183723372661258; l2 norm of weights: 0.4471674161943193\n","Iteration #3820  Loss: 0.5008748033314099; l2 norm of gradients: 0.1117152158220088; l2 norm of weights: 0.44712739309325533\n","Iteration #3821  Loss: 0.5008571876072284; l2 norm of gradients: 0.11159333249312156; l2 norm of weights: 0.44708743701723275\n","Iteration #3822  Loss: 0.5008396005694306; l2 norm of gradients: 0.11147158359094163; l2 norm of weights: 0.4470475478424377\n","Iteration #3823  Loss: 0.500822042158258; l2 norm of gradients: 0.11134996896662606; l2 norm of weights: 0.4470077254452782\n","Iteration #3824  Loss: 0.5008045123140835; l2 norm of gradients: 0.11122848847149806; l2 norm of weights: 0.4469679697023849\n","Iteration #3825  Loss: 0.5007870109774095; l2 norm of gradients: 0.11110714195704675; l2 norm of weights: 0.44692828049060956\n","Iteration #3826  Loss: 0.5007695380888693; l2 norm of gradients: 0.110985929274927; l2 norm of weights: 0.44688865768702557\n","Iteration #3827  Loss: 0.5007520935892257; l2 norm of gradients: 0.1108648502769592; l2 norm of weights: 0.44684910116892745\n","Iteration #3828  Loss: 0.5007346774193708; l2 norm of gradients: 0.11074390481512918; l2 norm of weights: 0.4468096108138301\n","Iteration #3829  Loss: 0.5007172895203263; l2 norm of gradients: 0.11062309274158788; l2 norm of weights: 0.44677018649946876\n","Iteration #3830  Loss: 0.5006999298332427; l2 norm of gradients: 0.1105024139086513; l2 norm of weights: 0.44673082810379894\n","Iteration #3831  Loss: 0.5006825982993991; l2 norm of gradients: 0.11038186816880025; l2 norm of weights: 0.4466915355049954\n","Iteration #3832  Loss: 0.5006652948602027; l2 norm of gradients: 0.11026145537468011; l2 norm of weights: 0.4466523085814523\n","Iteration #3833  Loss: 0.5006480194571895; l2 norm of gradients: 0.11014117537910079; l2 norm of weights: 0.44661314721178264\n","Iteration #3834  Loss: 0.5006307720320228; l2 norm of gradients: 0.11002102803503641; l2 norm of weights: 0.4465740512748181\n","Iteration #3835  Loss: 0.5006135525264934; l2 norm of gradients: 0.10990101319562516; l2 norm of weights: 0.4465350206496086\n","Iteration #3836  Loss: 0.5005963608825196; l2 norm of gradients: 0.10978113071416917; l2 norm of weights: 0.44649605521542174\n","Iteration #3837  Loss: 0.5005791970421465; l2 norm of gradients: 0.10966138044413423; l2 norm of weights: 0.4464571548517429\n","Iteration #3838  Loss: 0.500562060947546; l2 norm of gradients: 0.10954176223914966; l2 norm of weights: 0.44641831943827454\n","Iteration #3839  Loss: 0.5005449525410162; l2 norm of gradients: 0.10942227595300817; l2 norm of weights: 0.4463795488549359\n","Iteration #3840  Loss: 0.5005278717649816; l2 norm of gradients: 0.10930292143966557; l2 norm of weights: 0.4463408429818629\n","Iteration #3841  Loss: 0.5005108185619922; l2 norm of gradients: 0.10918369855324069; l2 norm of weights: 0.4463022016994075\n","Iteration #3842  Loss: 0.5004937928747241; l2 norm of gradients: 0.1090646071480151; l2 norm of weights: 0.44626362488813726\n","Iteration #3843  Loss: 0.500476794645978; l2 norm of gradients: 0.10894564707843302; l2 norm of weights: 0.4462251124288356\n","Iteration #3844  Loss: 0.5004598238186801; l2 norm of gradients: 0.10882681819910106; l2 norm of weights: 0.4461866642025011\n","Iteration #3845  Loss: 0.5004428803358816; l2 norm of gradients: 0.10870812036478815; l2 norm of weights: 0.44614828009034674\n","Iteration #3846  Loss: 0.5004259641407571; l2 norm of gradients: 0.1085895534304252; l2 norm of weights: 0.44610995997380004\n","Iteration #3847  Loss: 0.5004090751766068; l2 norm of gradients: 0.10847111725110509; l2 norm of weights: 0.446071703734503\n","Iteration #3848  Loss: 0.5003922133868535; l2 norm of gradients: 0.10835281168208226; l2 norm of weights: 0.446033511254311\n","Iteration #3849  Loss: 0.5003753787150444; l2 norm of gradients: 0.10823463657877279; l2 norm of weights: 0.4459953824152929\n","Iteration #3850  Loss: 0.5003585711048502; l2 norm of gradients: 0.10811659179675404; l2 norm of weights: 0.4459573170997309\n","Iteration #3851  Loss: 0.500341790500064; l2 norm of gradients: 0.10799867719176454; l2 norm of weights: 0.4459193151901196\n","Iteration #3852  Loss: 0.5003250368446023; l2 norm of gradients: 0.10788089261970382; l2 norm of weights: 0.4458813765691663\n","Iteration #3853  Loss: 0.500308310082504; l2 norm of gradients: 0.10776323793663219; l2 norm of weights: 0.4458435011197901\n","Iteration #3854  Loss: 0.5002916101579301; l2 norm of gradients: 0.1076457129987705; l2 norm of weights: 0.44580568872512205\n","Iteration #3855  Loss: 0.500274937015164; l2 norm of gradients: 0.10752831766250012; l2 norm of weights: 0.44576793926850455\n","Iteration #3856  Loss: 0.5002582905986106; l2 norm of gradients: 0.10741105178436268; l2 norm of weights: 0.44573025263349086\n","Iteration #3857  Loss: 0.5002416708527961; l2 norm of gradients: 0.10729391522105983; l2 norm of weights: 0.44569262870384524\n","Iteration #3858  Loss: 0.500225077722368; l2 norm of gradients: 0.10717690782945309; l2 norm of weights: 0.4456550673635421\n","Iteration #3859  Loss: 0.5002085111520955; l2 norm of gradients: 0.1070600294665638; l2 norm of weights: 0.4456175684967662\n","Iteration #3860  Loss: 0.5001919710868671; l2 norm of gradients: 0.10694327998957273; l2 norm of weights: 0.44558013198791147\n","Iteration #3861  Loss: 0.5001754574716926; l2 norm of gradients: 0.10682665925582009; l2 norm of weights: 0.4455427577215817\n","Iteration #3862  Loss: 0.5001589702517021; l2 norm of gradients: 0.10671016712280518; l2 norm of weights: 0.4455054455825895\n","Iteration #3863  Loss: 0.5001425093721448; l2 norm of gradients: 0.10659380344818634; l2 norm of weights: 0.4454681954559563\n","Iteration #3864  Loss: 0.5001260747783902; l2 norm of gradients: 0.10647756808978073; l2 norm of weights: 0.4454310072269116\n","Iteration #3865  Loss: 0.5001096664159268; l2 norm of gradients: 0.10636146090556421; l2 norm of weights: 0.4453938807808933\n","Iteration #3866  Loss: 0.5000932842303624; l2 norm of gradients: 0.10624548175367096; l2 norm of weights: 0.4453568160035468\n","Iteration #3867  Loss: 0.5000769281674233; l2 norm of gradients: 0.10612963049239357; l2 norm of weights: 0.44531981278072486\n","Iteration #3868  Loss: 0.5000605981729546; l2 norm of gradients: 0.10601390698018272; l2 norm of weights: 0.4452828709984872\n","Iteration #3869  Loss: 0.5000442941929198; l2 norm of gradients: 0.10589831107564697; l2 norm of weights: 0.44524599054310043\n","Iteration #3870  Loss: 0.5000280161734003; l2 norm of gradients: 0.10578284263755267; l2 norm of weights: 0.4452091713010374\n","Iteration #3871  Loss: 0.500011764060595; l2 norm of gradients: 0.10566750152482372; l2 norm of weights: 0.4451724131589769\n","Iteration #3872  Loss: 0.49999553780082084; l2 norm of gradients: 0.10555228759654144; l2 norm of weights: 0.4451357160038035\n","Iteration #3873  Loss: 0.4999793373405117; l2 norm of gradients: 0.10543720071194441; l2 norm of weights: 0.44509907972260726\n","Iteration #3874  Loss: 0.4999631626262185; l2 norm of gradients: 0.10532224073042816; l2 norm of weights: 0.445062504202683\n","Iteration #3875  Loss: 0.4999470136046089; l2 norm of gradients: 0.1052074075115452; l2 norm of weights: 0.44502598933153054\n","Iteration #3876  Loss: 0.49993089022246706; l2 norm of gradients: 0.10509270091500462; l2 norm of weights: 0.44498953499685373\n","Iteration #3877  Loss: 0.49991479242669346; l2 norm of gradients: 0.10497812080067213; l2 norm of weights: 0.4449531410865607\n","Iteration #3878  Loss: 0.49989872016430426; l2 norm of gradients: 0.1048636670285697; l2 norm of weights: 0.4449168074887633\n","Iteration #3879  Loss: 0.49988267338243153; l2 norm of gradients: 0.10474933945887552; l2 norm of weights: 0.44488053409177647\n","Iteration #3880  Loss: 0.49986665202832314; l2 norm of gradients: 0.10463513795192379; l2 norm of weights: 0.44484432078411873\n","Iteration #3881  Loss: 0.49985065604934154; l2 norm of gradients: 0.10452106236820445; l2 norm of weights: 0.44480816745451063\n","Iteration #3882  Loss: 0.4998346853929643; l2 norm of gradients: 0.10440711256836314; l2 norm of weights: 0.4447720739918757\n","Iteration #3883  Loss: 0.49981874000678395; l2 norm of gradients: 0.10429328841320097; l2 norm of weights: 0.4447360402853393\n","Iteration #3884  Loss: 0.49980281983850716; l2 norm of gradients: 0.10417958976367431; l2 norm of weights: 0.4447000662242284\n","Iteration #3885  Loss: 0.4997869248359552; l2 norm of gradients: 0.10406601648089467; l2 norm of weights: 0.4446641516980716\n","Iteration #3886  Loss: 0.49977105494706275; l2 norm of gradients: 0.10395256842612846; l2 norm of weights: 0.4446282965965985\n","Iteration #3887  Loss: 0.49975521011987845; l2 norm of gradients: 0.10383924546079694; l2 norm of weights: 0.4445925008097395\n","Iteration #3888  Loss: 0.49973939030256453; l2 norm of gradients: 0.1037260474464759; l2 norm of weights: 0.44455676422762513\n","Iteration #3889  Loss: 0.49972359544339595; l2 norm of gradients: 0.10361297424489563; l2 norm of weights: 0.44452108674058644\n","Iteration #3890  Loss: 0.49970782549076115; l2 norm of gradients: 0.1035000257179405; l2 norm of weights: 0.44448546823915397\n","Iteration #3891  Loss: 0.4996920803931607; l2 norm of gradients: 0.10338720172764919; l2 norm of weights: 0.44444990861405786\n","Iteration #3892  Loss: 0.49967636009920824; l2 norm of gradients: 0.10327450213621411; l2 norm of weights: 0.4444144077562274\n","Iteration #3893  Loss: 0.49966066455762903; l2 norm of gradients: 0.10316192680598146; l2 norm of weights: 0.4443789655567904\n","Iteration #3894  Loss: 0.49964499371726057; l2 norm of gradients: 0.10304947559945103; l2 norm of weights: 0.4443435819070735\n","Iteration #3895  Loss: 0.4996293475270517; l2 norm of gradients: 0.10293714837927591; l2 norm of weights: 0.44430825669860124\n","Iteration #3896  Loss: 0.49961372593606346; l2 norm of gradients: 0.10282494500826249; l2 norm of weights: 0.44427298982309615\n","Iteration #3897  Loss: 0.49959812889346733; l2 norm of gradients: 0.10271286534937019; l2 norm of weights: 0.44423778117247814\n","Iteration #3898  Loss: 0.4995825563485461; l2 norm of gradients: 0.10260090926571122; l2 norm of weights: 0.44420263063886434\n","Iteration #3899  Loss: 0.49956700825069306; l2 norm of gradients: 0.1024890766205506; l2 norm of weights: 0.4441675381145688\n","Iteration #3900  Loss: 0.49955148454941234; l2 norm of gradients: 0.10237736727730579; l2 norm of weights: 0.44413250349210215\n","Iteration #3901  Loss: 0.49953598519431786; l2 norm of gradients: 0.10226578109954666; l2 norm of weights: 0.4440975266641709\n","Iteration #3902  Loss: 0.499520510135134; l2 norm of gradients: 0.10215431795099526; l2 norm of weights: 0.44406260752367793\n","Iteration #3903  Loss: 0.49950505932169437; l2 norm of gradients: 0.10204297769552562; l2 norm of weights: 0.44402774596372135\n","Iteration #3904  Loss: 0.4994896327039422; l2 norm of gradients: 0.10193176019716357; l2 norm of weights: 0.44399294187759464\n","Iteration #3905  Loss: 0.49947423023193044; l2 norm of gradients: 0.10182066532008684; l2 norm of weights: 0.44395819515878615\n","Iteration #3906  Loss: 0.4994588518558204; l2 norm of gradients: 0.10170969292862435; l2 norm of weights: 0.44392350570097905\n","Iteration #3907  Loss: 0.49944349752588274; l2 norm of gradients: 0.10159884288725657; l2 norm of weights: 0.44388887339805055\n","Iteration #3908  Loss: 0.49942816719249594; l2 norm of gradients: 0.10148811506061507; l2 norm of weights: 0.44385429814407196\n","Iteration #3909  Loss: 0.4994128608061477; l2 norm of gradients: 0.10137750931348238; l2 norm of weights: 0.4438197798333082\n","Iteration #3910  Loss: 0.4993975783174329; l2 norm of gradients: 0.10126702551079196; l2 norm of weights: 0.44378531836021756\n","Iteration #3911  Loss: 0.4993823196770548; l2 norm of gradients: 0.10115666351762778; l2 norm of weights: 0.4437509136194513\n","Iteration #3912  Loss: 0.49936708483582404; l2 norm of gradients: 0.10104642319922438; l2 norm of weights: 0.44371656550585337\n","Iteration #3913  Loss: 0.49935187374465856; l2 norm of gradients: 0.10093630442096661; l2 norm of weights: 0.4436822739144603\n","Iteration #3914  Loss: 0.4993366863545834; l2 norm of gradients: 0.1008263070483895; l2 norm of weights: 0.44364803874050046\n","Iteration #3915  Loss: 0.4993215226167308; l2 norm of gradients: 0.10071643094717797; l2 norm of weights: 0.443613859879394\n","Iteration #3916  Loss: 0.49930638248233905; l2 norm of gradients: 0.10060667598316683; l2 norm of weights: 0.44357973722675254\n","Iteration #3917  Loss: 0.4992912659027533; l2 norm of gradients: 0.10049704202234051; l2 norm of weights: 0.44354567067837886\n","Iteration #3918  Loss: 0.49927617282942466; l2 norm of gradients: 0.10038752893083293; l2 norm of weights: 0.4435116601302665\n","Iteration #3919  Loss: 0.49926110321391015; l2 norm of gradients: 0.10027813657492725; l2 norm of weights: 0.44347770547859955\n","Iteration #3920  Loss: 0.4992460570078727; l2 norm of gradients: 0.1001688648210559; l2 norm of weights: 0.44344380661975213\n","Iteration #3921  Loss: 0.49923103416308046; l2 norm of gradients: 0.10005971353580011; l2 norm of weights: 0.4434099634502883\n","Iteration #3922  Loss: 0.49921603463140685; l2 norm of gradients: 0.09995068258589011; l2 norm of weights: 0.44337617586696176\n","Iteration #3923  Loss: 0.49920105836483036; l2 norm of gradients: 0.09984177183820461; l2 norm of weights: 0.4433424437667153\n","Iteration #3924  Loss: 0.49918610531543445; l2 norm of gradients: 0.09973298115977088; l2 norm of weights: 0.44330876704668065\n","Iteration #3925  Loss: 0.4991711754354066; l2 norm of gradients: 0.09962431041776447; l2 norm of weights: 0.44327514560417824\n","Iteration #3926  Loss: 0.49915626867703894; l2 norm of gradients: 0.09951575947950905; l2 norm of weights: 0.4432415793367167\n","Iteration #3927  Loss: 0.49914138499272764; l2 norm of gradients: 0.0994073282124763; l2 norm of weights: 0.4432080681419927\n","Iteration #3928  Loss: 0.4991265243349727; l2 norm of gradients: 0.09929901648428566; l2 norm of weights: 0.4431746119178904\n","Iteration #3929  Loss: 0.4991116866563776; l2 norm of gradients: 0.09919082416270425; l2 norm of weights: 0.4431412105624817\n","Iteration #3930  Loss: 0.4990968719096496; l2 norm of gradients: 0.0990827511156467; l2 norm of weights: 0.44310786397402513\n","Iteration #3931  Loss: 0.4990820800475984; l2 norm of gradients: 0.09897479721117482; l2 norm of weights: 0.4430745720509662\n","Iteration #3932  Loss: 0.4990673110231375; l2 norm of gradients: 0.09886696231749771; l2 norm of weights: 0.4430413346919368\n","Iteration #3933  Loss: 0.4990525647892824; l2 norm of gradients: 0.09875924630297139; l2 norm of weights: 0.4430081517957548\n","Iteration #3934  Loss: 0.4990378412991515; l2 norm of gradients: 0.09865164903609865; l2 norm of weights: 0.44297502326142424\n","Iteration #3935  Loss: 0.4990231405059653; l2 norm of gradients: 0.09854417038552901; l2 norm of weights: 0.44294194898813416\n","Iteration #3936  Loss: 0.4990084623630464; l2 norm of gradients: 0.09843681022005842; l2 norm of weights: 0.44290892887525923\n","Iteration #3937  Loss: 0.4989938068238188; l2 norm of gradients: 0.09832956840862915; l2 norm of weights: 0.44287596282235864\n","Iteration #3938  Loss: 0.49897917384180873; l2 norm of gradients: 0.09822244482032967; l2 norm of weights: 0.4428430507291765\n","Iteration #3939  Loss: 0.49896456337064327; l2 norm of gradients: 0.09811543932439445; l2 norm of weights: 0.44281019249564096\n","Iteration #3940  Loss: 0.49894997536405084; l2 norm of gradients: 0.09800855179020368; l2 norm of weights: 0.442777388021864\n","Iteration #3941  Loss: 0.4989354097758607; l2 norm of gradients: 0.09790178208728337; l2 norm of weights: 0.44274463720814167\n","Iteration #3942  Loss: 0.498920866560003; l2 norm of gradients: 0.09779513008530488; l2 norm of weights: 0.442711939954953\n","Iteration #3943  Loss: 0.49890634567050807; l2 norm of gradients: 0.09768859565408503; l2 norm of weights: 0.44267929616296026\n","Iteration #3944  Loss: 0.4988918470615063; l2 norm of gradients: 0.09758217866358575; l2 norm of weights: 0.4426467057330083\n","Iteration #3945  Loss: 0.4988773706872288; l2 norm of gradients: 0.09747587898391401; l2 norm of weights: 0.44261416856612446\n","Iteration #3946  Loss: 0.49886291650200587; l2 norm of gradients: 0.09736969648532162; l2 norm of weights: 0.4425816845635183\n","Iteration #3947  Loss: 0.49884848446026764; l2 norm of gradients: 0.0972636310382051; l2 norm of weights: 0.44254925362658104\n","Iteration #3948  Loss: 0.4988340745165434; l2 norm of gradients: 0.09715768251310541; l2 norm of weights: 0.44251687565688547\n","Iteration #3949  Loss: 0.4988196866254618; l2 norm of gradients: 0.09705185078070798; l2 norm of weights: 0.44248455055618574\n","Iteration #3950  Loss: 0.49880532074175027; l2 norm of gradients: 0.09694613571184238; l2 norm of weights: 0.4424522782264166\n","Iteration #3951  Loss: 0.4987909768202351; l2 norm of gradients: 0.09684053717748226; l2 norm of weights: 0.4424200585696937\n","Iteration #3952  Loss: 0.4987766548158407; l2 norm of gradients: 0.0967350550487451; l2 norm of weights: 0.4423878914883128\n","Iteration #3953  Loss: 0.49876235468359037; l2 norm of gradients: 0.09662968919689212; l2 norm of weights: 0.44235577688474975\n","Iteration #3954  Loss: 0.4987480763786049; l2 norm of gradients: 0.09652443949332809; l2 norm of weights: 0.44232371466166004\n","Iteration #3955  Loss: 0.498733819856103; l2 norm of gradients: 0.09641930580960122; l2 norm of weights: 0.4422917047218785\n","Iteration #3956  Loss: 0.49871958507140124; l2 norm of gradients: 0.09631428801740281; l2 norm of weights: 0.4422597469684194\n","Iteration #3957  Loss: 0.4987053719799133; l2 norm of gradients: 0.09620938598856744; l2 norm of weights: 0.44222784130447534\n","Iteration #3958  Loss: 0.4986911805371505; l2 norm of gradients: 0.09610459959507243; l2 norm of weights: 0.4421959876334176\n","Iteration #3959  Loss: 0.4986770106987207; l2 norm of gradients: 0.09599992870903798; l2 norm of weights: 0.44216418585879574\n","Iteration #3960  Loss: 0.49866286242032853; l2 norm of gradients: 0.09589537320272672; l2 norm of weights: 0.4421324358843372\n","Iteration #3961  Loss: 0.4986487356577754; l2 norm of gradients: 0.09579093294854388; l2 norm of weights: 0.44210073761394675\n","Iteration #3962  Loss: 0.498634630366959; l2 norm of gradients: 0.09568660781903683; l2 norm of weights: 0.44206909095170677\n","Iteration #3963  Loss: 0.4986205465038731; l2 norm of gradients: 0.09558239768689522; l2 norm of weights: 0.4420374958018766\n","Iteration #3964  Loss: 0.49860648402460706; l2 norm of gradients: 0.09547830242495041; l2 norm of weights: 0.44200595206889204\n","Iteration #3965  Loss: 0.4985924428853466; l2 norm of gradients: 0.09537432190617574; l2 norm of weights: 0.44197445965736554\n","Iteration #3966  Loss: 0.49857842304237254; l2 norm of gradients: 0.09527045600368618; l2 norm of weights: 0.44194301847208556\n","Iteration #3967  Loss: 0.49856442445206084; l2 norm of gradients: 0.09516670459073805; l2 norm of weights: 0.4419116284180163\n","Iteration #3968  Loss: 0.4985504470708829; l2 norm of gradients: 0.09506306754072909; l2 norm of weights: 0.4418802894002976\n","Iteration #3969  Loss: 0.49853649085540463; l2 norm of gradients: 0.09495954472719814; l2 norm of weights: 0.4418490013242444\n","Iteration #3970  Loss: 0.4985225557622868; l2 norm of gradients: 0.09485613602382513; l2 norm of weights: 0.4418177640953466\n","Iteration #3971  Loss: 0.4985086417482845; l2 norm of gradients: 0.09475284130443073; l2 norm of weights: 0.44178657761926876\n","Iteration #3972  Loss: 0.49849474877024713; l2 norm of gradients: 0.09464966044297633; l2 norm of weights: 0.4417554418018496\n","Iteration #3973  Loss: 0.4984808767851181; l2 norm of gradients: 0.09454659331356385; l2 norm of weights: 0.44172435654910225\n","Iteration #3974  Loss: 0.49846702574993457; l2 norm of gradients: 0.09444363979043559; l2 norm of weights: 0.44169332176721304\n","Iteration #3975  Loss: 0.4984531956218274; l2 norm of gradients: 0.09434079974797403; l2 norm of weights: 0.4416623373625421\n","Iteration #3976  Loss: 0.4984393863580209; l2 norm of gradients: 0.09423807306070171; l2 norm of weights: 0.4416314032416228\n","Iteration #3977  Loss: 0.4984255979158323; l2 norm of gradients: 0.0941354596032811; l2 norm of weights: 0.44160051931116096\n","Iteration #3978  Loss: 0.49841183025267205; l2 norm of gradients: 0.09403295925051436; l2 norm of weights: 0.44156968547803527\n","Iteration #3979  Loss: 0.4983980833260435; l2 norm of gradients: 0.09393057187734326; l2 norm of weights: 0.4415389016492967\n","Iteration #3980  Loss: 0.4983843570935424; l2 norm of gradients: 0.09382829735884897; l2 norm of weights: 0.44150816773216806\n","Iteration #3981  Loss: 0.49837065151285675; l2 norm of gradients: 0.09372613557025201; l2 norm of weights: 0.44147748363404393\n","Iteration #3982  Loss: 0.4983569665417669; l2 norm of gradients: 0.09362408638691191; l2 norm of weights: 0.4414468492624902\n","Iteration #3983  Loss: 0.4983433021381453; l2 norm of gradients: 0.09352214968432719; l2 norm of weights: 0.44141626452524396\n","Iteration #3984  Loss: 0.49832965825995584; l2 norm of gradients: 0.09342032533813524; l2 norm of weights: 0.441385729330213\n","Iteration #3985  Loss: 0.4983160348652544; l2 norm of gradients: 0.09331861322411193; l2 norm of weights: 0.44135524358547573\n","Iteration #3986  Loss: 0.49830243191218765; l2 norm of gradients: 0.09321701321817183; l2 norm of weights: 0.4413248071992808\n","Iteration #3987  Loss: 0.49828884935899403; l2 norm of gradients: 0.09311552519636768; l2 norm of weights: 0.44129442008004677\n","Iteration #3988  Loss: 0.4982752871640025; l2 norm of gradients: 0.09301414903489041; l2 norm of weights: 0.44126408213636176\n","Iteration #3989  Loss: 0.49826174528563305; l2 norm of gradients: 0.09291288461006915; l2 norm of weights: 0.44123379327698337\n","Iteration #3990  Loss: 0.498248223682396; l2 norm of gradients: 0.0928117317983707; l2 norm of weights: 0.4412035534108384\n","Iteration #3991  Loss: 0.4982347223128922; l2 norm of gradients: 0.09271069047639963; l2 norm of weights: 0.441173362447022\n","Iteration #3992  Loss: 0.4982212411358127; l2 norm of gradients: 0.0926097605208981; l2 norm of weights: 0.44114322029479835\n","Iteration #3993  Loss: 0.4982077801099382; l2 norm of gradients: 0.09250894180874569; l2 norm of weights: 0.4411131268635995\n","Iteration #3994  Loss: 0.4981943391941396; l2 norm of gradients: 0.09240823421695918; l2 norm of weights: 0.4410830820630255\n","Iteration #3995  Loss: 0.498180918347377; l2 norm of gradients: 0.09230763762269248; l2 norm of weights: 0.441053085802844\n","Iteration #3996  Loss: 0.49816751752870025; l2 norm of gradients: 0.0922071519032364; l2 norm of weights: 0.44102313799299037\n","Iteration #3997  Loss: 0.49815413669724795; l2 norm of gradients: 0.09210677693601861; l2 norm of weights: 0.4409932385435664\n","Iteration #3998  Loss: 0.49814077581224814; l2 norm of gradients: 0.09200651259860339; l2 norm of weights: 0.44096338736484125\n","Iteration #3999  Loss: 0.49812743483301686; l2 norm of gradients: 0.09190635876869148; l2 norm of weights: 0.4409335843672501\n","Iteration #4000  Loss: 0.49811411371895975; l2 norm of gradients: 0.09180631532411997; l2 norm of weights: 0.4409038294613948\n","Iteration #4001  Loss: 0.4981008124295701; l2 norm of gradients: 0.0917063821428621; l2 norm of weights: 0.4408741225580427\n","Iteration #4002  Loss: 0.49808753092442976; l2 norm of gradients: 0.09160655910302719; l2 norm of weights: 0.44084446356812723\n","Iteration #4003  Loss: 0.49807426916320835; l2 norm of gradients: 0.09150684608286039; l2 norm of weights: 0.4408148524027468\n","Iteration #4004  Loss: 0.49806102710566336; l2 norm of gradients: 0.09140724296074257; l2 norm of weights: 0.44078528897316493\n","Iteration #4005  Loss: 0.49804780471164; l2 norm of gradients: 0.09130774961519021; l2 norm of weights: 0.4407557731908104\n","Iteration #4006  Loss: 0.4980346019410711; l2 norm of gradients: 0.09120836592485514; l2 norm of weights: 0.44072630496727583\n","Iteration #4007  Loss: 0.49802141875397576; l2 norm of gradients: 0.0911090917685245; l2 norm of weights: 0.44069688421431846\n","Iteration #4008  Loss: 0.4980082551104614; l2 norm of gradients: 0.09100992702512054; l2 norm of weights: 0.44066751084385947\n","Iteration #4009  Loss: 0.4979951109707215; l2 norm of gradients: 0.09091087157370041; l2 norm of weights: 0.4406381847679838\n","Iteration #4010  Loss: 0.4979819862950363; l2 norm of gradients: 0.09081192529345618; l2 norm of weights: 0.44060890589893925\n","Iteration #4011  Loss: 0.4979688810437727; l2 norm of gradients: 0.09071308806371448; l2 norm of weights: 0.44057967414913746\n","Iteration #4012  Loss: 0.49795579517738364; l2 norm of gradients: 0.09061435976393648; l2 norm of weights: 0.4405504894311523\n","Iteration #4013  Loss: 0.4979427286564083; l2 norm of gradients: 0.09051574027371774; l2 norm of weights: 0.4405213516577206\n","Iteration #4014  Loss: 0.49792968144147154; l2 norm of gradients: 0.09041722947278796; l2 norm of weights: 0.44049226074174114\n","Iteration #4015  Loss: 0.4979166534932843; l2 norm of gradients: 0.09031882724101095; l2 norm of weights: 0.4404632165962749\n","Iteration #4016  Loss: 0.49790364477264265; l2 norm of gradients: 0.09022053345838438; l2 norm of weights: 0.44043421913454445\n","Iteration #4017  Loss: 0.4978906552404282; l2 norm of gradients: 0.09012234800503972; l2 norm of weights: 0.44040526826993387\n","Iteration #4018  Loss: 0.4978776848576075; l2 norm of gradients: 0.09002427076124202; l2 norm of weights: 0.4403763639159884\n","Iteration #4019  Loss: 0.4978647335852324; l2 norm of gradients: 0.08992630160738975; l2 norm of weights: 0.4403475059864141\n","Iteration #4020  Loss: 0.49785180138443913; l2 norm of gradients: 0.08982844042401474; l2 norm of weights: 0.4403186943950775\n","Iteration #4021  Loss: 0.4978388882164486; l2 norm of gradients: 0.08973068709178196; l2 norm of weights: 0.4402899290560058\n","Iteration #4022  Loss: 0.4978259940425663; l2 norm of gradients: 0.08963304149148936; l2 norm of weights: 0.4402612098833859\n","Iteration #4023  Loss: 0.49781311882418167; l2 norm of gradients: 0.08953550350406782; l2 norm of weights: 0.44023253679156465\n","Iteration #4024  Loss: 0.4978002625227683; l2 norm of gradients: 0.0894380730105808; l2 norm of weights: 0.44020390969504836\n","Iteration #4025  Loss: 0.4977874250998836; l2 norm of gradients: 0.08934074989222443; l2 norm of weights: 0.44017532850850255\n","Iteration #4026  Loss: 0.49777460651716865; l2 norm of gradients: 0.0892435340303272; l2 norm of weights: 0.44014679314675165\n","Iteration #4027  Loss: 0.49776180673634773; l2 norm of gradients: 0.08914642530634985; l2 norm of weights: 0.4401183035247788\n","Iteration #4028  Loss: 0.49774902571922885; l2 norm of gradients: 0.08904942360188532; l2 norm of weights: 0.4400898595577255\n","Iteration #4029  Loss: 0.49773626342770294; l2 norm of gradients: 0.08895252879865839; l2 norm of weights: 0.4400614611608915\n","Iteration #4030  Loss: 0.4977235198237435; l2 norm of gradients: 0.08885574077852575; l2 norm of weights: 0.4400331082497342\n","Iteration #4031  Loss: 0.49771079486940717; l2 norm of gradients: 0.08875905942347571; l2 norm of weights: 0.4400048007398687\n","Iteration #4032  Loss: 0.49769808852683306; l2 norm of gradients: 0.0886624846156281; l2 norm of weights: 0.43997653854706725\n","Iteration #4033  Loss: 0.4976854007582426; l2 norm of gradients: 0.0885660162372342; l2 norm of weights: 0.43994832158725944\n","Iteration #4034  Loss: 0.49767273152593916; l2 norm of gradients: 0.0884696541706764; l2 norm of weights: 0.4399201497765312\n","Iteration #4035  Loss: 0.4976600807923086; l2 norm of gradients: 0.08837339829846823; l2 norm of weights: 0.4398920230311253\n","Iteration #4036  Loss: 0.4976474485198182; l2 norm of gradients: 0.08827724850325418; l2 norm of weights: 0.4398639412674407\n","Iteration #4037  Loss: 0.49763483467101716; l2 norm of gradients: 0.08818120466780949; l2 norm of weights: 0.43983590440203196\n","Iteration #4038  Loss: 0.49762223920853577; l2 norm of gradients: 0.08808526667504002; l2 norm of weights: 0.4398079123516097\n","Iteration #4039  Loss: 0.49760966209508595; l2 norm of gradients: 0.08798943440798218; l2 norm of weights: 0.4397799650330398\n","Iteration #4040  Loss: 0.4975971032934606; l2 norm of gradients: 0.08789370774980265; l2 norm of weights: 0.4397520623633431\n","Iteration #4041  Loss: 0.49758456276653357; l2 norm of gradients: 0.08779808658379838; l2 norm of weights: 0.43972420425969555\n","Iteration #4042  Loss: 0.49757204047725945; l2 norm of gradients: 0.08770257079339634; l2 norm of weights: 0.4396963906394277\n","Iteration #4043  Loss: 0.49755953638867323; l2 norm of gradients: 0.08760716026215343; l2 norm of weights: 0.43966862142002433\n","Iteration #4044  Loss: 0.4975470504638906; l2 norm of gradients: 0.08751185487375625; l2 norm of weights: 0.4396408965191242\n","Iteration #4045  Loss: 0.4975345826661073; l2 norm of gradients: 0.0874166545120211; l2 norm of weights: 0.43961321585451996\n","Iteration #4046  Loss: 0.4975221329585988; l2 norm of gradients: 0.08732155906089371; l2 norm of weights: 0.43958557934415776\n","Iteration #4047  Loss: 0.4975097013047211; l2 norm of gradients: 0.0872265684044491; l2 norm of weights: 0.43955798690613707\n","Iteration #4048  Loss: 0.4974972876679094; l2 norm of gradients: 0.08713168242689155; l2 norm of weights: 0.43953043845871026\n","Iteration #4049  Loss: 0.49748489201167845; l2 norm of gradients: 0.08703690101255436; l2 norm of weights: 0.4395029339202825\n","Iteration #4050  Loss: 0.4974725142996225; l2 norm of gradients: 0.08694222404589966; l2 norm of weights: 0.43947547320941127\n","Iteration #4051  Loss: 0.4974601544954148; l2 norm of gradients: 0.08684765141151833; l2 norm of weights: 0.4394480562448065\n","Iteration #4052  Loss: 0.49744781256280757; l2 norm of gradients: 0.08675318299412994; l2 norm of weights: 0.43942068294532977\n","Iteration #4053  Loss: 0.497435488465632; l2 norm of gradients: 0.08665881867858248; l2 norm of weights: 0.4393933532299945\n","Iteration #4054  Loss: 0.4974231821677979; l2 norm of gradients: 0.08656455834985219; l2 norm of weights: 0.43936606701796543\n","Iteration #4055  Loss: 0.4974108936332933; l2 norm of gradients: 0.08647040189304356; l2 norm of weights: 0.43933882422855847\n","Iteration #4056  Loss: 0.49739862282618486; l2 norm of gradients: 0.08637634919338907; l2 norm of weights: 0.4393116247812403\n","Iteration #4057  Loss: 0.497386369710617; l2 norm of gradients: 0.0862824001362491; l2 norm of weights: 0.43928446859562836\n","Iteration #4058  Loss: 0.49737413425081245; l2 norm of gradients: 0.08618855460711175; l2 norm of weights: 0.43925735559149043\n","Iteration #4059  Loss: 0.4973619164110715; l2 norm of gradients: 0.08609481249159272; l2 norm of weights: 0.4392302856887441\n","Iteration #4060  Loss: 0.49734971615577195; l2 norm of gradients: 0.0860011736754352; l2 norm of weights: 0.43920325880745714\n","Iteration #4061  Loss: 0.4973375334493693; l2 norm of gradients: 0.08590763804450961; l2 norm of weights: 0.43917627486784666\n","Iteration #4062  Loss: 0.49732536825639606; l2 norm of gradients: 0.08581420548481365; l2 norm of weights: 0.43914933379027915\n","Iteration #4063  Loss: 0.4973132205414621; l2 norm of gradients: 0.08572087588247193; l2 norm of weights: 0.43912243549527\n","Iteration #4064  Loss: 0.49730109026925384; l2 norm of gradients: 0.08562764912373604; l2 norm of weights: 0.4390955799034836\n","Iteration #4065  Loss: 0.4972889774045347; l2 norm of gradients: 0.08553452509498422; l2 norm of weights: 0.4390687669357326\n","Iteration #4066  Loss: 0.4972768819121448; l2 norm of gradients: 0.08544150368272137; l2 norm of weights: 0.4390419965129781\n","Iteration #4067  Loss: 0.49726480375700033; l2 norm of gradients: 0.08534858477357887; l2 norm of weights: 0.43901526855632905\n","Iteration #4068  Loss: 0.4972527429040941; l2 norm of gradients: 0.08525576825431433; l2 norm of weights: 0.43898858298704235\n","Iteration #4069  Loss: 0.4972406993184946; l2 norm of gradients: 0.08516305401181161; l2 norm of weights: 0.43896193972652203\n","Iteration #4070  Loss: 0.49722867296534645; l2 norm of gradients: 0.08507044193308054; l2 norm of weights: 0.4389353386963197\n","Iteration #4071  Loss: 0.4972166638098702; l2 norm of gradients: 0.08497793190525689; l2 norm of weights: 0.43890877981813353\n","Iteration #4072  Loss: 0.4972046718173615; l2 norm of gradients: 0.08488552381560215; l2 norm of weights: 0.4388822630138088\n","Iteration #4073  Loss: 0.4971926969531919; l2 norm of gradients: 0.08479321755150342; l2 norm of weights: 0.43885578820533694\n","Iteration #4074  Loss: 0.49718073918280786; l2 norm of gradients: 0.08470101300047331; l2 norm of weights: 0.4388293553148558\n","Iteration #4075  Loss: 0.4971687984717311; l2 norm of gradients: 0.08460891005014967; l2 norm of weights: 0.4388029642646489\n","Iteration #4076  Loss: 0.49715687478555803; l2 norm of gradients: 0.08451690858829564; l2 norm of weights: 0.4387766149771456\n","Iteration #4077  Loss: 0.49714496808996017; l2 norm of gradients: 0.08442500850279933; l2 norm of weights: 0.4387503073749206\n","Iteration #4078  Loss: 0.4971330783506832; l2 norm of gradients: 0.08433320968167374; l2 norm of weights: 0.4387240413806939\n","Iteration #4079  Loss: 0.49712120553354755; l2 norm of gradients: 0.08424151201305673; l2 norm of weights: 0.4386978169173301\n","Iteration #4080  Loss: 0.49710934960444775; l2 norm of gradients: 0.08414991538521067; l2 norm of weights: 0.43867163390783886\n","Iteration #4081  Loss: 0.49709751052935247; l2 norm of gradients: 0.08405841968652253; l2 norm of weights: 0.43864549227537386\n","Iteration #4082  Loss: 0.4970856882743043; l2 norm of gradients: 0.08396702480550354; l2 norm of weights: 0.4386193919432333\n","Iteration #4083  Loss: 0.4970738828054195; l2 norm of gradients: 0.08387573063078921; l2 norm of weights: 0.4385933328348588\n","Iteration #4084  Loss: 0.4970620940888881; l2 norm of gradients: 0.083784537051139; l2 norm of weights: 0.4385673148738359\n","Iteration #4085  Loss: 0.49705032209097333; l2 norm of gradients: 0.08369344395543642; l2 norm of weights: 0.4385413379838936\n","Iteration #4086  Loss: 0.497038566778012; l2 norm of gradients: 0.08360245123268872; l2 norm of weights: 0.4385154020889039\n","Iteration #4087  Loss: 0.49702682811641385; l2 norm of gradients: 0.0835115587720268; l2 norm of weights: 0.43848950711288154\n","Iteration #4088  Loss: 0.4970151060726616; l2 norm of gradients: 0.08342076646270506; l2 norm of weights: 0.4384636529799842\n","Iteration #4089  Loss: 0.49700340061331083; l2 norm of gradients: 0.08333007419410132; l2 norm of weights: 0.4384378396145117\n","Iteration #4090  Loss: 0.4969917117049897; l2 norm of gradients: 0.08323948185571658; l2 norm of weights: 0.43841206694090606\n","Iteration #4091  Loss: 0.49698003931439866; l2 norm of gradients: 0.08314898933717495; l2 norm of weights: 0.43838633488375106\n","Iteration #4092  Loss: 0.4969683834083108; l2 norm of gradients: 0.08305859652822352; l2 norm of weights: 0.43836064336777225\n","Iteration #4093  Loss: 0.49695674395357126; l2 norm of gradients: 0.08296830331873219; l2 norm of weights: 0.43833499231783657\n","Iteration #4094  Loss: 0.49694512091709714; l2 norm of gradients: 0.08287810959869359; l2 norm of weights: 0.43830938165895184\n","Iteration #4095  Loss: 0.4969335142658771; l2 norm of gradients: 0.08278801525822276; l2 norm of weights: 0.438283811316267\n","Iteration #4096  Loss: 0.4969219239669722; l2 norm of gradients: 0.08269802018755729; l2 norm of weights: 0.43825828121507154\n","Iteration #4097  Loss: 0.4969103499875144; l2 norm of gradients: 0.08260812427705702; l2 norm of weights: 0.4382327912807953\n","Iteration #4098  Loss: 0.4968987922947071; l2 norm of gradients: 0.08251832741720386; l2 norm of weights: 0.4382073414390082\n","Iteration #4099  Loss: 0.49688725085582497; l2 norm of gradients: 0.08242862949860175; l2 norm of weights: 0.4381819316154201\n","Iteration #4100  Loss: 0.49687572563821397; l2 norm of gradients: 0.08233903041197647; l2 norm of weights: 0.4381565617358804\n","Iteration #4101  Loss: 0.4968642166092908; l2 norm of gradients: 0.0822495300481756; l2 norm of weights: 0.43813123172637813\n","Iteration #4102  Loss: 0.49685272373654266; l2 norm of gradients: 0.08216012829816823; l2 norm of weights: 0.4381059415130411\n","Iteration #4103  Loss: 0.4968412469875277; l2 norm of gradients: 0.08207082505304489; l2 norm of weights: 0.43808069102213626\n","Iteration #4104  Loss: 0.496829786329874; l2 norm of gradients: 0.08198162020401752; l2 norm of weights: 0.43805548018006907\n","Iteration #4105  Loss: 0.4968183417312807; l2 norm of gradients: 0.08189251364241912; l2 norm of weights: 0.4380303089133835\n","Iteration #4106  Loss: 0.49680691315951614; l2 norm of gradients: 0.08180350525970387; l2 norm of weights: 0.4380051771487615\n","Iteration #4107  Loss: 0.4967955005824194; l2 norm of gradients: 0.08171459494744669; l2 norm of weights: 0.43798008481302325\n","Iteration #4108  Loss: 0.49678410396789896; l2 norm of gradients: 0.0816257825973434; l2 norm of weights: 0.4379550318331261\n","Iteration #4109  Loss: 0.496772723283933; l2 norm of gradients: 0.08153706810121043; l2 norm of weights: 0.43793001813616533\n","Iteration #4110  Loss: 0.4967613584985692; l2 norm of gradients: 0.0814484513509847; l2 norm of weights: 0.43790504364937294\n","Iteration #4111  Loss: 0.49675000957992466; l2 norm of gradients: 0.0813599322387235; l2 norm of weights: 0.43788010830011814\n","Iteration #4112  Loss: 0.49673867649618575; l2 norm of gradients: 0.08127151065660432; l2 norm of weights: 0.4378552120159069\n","Iteration #4113  Loss: 0.4967273592156077; l2 norm of gradients: 0.0811831864969248; l2 norm of weights: 0.4378303547243812\n","Iteration #4114  Loss: 0.49671605770651456; l2 norm of gradients: 0.08109495965210248; l2 norm of weights: 0.43780553635331976\n","Iteration #4115  Loss: 0.49670477193729945; l2 norm of gradients: 0.0810068300146748; l2 norm of weights: 0.43778075683063683\n","Iteration #4116  Loss: 0.4966935018764239; l2 norm of gradients: 0.08091879747729885; l2 norm of weights: 0.43775601608438275\n","Iteration #4117  Loss: 0.49668224749241785; l2 norm of gradients: 0.0808308619327513; l2 norm of weights: 0.4377313140427429\n","Iteration #4118  Loss: 0.49667100875387954; l2 norm of gradients: 0.08074302327392818; l2 norm of weights: 0.4377066506340382\n","Iteration #4119  Loss: 0.4966597856294753; l2 norm of gradients: 0.0806552813938449; l2 norm of weights: 0.43768202578672455\n","Iteration #4120  Loss: 0.4966485780879396; l2 norm of gradients: 0.080567636185636; l2 norm of weights: 0.43765743942939245\n","Iteration #4121  Loss: 0.4966373860980746; l2 norm of gradients: 0.080480087542555; l2 norm of weights: 0.43763289149076706\n","Iteration #4122  Loss: 0.4966262096287501; l2 norm of gradients: 0.08039263535797436; l2 norm of weights: 0.4376083818997077\n","Iteration #4123  Loss: 0.4966150486489038; l2 norm of gradients: 0.08030527952538528; l2 norm of weights: 0.4375839105852078\n","Iteration #4124  Loss: 0.4966039031275401; l2 norm of gradients: 0.08021801993839758; l2 norm of weights: 0.4375594774763945\n","Iteration #4125  Loss: 0.4965927730337316; l2 norm of gradients: 0.08013085649073959; l2 norm of weights: 0.4375350825025286\n","Iteration #4126  Loss: 0.49658165833661705; l2 norm of gradients: 0.08004378907625796; l2 norm of weights: 0.43751072559300414\n","Iteration #4127  Loss: 0.4965705590054027; l2 norm of gradients: 0.07995681758891762; l2 norm of weights: 0.4374864066773484\n","Iteration #4128  Loss: 0.49655947500936143; l2 norm of gradients: 0.07986994192280153; l2 norm of weights: 0.4374621256852211\n","Iteration #4129  Loss: 0.49654840631783276; l2 norm of gradients: 0.07978316197211067; l2 norm of weights: 0.437437882546415\n","Iteration #4130  Loss: 0.4965373529002229; l2 norm of gradients: 0.0796964776311638; l2 norm of weights: 0.4374136771908551\n","Iteration #4131  Loss: 0.496526314726004; l2 norm of gradients: 0.07960988879439745; l2 norm of weights: 0.4373895095485983\n","Iteration #4132  Loss: 0.4965152917647149; l2 norm of gradients: 0.0795233953563656; l2 norm of weights: 0.43736537954983373\n","Iteration #4133  Loss: 0.4965042839859603; l2 norm of gradients: 0.07943699721173975; l2 norm of weights: 0.437341287124882\n","Iteration #4134  Loss: 0.4964932913594107; l2 norm of gradients: 0.07935069425530868; l2 norm of weights: 0.4373172322041951\n","Iteration #4135  Loss: 0.49648231385480257; l2 norm of gradients: 0.07926448638197833; l2 norm of weights: 0.4372932147183564\n","Iteration #4136  Loss: 0.4964713514419379; l2 norm of gradients: 0.0791783734867717; l2 norm of weights: 0.43726923459808004\n","Iteration #4137  Loss: 0.49646040409068415; l2 norm of gradients: 0.07909235546482868; l2 norm of weights: 0.43724529177421084\n","Iteration #4138  Loss: 0.4964494717709741; l2 norm of gradients: 0.07900643221140591; l2 norm of weights: 0.4372213861777244\n","Iteration #4139  Loss: 0.49643855445280605; l2 norm of gradients: 0.07892060362187674; l2 norm of weights: 0.4371975177397262\n","Iteration #4140  Loss: 0.496427652106243; l2 norm of gradients: 0.07883486959173099; l2 norm of weights: 0.4371736863914521\n","Iteration #4141  Loss: 0.4964167647014128; l2 norm of gradients: 0.07874923001657486; l2 norm of weights: 0.43714989206426735\n","Iteration #4142  Loss: 0.49640589220850834; l2 norm of gradients: 0.07866368479213084; l2 norm of weights: 0.43712613468966705\n","Iteration #4143  Loss: 0.4963950345977871; l2 norm of gradients: 0.07857823381423748; l2 norm of weights: 0.43710241419927554\n","Iteration #4144  Loss: 0.4963841918395709; l2 norm of gradients: 0.07849287697884941; l2 norm of weights: 0.4370787305248463\n","Iteration #4145  Loss: 0.49637336390424597; l2 norm of gradients: 0.07840761418203704; l2 norm of weights: 0.43705508359826145\n","Iteration #4146  Loss: 0.4963625507622627; l2 norm of gradients: 0.07832244531998657; l2 norm of weights: 0.4370314733515321\n","Iteration #4147  Loss: 0.49635175238413565; l2 norm of gradients: 0.07823737028899977; l2 norm of weights: 0.4370078997167973\n","Iteration #4148  Loss: 0.4963409687404433; l2 norm of gradients: 0.07815238898549393; l2 norm of weights: 0.4369843626263248\n","Iteration #4149  Loss: 0.4963301998018277; l2 norm of gradients: 0.07806750130600157; l2 norm of weights: 0.4369608620125097\n","Iteration #4150  Loss: 0.4963194455389946; l2 norm of gradients: 0.07798270714717061; l2 norm of weights: 0.4369373978078752\n","Iteration #4151  Loss: 0.4963087059227136; l2 norm of gradients: 0.07789800640576389; l2 norm of weights: 0.43691396994507203\n","Iteration #4152  Loss: 0.4962979809238172; l2 norm of gradients: 0.07781339897865928; l2 norm of weights: 0.4368905783568779\n","Iteration #4153  Loss: 0.49628727051320115; l2 norm of gradients: 0.07772888476284949; l2 norm of weights: 0.43686722297619746\n","Iteration #4154  Loss: 0.4962765746618248; l2 norm of gradients: 0.07764446365544186; l2 norm of weights: 0.4368439037360626\n","Iteration #4155  Loss: 0.4962658933407097; l2 norm of gradients: 0.0775601355536584; l2 norm of weights: 0.4368206205696314\n","Iteration #4156  Loss: 0.49625522652094084; l2 norm of gradients: 0.07747590035483554; l2 norm of weights: 0.4367973734101884\n","Iteration #4157  Loss: 0.49624457417366535; l2 norm of gradients: 0.0773917579564239; l2 norm of weights: 0.4367741621911441\n","Iteration #4158  Loss: 0.49623393627009327; l2 norm of gradients: 0.07730770825598847; l2 norm of weights: 0.43675098684603525\n","Iteration #4159  Loss: 0.49622331278149706; l2 norm of gradients: 0.07722375115120818; l2 norm of weights: 0.4367278473085239\n","Iteration #4160  Loss: 0.4962127036792108; l2 norm of gradients: 0.07713988653987593; l2 norm of weights: 0.43670474351239735\n","Iteration #4161  Loss: 0.49620210893463146; l2 norm of gradients: 0.0770561143198984; l2 norm of weights: 0.43668167539156866\n","Iteration #4162  Loss: 0.49619152851921783; l2 norm of gradients: 0.07697243438929596; l2 norm of weights: 0.4366586428800756\n","Iteration #4163  Loss: 0.49618096240448994; l2 norm of gradients: 0.07688884664620259; l2 norm of weights: 0.43663564591208054\n","Iteration #4164  Loss: 0.49617041056203015; l2 norm of gradients: 0.07680535098886557; l2 norm of weights: 0.4366126844218705\n","Iteration #4165  Loss: 0.4961598729634825; l2 norm of gradients: 0.07672194731564555; l2 norm of weights: 0.43658975834385694\n","Iteration #4166  Loss: 0.4961493495805518; l2 norm of gradients: 0.0766386355250163; l2 norm of weights: 0.43656686761257524\n","Iteration #4167  Loss: 0.49613884038500455; l2 norm of gradients: 0.07655541551556476; l2 norm of weights: 0.4365440121626845\n","Iteration #4168  Loss: 0.49612834534866845; l2 norm of gradients: 0.07647228718599058; l2 norm of weights: 0.4365211919289676\n","Iteration #4169  Loss: 0.4961178644434324; l2 norm of gradients: 0.07638925043510636; l2 norm of weights: 0.436498406846331\n","Iteration #4170  Loss: 0.49610739764124545; l2 norm of gradients: 0.07630630516183731; l2 norm of weights: 0.43647565684980405\n","Iteration #4171  Loss: 0.49609694491411827; l2 norm of gradients: 0.07622345126522112; l2 norm of weights: 0.4364529418745391\n","Iteration #4172  Loss: 0.49608650623412176; l2 norm of gradients: 0.07614068864440797; l2 norm of weights: 0.43643026185581135\n","Iteration #4173  Loss: 0.49607608157338734; l2 norm of gradients: 0.0760580171986603; l2 norm of weights: 0.43640761672901857\n","Iteration #4174  Loss: 0.4960656709041067; l2 norm of gradients: 0.07597543682735267; l2 norm of weights: 0.4363850064296805\n","Iteration #4175  Loss: 0.496055274198532; l2 norm of gradients: 0.07589294742997169; l2 norm of weights: 0.4363624308934395\n","Iteration #4176  Loss: 0.4960448914289753; l2 norm of gradients: 0.0758105489061159; l2 norm of weights: 0.4363398900560592\n","Iteration #4177  Loss: 0.4960345225678086; l2 norm of gradients: 0.07572824115549559; l2 norm of weights: 0.4363173838534252\n","Iteration #4178  Loss: 0.4960241675874639; l2 norm of gradients: 0.07564602407793272; l2 norm of weights: 0.4362949122215444\n","Iteration #4179  Loss: 0.49601382646043274; l2 norm of gradients: 0.0755638975733608; l2 norm of weights: 0.43627247509654493\n","Iteration #4180  Loss: 0.49600349915926634; l2 norm of gradients: 0.07548186154182467; l2 norm of weights: 0.43625007241467595\n","Iteration #4181  Loss: 0.4959931856565754; l2 norm of gradients: 0.07539991588348054; l2 norm of weights: 0.43622770411230727\n","Iteration #4182  Loss: 0.4959828859250294; l2 norm of gradients: 0.07531806049859571; l2 norm of weights: 0.43620537012592914\n","Iteration #4183  Loss: 0.49597259993735804; l2 norm of gradients: 0.07523629528754855; l2 norm of weights: 0.4361830703921525\n","Iteration #4184  Loss: 0.49596232766634907; l2 norm of gradients: 0.0751546201508283; l2 norm of weights: 0.4361608048477079\n","Iteration #4185  Loss: 0.4959520690848497; l2 norm of gradients: 0.07507303498903503; l2 norm of weights: 0.43613857342944623\n","Iteration #4186  Loss: 0.49594182416576554; l2 norm of gradients: 0.07499153970287943; l2 norm of weights: 0.4361163760743376\n","Iteration #4187  Loss: 0.4959315928820612; l2 norm of gradients: 0.07491013419318275; l2 norm of weights: 0.4360942127194719\n","Iteration #4188  Loss: 0.4959213752067597; l2 norm of gradients: 0.0748288183608766; l2 norm of weights: 0.4360720833020582\n","Iteration #4189  Loss: 0.49591117111294253; l2 norm of gradients: 0.07474759210700295; l2 norm of weights: 0.43604998775942444\n","Iteration #4190  Loss: 0.4959009805737493; l2 norm of gradients: 0.07466645533271388; l2 norm of weights: 0.43602792602901747\n","Iteration #4191  Loss: 0.4958908035623779; l2 norm of gradients: 0.07458540793927151; l2 norm of weights: 0.43600589804840273\n","Iteration #4192  Loss: 0.4958806400520839; l2 norm of gradients: 0.0745044498280479; l2 norm of weights: 0.43598390375526386\n","Iteration #4193  Loss: 0.4958704900161812; l2 norm of gradients: 0.07442358090052487; l2 norm of weights: 0.4359619430874028\n","Iteration #4194  Loss: 0.4958603534280414; l2 norm of gradients: 0.07434280105829398; l2 norm of weights: 0.43594001598273946\n","Iteration #4195  Loss: 0.4958502302610934; l2 norm of gradients: 0.07426211020305624; l2 norm of weights: 0.4359181223793115\n","Iteration #4196  Loss: 0.4958401204888239; l2 norm of gradients: 0.07418150823662219; l2 norm of weights: 0.4358962622152738\n","Iteration #4197  Loss: 0.49583002408477694; l2 norm of gradients: 0.07410099506091157; l2 norm of weights: 0.4358744354288988\n","Iteration #4198  Loss: 0.49581994102255367; l2 norm of gradients: 0.07402057057795336; l2 norm of weights: 0.43585264195857604\n","Iteration #4199  Loss: 0.49580987127581244; l2 norm of gradients: 0.07394023468988559; l2 norm of weights: 0.43583088174281176\n","Iteration #4200  Loss: 0.4957998148182686; l2 norm of gradients: 0.07385998729895517; l2 norm of weights: 0.4358091547202289\n","Iteration #4201  Loss: 0.49578977162369464; l2 norm of gradients: 0.07377982830751793; l2 norm of weights: 0.43578746082956693\n","Iteration #4202  Loss: 0.4957797416659193; l2 norm of gradients: 0.07369975761803832; l2 norm of weights: 0.43576580000968157\n","Iteration #4203  Loss: 0.4957697249188282; l2 norm of gradients: 0.07361977513308934; l2 norm of weights: 0.43574417219954437\n","Iteration #4204  Loss: 0.49575972135636354; l2 norm of gradients: 0.07353988075535249; l2 norm of weights: 0.4357225773382428\n","Iteration #4205  Loss: 0.4957497309525237; l2 norm of gradients: 0.07346007438761756; l2 norm of weights: 0.43570101536498\n","Iteration #4206  Loss: 0.49573975368136364; l2 norm of gradients: 0.07338035593278258; l2 norm of weights: 0.4356794862190744\n","Iteration #4207  Loss: 0.49572978951699403; l2 norm of gradients: 0.07330072529385362; l2 norm of weights: 0.4356579898399597\n","Iteration #4208  Loss: 0.49571983843358186; l2 norm of gradients: 0.07322118237394475; l2 norm of weights: 0.4356365261671844\n","Iteration #4209  Loss: 0.4957099004053498; l2 norm of gradients: 0.07314172707627783; l2 norm of weights: 0.43561509514041186\n","Iteration #4210  Loss: 0.4956999754065764; l2 norm of gradients: 0.0730623593041825; l2 norm of weights: 0.43559369669942005\n","Iteration #4211  Loss: 0.4956900634115957; l2 norm of gradients: 0.07298307896109603; l2 norm of weights: 0.435572330784101\n","Iteration #4212  Loss: 0.4956801643947977; l2 norm of gradients: 0.07290388595056303; l2 norm of weights: 0.4355509973344615\n","Iteration #4213  Loss: 0.49567027833062705; l2 norm of gradients: 0.0728247801762356; l2 norm of weights: 0.4355296962906215\n","Iteration #4214  Loss: 0.4956604051935843; l2 norm of gradients: 0.07274576154187301; l2 norm of weights: 0.43550842759281505\n","Iteration #4215  Loss: 0.49565054495822497; l2 norm of gradients: 0.07266682995134172; l2 norm of weights: 0.43548719118138973\n","Iteration #4216  Loss: 0.49564069759915935; l2 norm of gradients: 0.07258798530861504; l2 norm of weights: 0.43546598699680633\n","Iteration #4217  Loss: 0.4956308630910529; l2 norm of gradients: 0.07250922751777339; l2 norm of weights: 0.4354448149796387\n","Iteration #4218  Loss: 0.495621041408626; l2 norm of gradients: 0.07243055648300374; l2 norm of weights: 0.43542367507057367\n","Iteration #4219  Loss: 0.49561123252665323; l2 norm of gradients: 0.0723519721085998; l2 norm of weights: 0.4354025672104107\n","Iteration #4220  Loss: 0.49560143641996424; l2 norm of gradients: 0.07227347429896179; l2 norm of weights: 0.43538149134006165\n","Iteration #4221  Loss: 0.49559165306344266; l2 norm of gradients: 0.07219506295859629; l2 norm of weights: 0.43536044740055085\n","Iteration #4222  Loss: 0.4955818824320267; l2 norm of gradients: 0.07211673799211621; l2 norm of weights: 0.43533943533301445\n","Iteration #4223  Loss: 0.49557212450070875; l2 norm of gradients: 0.0720384993042406; l2 norm of weights: 0.43531845507870065\n","Iteration #4224  Loss: 0.49556237924453517; l2 norm of gradients: 0.07196034679979456; l2 norm of weights: 0.43529750657896915\n","Iteration #4225  Loss: 0.49555264663860604; l2 norm of gradients: 0.07188228038370913; l2 norm of weights: 0.43527658977529116\n","Iteration #4226  Loss: 0.4955429266580757; l2 norm of gradients: 0.07180429996102111; l2 norm of weights: 0.43525570460924917\n","Iteration #4227  Loss: 0.4955332192781521; l2 norm of gradients: 0.07172640543687306; l2 norm of weights: 0.4352348510225366\n","Iteration #4228  Loss: 0.4955235244740964; l2 norm of gradients: 0.07164859671651302; l2 norm of weights: 0.435214028956958\n","Iteration #4229  Loss: 0.4955138422212239; l2 norm of gradients: 0.07157087370529457; l2 norm of weights: 0.4351932383544281\n","Iteration #4230  Loss: 0.4955041724949026; l2 norm of gradients: 0.07149323630867654; l2 norm of weights: 0.4351724791569723\n","Iteration #4231  Loss: 0.4954945152705542; l2 norm of gradients: 0.07141568443222304; l2 norm of weights: 0.43515175130672645\n","Iteration #4232  Loss: 0.49548487052365314; l2 norm of gradients: 0.07133821798160325; l2 norm of weights: 0.43513105474593594\n","Iteration #4233  Loss: 0.49547523822972733; l2 norm of gradients: 0.07126083686259135; l2 norm of weights: 0.4351103894169565\n","Iteration #4234  Loss: 0.49546561836435715; l2 norm of gradients: 0.07118354098106636; l2 norm of weights: 0.4350897552622529\n","Iteration #4235  Loss: 0.49545601090317587; l2 norm of gradients: 0.07110633024301205; l2 norm of weights: 0.43506915222439996\n","Iteration #4236  Loss: 0.49544641582186966; l2 norm of gradients: 0.07102920455451676; l2 norm of weights: 0.4350485802460813\n","Iteration #4237  Loss: 0.4954368330961767; l2 norm of gradients: 0.07095216382177351; l2 norm of weights: 0.43502803927008965\n","Iteration #4238  Loss: 0.49542726270188825; l2 norm of gradients: 0.07087520795107952; l2 norm of weights: 0.4350075292393266\n","Iteration #4239  Loss: 0.4954177046148473; l2 norm of gradients: 0.0707983368488364; l2 norm of weights: 0.43498705009680244\n","Iteration #4240  Loss: 0.49540815881094935; l2 norm of gradients: 0.07072155042154986; l2 norm of weights: 0.4349666017856357\n","Iteration #4241  Loss: 0.4953986252661421; l2 norm of gradients: 0.07064484857582969; l2 norm of weights: 0.4349461842490533\n","Iteration #4242  Loss: 0.49538910395642494; l2 norm of gradients: 0.07056823121838962; l2 norm of weights: 0.43492579743039\n","Iteration #4243  Loss: 0.495379594857849; l2 norm of gradients: 0.07049169825604715; l2 norm of weights: 0.43490544127308856\n","Iteration #4244  Loss: 0.49537009794651754; l2 norm of gradients: 0.07041524959572347; l2 norm of weights: 0.4348851157206992\n","Iteration #4245  Loss: 0.4953606131985853; l2 norm of gradients: 0.0703388851444434; l2 norm of weights: 0.43486482071687993\n","Iteration #4246  Loss: 0.49535114059025825; l2 norm of gradients: 0.0702626048093352; l2 norm of weights: 0.4348445562053954\n","Iteration #4247  Loss: 0.49534168009779433; l2 norm of gradients: 0.0701864084976304; l2 norm of weights: 0.4348243221301179\n","Iteration #4248  Loss: 0.49533223169750207; l2 norm of gradients: 0.07011029611666388; l2 norm of weights: 0.43480411843502603\n","Iteration #4249  Loss: 0.49532279536574164; l2 norm of gradients: 0.07003426757387356; l2 norm of weights: 0.43478394506420537\n","Iteration #4250  Loss: 0.49531337107892426; l2 norm of gradients: 0.06995832277680039; l2 norm of weights: 0.43476380196184794\n","Iteration #4251  Loss: 0.49530395881351186; l2 norm of gradients: 0.06988246163308819; l2 norm of weights: 0.4347436890722519\n","Iteration #4252  Loss: 0.49529455854601745; l2 norm of gradients: 0.06980668405048353; l2 norm of weights: 0.4347236063398214\n","Iteration #4253  Loss: 0.49528517025300456; l2 norm of gradients: 0.06973098993683566; l2 norm of weights: 0.43470355370906655\n","Iteration #4254  Loss: 0.49527579391108734; l2 norm of gradients: 0.06965537920009639; l2 norm of weights: 0.4346835311246032\n","Iteration #4255  Loss: 0.4952664294969306; l2 norm of gradients: 0.06957985174831988; l2 norm of weights: 0.43466353853115236\n","Iteration #4256  Loss: 0.4952570769872495; l2 norm of gradients: 0.06950440748966268; l2 norm of weights: 0.43464357587354074\n","Iteration #4257  Loss: 0.4952477363588094; l2 norm of gradients: 0.06942904633238349; l2 norm of weights: 0.4346236430966998\n","Iteration #4258  Loss: 0.4952384075884257; l2 norm of gradients: 0.06935376818484305; l2 norm of weights: 0.4346037401456661\n","Iteration #4259  Loss: 0.4952290906529642; l2 norm of gradients: 0.0692785729555042; l2 norm of weights: 0.4345838669655807\n","Iteration #4260  Loss: 0.4952197855293405; l2 norm of gradients: 0.06920346055293149; l2 norm of weights: 0.43456402350168927\n","Iteration #4261  Loss: 0.49521049219451974; l2 norm of gradients: 0.06912843088579128; l2 norm of weights: 0.4345442096993417\n","Iteration #4262  Loss: 0.49520121062551714; l2 norm of gradients: 0.06905348386285154; l2 norm of weights: 0.4345244255039922\n","Iteration #4263  Loss: 0.4951919407993976; l2 norm of gradients: 0.06897861939298171; l2 norm of weights: 0.43450467086119865\n","Iteration #4264  Loss: 0.4951826826932753; l2 norm of gradients: 0.06890383738515271; l2 norm of weights: 0.43448494571662283\n","Iteration #4265  Loss: 0.4951734362843137; l2 norm of gradients: 0.06882913774843667; l2 norm of weights: 0.4344652500160299\n","Iteration #4266  Loss: 0.49516420154972607; l2 norm of gradients: 0.0687545203920069; l2 norm of weights: 0.4344455837052887\n","Iteration #4267  Loss: 0.49515497846677414; l2 norm of gradients: 0.06867998522513782; l2 norm of weights: 0.43442594673037066\n","Iteration #4268  Loss: 0.49514576701276947; l2 norm of gradients: 0.0686055321572047; l2 norm of weights: 0.4344063390373509\n","Iteration #4269  Loss: 0.4951365671650721; l2 norm of gradients: 0.06853116109768372; l2 norm of weights: 0.43438676057240655\n","Iteration #4270  Loss: 0.4951273789010908; l2 norm of gradients: 0.06845687195615176; l2 norm of weights: 0.4343672112818179\n","Iteration #4271  Loss: 0.49511820219828373; l2 norm of gradients: 0.06838266464228626; l2 norm of weights: 0.4343476911119674\n","Iteration #4272  Loss: 0.49510903703415715; l2 norm of gradients: 0.0683085390658652; l2 norm of weights: 0.4343282000093398\n","Iteration #4273  Loss: 0.49509988338626604; l2 norm of gradients: 0.06823449513676692; l2 norm of weights: 0.43430873792052166\n","Iteration #4274  Loss: 0.4950907412322138; l2 norm of gradients: 0.06816053276497004; l2 norm of weights: 0.4342893047922016\n","Iteration #4275  Loss: 0.49508161054965205; l2 norm of gradients: 0.06808665186055331; l2 norm of weights: 0.4342699005711699\n","Iteration #4276  Loss: 0.49507249131628084; l2 norm of gradients: 0.06801285233369554; l2 norm of weights: 0.43425052520431806\n","Iteration #4277  Loss: 0.4950633835098481; l2 norm of gradients: 0.06793913409467547; l2 norm of weights: 0.4342311786386391\n","Iteration #4278  Loss: 0.49505428710815; l2 norm of gradients: 0.06786549705387163; l2 norm of weights: 0.4342118608212268\n","Iteration #4279  Loss: 0.49504520208903036; l2 norm of gradients: 0.06779194112176229; l2 norm of weights: 0.4341925716992762\n","Iteration #4280  Loss: 0.49503612843038125; l2 norm of gradients: 0.06771846620892533; l2 norm of weights: 0.43417331122008285\n","Iteration #4281  Loss: 0.49502706611014163; l2 norm of gradients: 0.06764507222603805; l2 norm of weights: 0.4341540793310428\n","Iteration #4282  Loss: 0.49501801510629895; l2 norm of gradients: 0.06757175908387719; l2 norm of weights: 0.4341348759796524\n","Iteration #4283  Loss: 0.49500897539688743; l2 norm of gradients: 0.06749852669331871; l2 norm of weights: 0.43411570111350833\n","Iteration #4284  Loss: 0.494999946959989; l2 norm of gradients: 0.06742537496533772; l2 norm of weights: 0.434096554680307\n","Iteration #4285  Loss: 0.49499092977373305; l2 norm of gradients: 0.06735230381100842; l2 norm of weights: 0.43407743662784476\n","Iteration #4286  Loss: 0.4949819238162957; l2 norm of gradients: 0.06727931314150383; l2 norm of weights: 0.43405834690401757\n","Iteration #4287  Loss: 0.49497292906590035; l2 norm of gradients: 0.0672064028680959; l2 norm of weights: 0.4340392854568205\n","Iteration #4288  Loss: 0.4949639455008175; l2 norm of gradients: 0.0671335729021552; l2 norm of weights: 0.4340202522343483\n","Iteration #4289  Loss: 0.4949549730993642; l2 norm of gradients: 0.067060823155151; l2 norm of weights: 0.43400124718479444\n","Iteration #4290  Loss: 0.4949460118399044; l2 norm of gradients: 0.06698815353865094; l2 norm of weights: 0.43398227025645125\n","Iteration #4291  Loss: 0.49493706170084906; l2 norm of gradients: 0.0669155639643211; l2 norm of weights: 0.4339633213977098\n","Iteration #4292  Loss: 0.4949281226606549; l2 norm of gradients: 0.06684305434392582; l2 norm of weights: 0.4339444005570597\n","Iteration #4293  Loss: 0.49491919469782586; l2 norm of gradients: 0.06677062458932757; l2 norm of weights: 0.43392550768308874\n","Iteration #4294  Loss: 0.4949102777909117; l2 norm of gradients: 0.0666982746124869; l2 norm of weights: 0.433906642724483\n","Iteration #4295  Loss: 0.49490137191850875; l2 norm of gradients: 0.06662600432546226; l2 norm of weights: 0.4338878056300262\n","Iteration #4296  Loss: 0.49489247705925926; l2 norm of gradients: 0.06655381364040996; l2 norm of weights: 0.4338689963486002\n","Iteration #4297  Loss: 0.49488359319185193; l2 norm of gradients: 0.06648170246958401; l2 norm of weights: 0.4338502148291842\n","Iteration #4298  Loss: 0.4948747202950209; l2 norm of gradients: 0.06640967072533603; l2 norm of weights: 0.4338314610208548\n","Iteration #4299  Loss: 0.49486585834754615; l2 norm of gradients: 0.0663377183201151; l2 norm of weights: 0.43381273487278577\n","Iteration #4300  Loss: 0.4948570073282539; l2 norm of gradients: 0.06626584516646777; l2 norm of weights: 0.43379403633424807\n","Iteration #4301  Loss: 0.4948481672160155; l2 norm of gradients: 0.06619405117703779; l2 norm of weights: 0.4337753653546094\n","Iteration #4302  Loss: 0.4948393379897482; l2 norm of gradients: 0.06612233626456612; l2 norm of weights: 0.43375672188333425\n","Iteration #4303  Loss: 0.4948305196284144; l2 norm of gradients: 0.06605070034189081; l2 norm of weights: 0.43373810586998335\n","Iteration #4304  Loss: 0.49482171211102194; l2 norm of gradients: 0.06597914332194676; l2 norm of weights: 0.43371951726421404\n","Iteration #4305  Loss: 0.4948129154166242; l2 norm of gradients: 0.06590766511776586; l2 norm of weights: 0.4337009560157796\n","Iteration #4306  Loss: 0.49480412952431907; l2 norm of gradients: 0.06583626564247662; l2 norm of weights: 0.43368242207452934\n","Iteration #4307  Loss: 0.49479535441325; l2 norm of gradients: 0.06576494480930421; l2 norm of weights: 0.4336639153904082\n","Iteration #4308  Loss: 0.4947865900626052; l2 norm of gradients: 0.06569370253157035; l2 norm of weights: 0.433645435913457\n","Iteration #4309  Loss: 0.4947778364516178; l2 norm of gradients: 0.06562253872269314; l2 norm of weights: 0.43362698359381185\n","Iteration #4310  Loss: 0.49476909355956555; l2 norm of gradients: 0.06555145329618699; l2 norm of weights: 0.43360855838170387\n","Iteration #4311  Loss: 0.49476036136577084; l2 norm of gradients: 0.06548044616566255; l2 norm of weights: 0.4335901602274596\n","Iteration #4312  Loss: 0.4947516398496008; l2 norm of gradients: 0.06540951724482645; l2 norm of weights: 0.4335717890815002\n","Iteration #4313  Loss: 0.4947429289904668; l2 norm of gradients: 0.06533866644748146; l2 norm of weights: 0.4335534448943417\n","Iteration #4314  Loss: 0.49473422876782447; l2 norm of gradients: 0.06526789368752602; l2 norm of weights: 0.43353512761659463\n","Iteration #4315  Loss: 0.49472553916117423; l2 norm of gradients: 0.06519719887895455; l2 norm of weights: 0.43351683719896383\n","Iteration #4316  Loss: 0.4947168601500601; l2 norm of gradients: 0.06512658193585699; l2 norm of weights: 0.43349857359224836\n","Iteration #4317  Loss: 0.4947081917140704; l2 norm of gradients: 0.06505604277241887; l2 norm of weights: 0.43348033674734127\n","Iteration #4318  Loss: 0.49469953383283727; l2 norm of gradients: 0.06498558130292119; l2 norm of weights: 0.4334621266152294\n","Iteration #4319  Loss: 0.49469088648603665; l2 norm of gradients: 0.06491519744174025; l2 norm of weights: 0.43344394314699336\n","Iteration #4320  Loss: 0.4946822496533889; l2 norm of gradients: 0.06484489110334757; l2 norm of weights: 0.4334257862938072\n","Iteration #4321  Loss: 0.49467362331465725; l2 norm of gradients: 0.06477466220230985; l2 norm of weights: 0.4334076560069381\n","Iteration #4322  Loss: 0.4946650074496488; l2 norm of gradients: 0.0647045106532888; l2 norm of weights: 0.43338955223774656\n","Iteration #4323  Loss: 0.49465640203821415; l2 norm of gradients: 0.06463443637104098; l2 norm of weights: 0.4333714749376861\n","Iteration #4324  Loss: 0.4946478070602474; l2 norm of gradients: 0.06456443927041781; l2 norm of weights: 0.43335342405830274\n","Iteration #4325  Loss: 0.49463922249568587; l2 norm of gradients: 0.06449451926636544; l2 norm of weights: 0.43333539955123546\n","Iteration #4326  Loss: 0.49463064832450987; l2 norm of gradients: 0.06442467627392454; l2 norm of weights: 0.43331740136821545\n","Iteration #4327  Loss: 0.49462208452674317; l2 norm of gradients: 0.0643549102082303; l2 norm of weights: 0.4332994294610661\n","Iteration #4328  Loss: 0.4946135310824521; l2 norm of gradients: 0.06428522098451228; l2 norm of weights: 0.43328148378170317\n","Iteration #4329  Loss: 0.49460498797174635; l2 norm of gradients: 0.06421560851809437; l2 norm of weights: 0.4332635642821341\n","Iteration #4330  Loss: 0.4945964551747782; l2 norm of gradients: 0.06414607272439454; l2 norm of weights: 0.43324567091445826\n","Iteration #4331  Loss: 0.49458793267174267; l2 norm of gradients: 0.06407661351892495; l2 norm of weights: 0.4332278036308663\n","Iteration #4332  Loss: 0.4945794204428774; l2 norm of gradients: 0.06400723081729158; l2 norm of weights: 0.4332099623836407\n","Iteration #4333  Loss: 0.49457091846846274; l2 norm of gradients: 0.06393792453519437; l2 norm of weights: 0.4331921471251548\n","Iteration #4334  Loss: 0.4945624267288211; l2 norm of gradients: 0.06386869458842692; l2 norm of weights: 0.4331743578078732\n","Iteration #4335  Loss: 0.49455394520431767; l2 norm of gradients: 0.06379954089287662; l2 norm of weights: 0.4331565943843514\n","Iteration #4336  Loss: 0.49454547387535963; l2 norm of gradients: 0.06373046336452425; l2 norm of weights: 0.4331388568072355\n","Iteration #4337  Loss: 0.4945370127223964; l2 norm of gradients: 0.0636614619194441; l2 norm of weights: 0.43312114502926224\n","Iteration #4338  Loss: 0.49452856172591947; l2 norm of gradients: 0.06359253647380378; l2 norm of weights: 0.43310345900325875\n","Iteration #4339  Loss: 0.4945201208664624; l2 norm of gradients: 0.06352368694386412; l2 norm of weights: 0.4330857986821424\n","Iteration #4340  Loss: 0.49451169012460044; l2 norm of gradients: 0.06345491324597909; l2 norm of weights: 0.4330681640189204\n","Iteration #4341  Loss: 0.49450326948095075; l2 norm of gradients: 0.06338621529659565; l2 norm of weights: 0.43305055496669026\n","Iteration #4342  Loss: 0.4944948589161723; l2 norm of gradients: 0.06331759301225369; l2 norm of weights: 0.43303297147863873\n","Iteration #4343  Loss: 0.4944864584109653; l2 norm of gradients: 0.06324904630958594; l2 norm of weights: 0.43301541350804235\n","Iteration #4344  Loss: 0.4944780679460718; l2 norm of gradients: 0.0631805751053178; l2 norm of weights: 0.4329978810082671\n","Iteration #4345  Loss: 0.49446968750227527; l2 norm of gradients: 0.06311217931626727; l2 norm of weights: 0.4329803739327679\n","Iteration #4346  Loss: 0.49446131706040075; l2 norm of gradients: 0.06304385885934485; l2 norm of weights: 0.4329628922350891\n","Iteration #4347  Loss: 0.49445295660131383; l2 norm of gradients: 0.06297561365155346; l2 norm of weights: 0.4329454358688635\n","Iteration #4348  Loss: 0.4944446061059217; l2 norm of gradients: 0.0629074436099883; l2 norm of weights: 0.43292800478781285\n","Iteration #4349  Loss: 0.4944362655551728; l2 norm of gradients: 0.06283934865183675; l2 norm of weights: 0.43291059894574746\n","Iteration #4350  Loss: 0.4944279349300561; l2 norm of gradients: 0.06277132869437832; l2 norm of weights: 0.4328932182965659\n","Iteration #4351  Loss: 0.49441961421160174; l2 norm of gradients: 0.06270338365498442; l2 norm of weights: 0.4328758627942549\n","Iteration #4352  Loss: 0.49441130338088074; l2 norm of gradients: 0.06263551345111842; l2 norm of weights: 0.43285853239288946\n","Iteration #4353  Loss: 0.4944030024190045; l2 norm of gradients: 0.06256771800033539; l2 norm of weights: 0.4328412270466323\n","Iteration #4354  Loss: 0.4943947113071252; l2 norm of gradients: 0.0624999972202822; l2 norm of weights: 0.4328239467097338\n","Iteration #4355  Loss: 0.4943864300264357; l2 norm of gradients: 0.06243235102869711; l2 norm of weights: 0.432806691336532\n","Iteration #4356  Loss: 0.4943781585581692; l2 norm of gradients: 0.062364779343410046; l2 norm of weights: 0.43278946088145237\n","Iteration #4357  Loss: 0.49436989688359895; l2 norm of gradients: 0.06229728208234215; l2 norm of weights: 0.43277225529900726\n","Iteration #4358  Loss: 0.494361644984039; l2 norm of gradients: 0.06222985916350591; l2 norm of weights: 0.43275507454379664\n","Iteration #4359  Loss: 0.49435340284084317; l2 norm of gradients: 0.06216251050500493; l2 norm of weights: 0.43273791857050686\n","Iteration #4360  Loss: 0.4943451704354055; l2 norm of gradients: 0.062095236025033924; l2 norm of weights: 0.4327207873339112\n","Iteration #4361  Loss: 0.49433694774916015; l2 norm of gradients: 0.06202803564187851; l2 norm of weights: 0.4327036807888696\n","Iteration #4362  Loss: 0.49432873476358075; l2 norm of gradients: 0.061960909273915225; l2 norm of weights: 0.43268659889032823\n","Iteration #4363  Loss: 0.4943205314601814; l2 norm of gradients: 0.061893856839611315; l2 norm of weights: 0.4326695415933196\n","Iteration #4364  Loss: 0.4943123378205153; l2 norm of gradients: 0.06182687825752468; l2 norm of weights: 0.4326525088529623\n","Iteration #4365  Loss: 0.49430415382617576; l2 norm of gradients: 0.0617599734463038; l2 norm of weights: 0.4326355006244608\n","Iteration #4366  Loss: 0.4942959794587955; l2 norm of gradients: 0.061693142324687596; l2 norm of weights: 0.43261851686310526\n","Iteration #4367  Loss: 0.4942878147000465; l2 norm of gradients: 0.061626384811505334; l2 norm of weights: 0.4326015575242717\n","Iteration #4368  Loss: 0.4942796595316406; l2 norm of gradients: 0.061559700825676515; l2 norm of weights: 0.43258462256342106\n","Iteration #4369  Loss: 0.4942715139353283; l2 norm of gradients: 0.06149309028621084; l2 norm of weights: 0.43256771193610016\n","Iteration #4370  Loss: 0.4942633778929001; l2 norm of gradients: 0.061426553112207995; l2 norm of weights: 0.43255082559794056\n","Iteration #4371  Loss: 0.4942552513861848; l2 norm of gradients: 0.06136008922285764; l2 norm of weights: 0.4325339635046589\n","Iteration #4372  Loss: 0.4942471343970508; l2 norm of gradients: 0.061293698537439295; l2 norm of weights: 0.43251712561205635\n","Iteration #4373  Loss: 0.49423902690740545; l2 norm of gradients: 0.06122738097532218; l2 norm of weights: 0.4325003118760192\n","Iteration #4374  Loss: 0.4942309288991945; l2 norm of gradients: 0.06116113645596525; l2 norm of weights: 0.4324835222525178\n","Iteration #4375  Loss: 0.4942228403544029; l2 norm of gradients: 0.06109496489891688; l2 norm of weights: 0.432466756697607\n","Iteration #4376  Loss: 0.49421476125505426; l2 norm of gradients: 0.061028866223815; l2 norm of weights: 0.43245001516742565\n","Iteration #4377  Loss: 0.49420669158321073; l2 norm of gradients: 0.060962840350386835; l2 norm of weights: 0.4324332976181969\n","Iteration #4378  Loss: 0.49419863132097297; l2 norm of gradients: 0.060896887198448844; l2 norm of weights: 0.43241660400622745\n","Iteration #4379  Loss: 0.49419058045048014; l2 norm of gradients: 0.060831006687906655; l2 norm of weights: 0.43239993428790774\n","Iteration #4380  Loss: 0.4941825389539096; l2 norm of gradients: 0.060765198738754946; l2 norm of weights: 0.43238328841971174\n","Iteration #4381  Loss: 0.49417450681347735; l2 norm of gradients: 0.06069946327107731; l2 norm of weights: 0.4323666663581968\n","Iteration #4382  Loss: 0.49416648401143726; l2 norm of gradients: 0.06063380020504621; l2 norm of weights: 0.4323500680600037\n","Iteration #4383  Loss: 0.49415847053008133; l2 norm of gradients: 0.060568209460922846; l2 norm of weights: 0.4323334934818559\n","Iteration #4384  Loss: 0.49415046635173954; l2 norm of gradients: 0.060502690959057075; l2 norm of weights: 0.43231694258055986\n","Iteration #4385  Loss: 0.4941424714587802; l2 norm of gradients: 0.0604372446198873; l2 norm of weights: 0.4323004153130049\n","Iteration #4386  Loss: 0.4941344858336092; l2 norm of gradients: 0.06037187036394035; l2 norm of weights: 0.4322839116361628\n","Iteration #4387  Loss: 0.4941265094586699; l2 norm of gradients: 0.06030656811183144; l2 norm of weights: 0.4322674315070879\n","Iteration #4388  Loss: 0.494118542316444; l2 norm of gradients: 0.06024133778426403; l2 norm of weights: 0.4322509748829167\n","Iteration #4389  Loss: 0.4941105843894503; l2 norm of gradients: 0.06017617930202968; l2 norm of weights: 0.4322345417208678\n","Iteration #4390  Loss: 0.4941026356602453; l2 norm of gradients: 0.06011109258600812; l2 norm of weights: 0.4322181319782418\n","Iteration #4391  Loss: 0.49409469611142287; l2 norm of gradients: 0.0600460775571669; l2 norm of weights: 0.4322017456124212\n","Iteration #4392  Loss: 0.4940867657256146; l2 norm of gradients: 0.05998113413656151; l2 norm of weights: 0.4321853825808701\n","Iteration #4393  Loss: 0.4940788444854888; l2 norm of gradients: 0.05991626224533517; l2 norm of weights: 0.4321690428411338\n","Iteration #4394  Loss: 0.4940709323737513; l2 norm of gradients: 0.059851461804718765; l2 norm of weights: 0.4321527263508395\n","Iteration #4395  Loss: 0.494063029373145; l2 norm of gradients: 0.05978673273603077; l2 norm of weights: 0.43213643306769506\n","Iteration #4396  Loss: 0.4940551354664498; l2 norm of gradients: 0.0597220749606771; l2 norm of weights: 0.4321201629494897\n","Iteration #4397  Loss: 0.4940472506364828; l2 norm of gradients: 0.05965748840015103; l2 norm of weights: 0.4321039159540935\n","Iteration #4398  Loss: 0.4940393748660975; l2 norm of gradients: 0.0595929729760331; l2 norm of weights: 0.43208769203945696\n","Iteration #4399  Loss: 0.49403150813818447; l2 norm of gradients: 0.05952852860999107; l2 norm of weights: 0.4320714911636117\n","Iteration #4400  Loss: 0.49402365043567126; l2 norm of gradients: 0.059464155223779715; l2 norm of weights: 0.4320553132846694\n","Iteration #4401  Loss: 0.4940158017415215; l2 norm of gradients: 0.05939985273924082; l2 norm of weights: 0.43203915836082185\n","Iteration #4402  Loss: 0.49400796203873554; l2 norm of gradients: 0.05933562107830302; l2 norm of weights: 0.4320230263503414\n","Iteration #4403  Loss: 0.4940001313103506; l2 norm of gradients: 0.0592714601629818; l2 norm of weights: 0.43200691721158013\n","Iteration #4404  Loss: 0.49399230953943973; l2 norm of gradients: 0.05920736991537925; l2 norm of weights: 0.43199083090296986\n","Iteration #4405  Loss: 0.49398449670911265; l2 norm of gradients: 0.05914335025768408; l2 norm of weights: 0.43197476738302226\n","Iteration #4406  Loss: 0.49397669280251527; l2 norm of gradients: 0.05907940111217149; l2 norm of weights: 0.43195872661032847\n","Iteration #4407  Loss: 0.4939688978028293; l2 norm of gradients: 0.05901552240120309; l2 norm of weights: 0.4319427085435589\n","Iteration #4408  Loss: 0.4939611116932732; l2 norm of gradients: 0.05895171404722678; l2 norm of weights: 0.4319267131414633\n","Iteration #4409  Loss: 0.4939533344571007; l2 norm of gradients: 0.05888797597277668; l2 norm of weights: 0.4319107403628704\n","Iteration #4410  Loss: 0.49394556607760204; l2 norm of gradients: 0.05882430810047294; l2 norm of weights: 0.43189479016668786\n","Iteration #4411  Loss: 0.4939378065381028; l2 norm of gradients: 0.05876071035302182; l2 norm of weights: 0.4318788625119022\n","Iteration #4412  Loss: 0.49393005582196464; l2 norm of gradients: 0.05869718265321544; l2 norm of weights: 0.4318629573575784\n","Iteration #4413  Loss: 0.4939223139125848; l2 norm of gradients: 0.058633724923931714; l2 norm of weights: 0.43184707466286\n","Iteration #4414  Loss: 0.493914580793396; l2 norm of gradients: 0.058570337088134326; l2 norm of weights: 0.4318312143869688\n","Iteration #4415  Loss: 0.4939068564478667; l2 norm of gradients: 0.05850701906887256; l2 norm of weights: 0.43181537648920504\n","Iteration #4416  Loss: 0.4938991408595007; l2 norm of gradients: 0.05844377078928126; l2 norm of weights: 0.43179956092894645\n","Iteration #4417  Loss: 0.49389143401183716; l2 norm of gradients: 0.05838059217258064; l2 norm of weights: 0.43178376766564924\n","Iteration #4418  Loss: 0.49388373588845064; l2 norm of gradients: 0.05831748314207627; l2 norm of weights: 0.43176799665884696\n","Iteration #4419  Loss: 0.4938760464729504; l2 norm of gradients: 0.05825444362115905; l2 norm of weights: 0.43175224786815075\n","Iteration #4420  Loss: 0.49386836574898174; l2 norm of gradients: 0.058191473533304915; l2 norm of weights: 0.43173652125324946\n","Iteration #4421  Loss: 0.49386069370022423; l2 norm of gradients: 0.058128572802074924; l2 norm of weights: 0.43172081677390894\n","Iteration #4422  Loss: 0.4938530303103927; l2 norm of gradients: 0.058065741351115016; l2 norm of weights: 0.4317051343899723\n","Iteration #4423  Loss: 0.493845375563237; l2 norm of gradients: 0.0580029791041561; l2 norm of weights: 0.43168947406135966\n","Iteration #4424  Loss: 0.49383772944254156; l2 norm of gradients: 0.05794028598501375; l2 norm of weights: 0.43167383574806795\n","Iteration #4425  Loss: 0.4938300919321258; l2 norm of gradients: 0.05787766191758828; l2 norm of weights: 0.4316582194101708\n","Iteration #4426  Loss: 0.49382246301584365; l2 norm of gradients: 0.05781510682586455; l2 norm of weights: 0.43164262500781847\n","Iteration #4427  Loss: 0.4938148426775837; l2 norm of gradients: 0.057752620633911886; l2 norm of weights: 0.43162705250123756\n","Iteration #4428  Loss: 0.4938072309012689; l2 norm of gradients: 0.05769020326588407; l2 norm of weights: 0.4316115018507308\n","Iteration #4429  Loss: 0.4937996276708571; l2 norm of gradients: 0.0576278546460191; l2 norm of weights: 0.4315959730166775\n","Iteration #4430  Loss: 0.4937920329703399; l2 norm of gradients: 0.05756557469863923; l2 norm of weights: 0.4315804659595324\n","Iteration #4431  Loss: 0.49378444678374367; l2 norm of gradients: 0.057503363348150816; l2 norm of weights: 0.4315649806398265\n","Iteration #4432  Loss: 0.4937768690951288; l2 norm of gradients: 0.05744122051904416; l2 norm of weights: 0.4315495170181662\n","Iteration #4433  Loss: 0.49376929988858986; l2 norm of gradients: 0.05737914613589356; l2 norm of weights: 0.4315340750552335\n","Iteration #4434  Loss: 0.4937617391482554; l2 norm of gradients: 0.05731714012335711; l2 norm of weights: 0.4315186547117859\n","Iteration #4435  Loss: 0.49375418685828815; l2 norm of gradients: 0.05725520240617663; l2 norm of weights: 0.43150325594865635\n","Iteration #4436  Loss: 0.4937466430028846; l2 norm of gradients: 0.05719333290917756; l2 norm of weights: 0.43148787872675226\n","Iteration #4437  Loss: 0.49373910756627526; l2 norm of gradients: 0.05713153155726895; l2 norm of weights: 0.43147252300705685\n","Iteration #4438  Loss: 0.49373158053272415; l2 norm of gradients: 0.05706979827544318; l2 norm of weights: 0.43145718875062744\n","Iteration #4439  Loss: 0.49372406188652923; l2 norm of gradients: 0.05700813298877613; l2 norm of weights: 0.4314418759185967\n","Iteration #4440  Loss: 0.49371655161202194; l2 norm of gradients: 0.05694653562242684; l2 norm of weights: 0.43142658447217114\n","Iteration #4441  Loss: 0.4937090496935672; l2 norm of gradients: 0.05688500610163754; l2 norm of weights: 0.43141131437263225\n","Iteration #4442  Loss: 0.4937015561155638; l2 norm of gradients: 0.056823544351733576; l2 norm of weights: 0.4313960655813356\n","Iteration #4443  Loss: 0.4936940708624435; l2 norm of gradients: 0.056762150298123246; l2 norm of weights: 0.43138083805971084\n","Iteration #4444  Loss: 0.4936865939186714; l2 norm of gradients: 0.05670082386629773; l2 norm of weights: 0.43136563176926157\n","Iteration #4445  Loss: 0.49367912526874613; l2 norm of gradients: 0.05663956498183104; l2 norm of weights: 0.43135044667156536\n","Iteration #4446  Loss: 0.49367166489719944; l2 norm of gradients: 0.05657837357037986; l2 norm of weights: 0.4313352827282733\n","Iteration #4447  Loss: 0.49366421278859596; l2 norm of gradients: 0.05651724955768354; l2 norm of weights: 0.4313201399011103\n","Iteration #4448  Loss: 0.4936567689275336; l2 norm of gradients: 0.056456192869563854; l2 norm of weights: 0.4313050181518744\n","Iteration #4449  Loss: 0.4936493332986432; l2 norm of gradients: 0.05639520343192514; l2 norm of weights: 0.4312899174424371\n","Iteration #4450  Loss: 0.4936419058865883; l2 norm of gradients: 0.05633428117075395; l2 norm of weights: 0.43127483773474307\n","Iteration #4451  Loss: 0.4936344866760656; l2 norm of gradients: 0.056273426012119133; l2 norm of weights: 0.43125977899080986\n","Iteration #4452  Loss: 0.493627075651804; l2 norm of gradients: 0.05621263788217173; l2 norm of weights: 0.4312447411727279\n","Iteration #4453  Loss: 0.4936196727985659; l2 norm of gradients: 0.056151916707144756; l2 norm of weights: 0.43122972424266054\n","Iteration #4454  Loss: 0.4936122781011455; l2 norm of gradients: 0.05609126241335326; l2 norm of weights: 0.4312147281628434\n","Iteration #4455  Loss: 0.49360489154436993; l2 norm of gradients: 0.05603067492719414; l2 norm of weights: 0.4311997528955847\n","Iteration #4456  Loss: 0.49359751311309874; l2 norm of gradients: 0.05597015417514612; l2 norm of weights: 0.43118479840326507\n","Iteration #4457  Loss: 0.49359014279222396; l2 norm of gradients: 0.055909700083769574; l2 norm of weights: 0.43116986464833723\n","Iteration #4458  Loss: 0.4935827805666696; l2 norm of gradients: 0.05584931257970648; l2 norm of weights: 0.4311549515933259\n","Iteration #4459  Loss: 0.4935754264213923; l2 norm of gradients: 0.05578899158968037; l2 norm of weights: 0.43114005920082765\n","Iteration #4460  Loss: 0.49356808034138067; l2 norm of gradients: 0.05572873704049615; l2 norm of weights: 0.43112518743351097\n","Iteration #4461  Loss: 0.4935607423116555; l2 norm of gradients: 0.05566854885904012; l2 norm of weights: 0.43111033625411593\n","Iteration #4462  Loss: 0.49355341231726957; l2 norm of gradients: 0.055608426972279724; l2 norm of weights: 0.43109550562545396\n","Iteration #4463  Loss: 0.4935460903433077; l2 norm of gradients: 0.05554837130726363; l2 norm of weights: 0.431080695510408\n","Iteration #4464  Loss: 0.49353877637488647; l2 norm of gradients: 0.05548838179112155; l2 norm of weights: 0.43106590587193194\n","Iteration #4465  Loss: 0.4935314703971546; l2 norm of gradients: 0.055428458351064136; l2 norm of weights: 0.4310511366730513\n","Iteration #4466  Loss: 0.4935241723952921; l2 norm of gradients: 0.055368600914382934; l2 norm of weights: 0.4310363878768619\n","Iteration #4467  Loss: 0.4935168823545111; l2 norm of gradients: 0.05530880940845032; l2 norm of weights: 0.43102165944653065\n","Iteration #4468  Loss: 0.4935096002600551; l2 norm of gradients: 0.055249083760719284; l2 norm of weights: 0.43100695134529543\n","Iteration #4469  Loss: 0.4935023260971989; l2 norm of gradients: 0.055189423898723465; l2 norm of weights: 0.43099226353646397\n","Iteration #4470  Loss: 0.4934950598512496; l2 norm of gradients: 0.05512982975007701; l2 norm of weights: 0.4309775959834151\n","Iteration #4471  Loss: 0.4934878015075449; l2 norm of gradients: 0.055070301242474515; l2 norm of weights: 0.4309629486495976\n","Iteration #4472  Loss: 0.4934805510514541; l2 norm of gradients: 0.05501083830369088; l2 norm of weights: 0.4309483214985303\n","Iteration #4473  Loss: 0.49347330846837817; l2 norm of gradients: 0.05495144086158126; l2 norm of weights: 0.43093371449380224\n","Iteration #4474  Loss: 0.4934660737437485; l2 norm of gradients: 0.054892108844080974; l2 norm of weights: 0.43091912759907214\n","Iteration #4475  Loss: 0.49345884686302827; l2 norm of gradients: 0.054832842179205414; l2 norm of weights: 0.43090456077806855\n","Iteration #4476  Loss: 0.49345162781171137; l2 norm of gradients: 0.05477364079504994; l2 norm of weights: 0.4308900139945897\n","Iteration #4477  Loss: 0.4934444165753229; l2 norm of gradients: 0.054714504619789765; l2 norm of weights: 0.4308754872125031\n","Iteration #4478  Loss: 0.49343721313941885; l2 norm of gradients: 0.054655433581679964; l2 norm of weights: 0.43086098039574583\n","Iteration #4479  Loss: 0.49343001748958604; l2 norm of gradients: 0.0545964276090553; l2 norm of weights: 0.43084649350832405\n","Iteration #4480  Loss: 0.4934228296114419; l2 norm of gradients: 0.05453748663033012; l2 norm of weights: 0.4308320265143128\n","Iteration #4481  Loss: 0.4934156494906349; l2 norm of gradients: 0.054478610573998346; l2 norm of weights: 0.4308175793778565\n","Iteration #4482  Loss: 0.4934084771128441; l2 norm of gradients: 0.05441979936863334; l2 norm of weights: 0.430803152063168\n","Iteration #4483  Loss: 0.49340131246377894; l2 norm of gradients: 0.05436105294288776; l2 norm of weights: 0.43078874453452887\n","Iteration #4484  Loss: 0.4933941555291797; l2 norm of gradients: 0.05430237122549361; l2 norm of weights: 0.43077435675628956\n","Iteration #4485  Loss: 0.4933870062948168; l2 norm of gradients: 0.05424375414526203; l2 norm of weights: 0.43075998869286847\n","Iteration #4486  Loss: 0.4933798647464914; l2 norm of gradients: 0.05418520163108325; l2 norm of weights: 0.43074564030875256\n","Iteration #4487  Loss: 0.49337273087003464; l2 norm of gradients: 0.054126713611926475; l2 norm of weights: 0.430731311568497\n","Iteration #4488  Loss: 0.493365604651308; l2 norm of gradients: 0.05406829001683989; l2 norm of weights: 0.43071700243672456\n","Iteration #4489  Loss: 0.4933584860762034; l2 norm of gradients: 0.05400993077495043; l2 norm of weights: 0.43070271287812656\n","Iteration #4490  Loss: 0.49335137513064276; l2 norm of gradients: 0.0539516358154638; l2 norm of weights: 0.4306884428574616\n","Iteration #4491  Loss: 0.4933442718005778; l2 norm of gradients: 0.053893405067664345; l2 norm of weights: 0.43067419233955595\n","Iteration #4492  Loss: 0.4933371760719906; l2 norm of gradients: 0.05383523846091497; l2 norm of weights: 0.43065996128930367\n","Iteration #4493  Loss: 0.4933300879308931; l2 norm of gradients: 0.05377713592465708; l2 norm of weights: 0.43064574967166586\n","Iteration #4494  Loss: 0.49332300736332674; l2 norm of gradients: 0.05371909738841039; l2 norm of weights: 0.4306315574516711\n","Iteration #4495  Loss: 0.49331593435536336; l2 norm of gradients: 0.053661122781772985; l2 norm of weights: 0.43061738459441495\n","Iteration #4496  Loss: 0.493308868893104; l2 norm of gradients: 0.05360321203442113; l2 norm of weights: 0.43060323106506004\n","Iteration #4497  Loss: 0.4933018109626798; l2 norm of gradients: 0.053545365076109186; l2 norm of weights: 0.43058909682883584\n","Iteration #4498  Loss: 0.4932947605502511; l2 norm of gradients: 0.05348758183666958; l2 norm of weights: 0.4305749818510385\n","Iteration #4499  Loss: 0.4932877176420079; l2 norm of gradients: 0.053429862246012705; l2 norm of weights: 0.4305608860970309\n","Iteration #4500  Loss: 0.49328068222416993; l2 norm of gradients: 0.053372206234126765; l2 norm of weights: 0.4305468095322422\n","Iteration #4501  Loss: 0.49327365428298614; l2 norm of gradients: 0.05331461373107774; l2 norm of weights: 0.4305327521221682\n","Iteration #4502  Loss: 0.49326663380473473; l2 norm of gradients: 0.05325708466700936; l2 norm of weights: 0.4305187138323706\n","Iteration #4503  Loss: 0.4932596207757232; l2 norm of gradients: 0.05319961897214286; l2 norm of weights: 0.4305046946284774\n","Iteration #4504  Loss: 0.49325261518228847; l2 norm of gradients: 0.05314221657677706; l2 norm of weights: 0.43049069447618254\n","Iteration #4505  Loss: 0.4932456170107965; l2 norm of gradients: 0.05308487741128817; l2 norm of weights: 0.4304767133412459\n","Iteration #4506  Loss: 0.49323862624764214; l2 norm of gradients: 0.05302760140612976; l2 norm of weights: 0.4304627511894927\n","Iteration #4507  Loss: 0.4932316428792495; l2 norm of gradients: 0.05297038849183261; l2 norm of weights: 0.4304488079868142\n","Iteration #4508  Loss: 0.49322466689207184; l2 norm of gradients: 0.05291323859900473; l2 norm of weights: 0.4304348836991669\n","Iteration #4509  Loss: 0.49321769827259065; l2 norm of gradients: 0.05285615165833116; l2 norm of weights: 0.43042097829257264\n","Iteration #4510  Loss: 0.49321073700731705; l2 norm of gradients: 0.05279912760057395; l2 norm of weights: 0.4304070917331186\n","Iteration #4511  Loss: 0.4932037830827904; l2 norm of gradients: 0.05274216635657204; l2 norm of weights: 0.43039322398695695\n","Iteration #4512  Loss: 0.49319683648557877; l2 norm of gradients: 0.05268526785724126; l2 norm of weights: 0.4303793750203048\n","Iteration #4513  Loss: 0.4931898972022793; l2 norm of gradients: 0.05262843203357406; l2 norm of weights: 0.4303655447994443\n","Iteration #4514  Loss: 0.4931829652195171; l2 norm of gradients: 0.05257165881663967; l2 norm of weights: 0.43035173329072185\n","Iteration #4515  Loss: 0.4931760405239465; l2 norm of gradients: 0.05251494813758376; l2 norm of weights: 0.4303379404605491\n","Iteration #4516  Loss: 0.4931691231022496; l2 norm of gradients: 0.05245829992762859; l2 norm of weights: 0.4303241662754016\n","Iteration #4517  Loss: 0.4931622129411375; l2 norm of gradients: 0.05240171411807272; l2 norm of weights: 0.4303104107018196\n","Iteration #4518  Loss: 0.49315531002734914; l2 norm of gradients: 0.052345190640291105; l2 norm of weights: 0.4302966737064074\n","Iteration #4519  Loss: 0.4931484143476519; l2 norm of gradients: 0.05228872942573483; l2 norm of weights: 0.4302829552558334\n","Iteration #4520  Loss: 0.49314152588884147; l2 norm of gradients: 0.052232330405931206; l2 norm of weights: 0.43026925531683\n","Iteration #4521  Loss: 0.4931346446377417; l2 norm of gradients: 0.052175993512483525; l2 norm of weights: 0.4302555738561936\n","Iteration #4522  Loss: 0.4931277705812044; l2 norm of gradients: 0.05211971867707111; l2 norm of weights: 0.43024191084078417\n","Iteration #4523  Loss: 0.4931209037061094; l2 norm of gradients: 0.052063505831449114; l2 norm of weights: 0.43022826623752525\n","Iteration #4524  Loss: 0.4931140439993645; l2 norm of gradients: 0.05200735490744851; l2 norm of weights: 0.430214640013404\n","Iteration #4525  Loss: 0.4931071914479055; l2 norm of gradients: 0.051951265836976; l2 norm of weights: 0.4302010321354709\n","Iteration #4526  Loss: 0.493100346038696; l2 norm of gradients: 0.051895238552013875; l2 norm of weights: 0.4301874425708396\n","Iteration #4527  Loss: 0.49309350775872735; l2 norm of gradients: 0.051839272984619995; l2 norm of weights: 0.43017387128668694\n","Iteration #4528  Loss: 0.4930866765950184; l2 norm of gradients: 0.0517833690669277; l2 norm of weights: 0.4301603182502527\n","Iteration #4529  Loss: 0.4930798525346163; l2 norm of gradients: 0.05172752673114568; l2 norm of weights: 0.4301467834288398\n","Iteration #4530  Loss: 0.4930730355645949; l2 norm of gradients: 0.0516717459095579; l2 norm of weights: 0.43013326678981334\n","Iteration #4531  Loss: 0.49306622567205644; l2 norm of gradients: 0.05161602653452357; l2 norm of weights: 0.4301197683006019\n","Iteration #4532  Loss: 0.4930594228441299; l2 norm of gradients: 0.05156036853847699; l2 norm of weights: 0.43010628792869565\n","Iteration #4533  Loss: 0.49305262706797237; l2 norm of gradients: 0.051504771853927545; l2 norm of weights: 0.43009282564164797\n","Iteration #4534  Loss: 0.4930458383307678; l2 norm of gradients: 0.05144923641345952; l2 norm of weights: 0.43007938140707386\n","Iteration #4535  Loss: 0.4930390566197275; l2 norm of gradients: 0.05139376214973211; l2 norm of weights: 0.43006595519265106\n","Iteration #4536  Loss: 0.4930322819220902; l2 norm of gradients: 0.05133834899547929; l2 norm of weights: 0.43005254696611883\n","Iteration #4537  Loss: 0.49302551422512175; l2 norm of gradients: 0.05128299688350969; l2 norm of weights: 0.4300391566952788\n","Iteration #4538  Loss: 0.4930187535161149; l2 norm of gradients: 0.05122770574670666; l2 norm of weights: 0.430025784347994\n","Iteration #4539  Loss: 0.49301199978238963; l2 norm of gradients: 0.05117247551802801; l2 norm of weights: 0.4300124298921894\n","Iteration #4540  Loss: 0.49300525301129305; l2 norm of gradients: 0.051117306130506045; l2 norm of weights: 0.4299990932958515\n","Iteration #4541  Loss: 0.49299851319019916; l2 norm of gradients: 0.05106219751724741; l2 norm of weights: 0.42998577452702824\n","Iteration #4542  Loss: 0.49299178030650836; l2 norm of gradients: 0.05100714961143303; l2 norm of weights: 0.42997247355382867\n","Iteration #4543  Loss: 0.4929850543476487; l2 norm of gradients: 0.0509521623463181; l2 norm of weights: 0.4299591903444235\n","Iteration #4544  Loss: 0.4929783353010743; l2 norm of gradients: 0.050897235655231854; l2 norm of weights: 0.4299459248670441\n","Iteration #4545  Loss: 0.49297162315426635; l2 norm of gradients: 0.05084236947157766; l2 norm of weights: 0.429932677089983\n","Iteration #4546  Loss: 0.49296491789473235; l2 norm of gradients: 0.05078756372883273; l2 norm of weights: 0.42991944698159357\n","Iteration #4547  Loss: 0.4929582195100068; l2 norm of gradients: 0.05073281836054822; l2 norm of weights: 0.42990623451029\n","Iteration #4548  Loss: 0.4929515279876506; l2 norm of gradients: 0.05067813330034912; l2 norm of weights: 0.4298930396445469\n","Iteration #4549  Loss: 0.4929448433152507; l2 norm of gradients: 0.050623508481934025; l2 norm of weights: 0.42987986235289954\n","Iteration #4550  Loss: 0.49293816548042113; l2 norm of gradients: 0.05056894383907519; l2 norm of weights: 0.42986670260394355\n","Iteration #4551  Loss: 0.492931494470802; l2 norm of gradients: 0.0505144393056185; l2 norm of weights: 0.429853560366335\n","Iteration #4552  Loss: 0.4929248302740593; l2 norm of gradients: 0.05045999481548317; l2 norm of weights: 0.42984043560878965\n","Iteration #4553  Loss: 0.492918172877886; l2 norm of gradients: 0.05040561030266189; l2 norm of weights: 0.4298273283000838\n","Iteration #4554  Loss: 0.4929115222700005; l2 norm of gradients: 0.05035128570122063; l2 norm of weights: 0.4298142384090533\n","Iteration #4555  Loss: 0.49290487843814806; l2 norm of gradients: 0.05029702094529854; l2 norm of weights: 0.42980116590459433\n","Iteration #4556  Loss: 0.49289824137009947; l2 norm of gradients: 0.05024281596910794; l2 norm of weights: 0.42978811075566203\n","Iteration #4557  Loss: 0.49289161105365187; l2 norm of gradients: 0.05018867070693419; l2 norm of weights: 0.42977507293127176\n","Iteration #4558  Loss: 0.4928849874766279; l2 norm of gradients: 0.050134585093135633; l2 norm of weights: 0.4297620524004981\n","Iteration #4559  Loss: 0.49287837062687656; l2 norm of gradients: 0.050080559062143475; l2 norm of weights: 0.42974904913247486\n","Iteration #4560  Loss: 0.4928717604922725; l2 norm of gradients: 0.05002659254846178; l2 norm of weights: 0.4297360630963952\n","Iteration #4561  Loss: 0.4928651570607161; l2 norm of gradients: 0.04997268548666728; l2 norm of weights: 0.42972309426151156\n","Iteration #4562  Loss: 0.4928585603201336; l2 norm of gradients: 0.0499188378114094; l2 norm of weights: 0.42971014259713514\n","Iteration #4563  Loss: 0.4928519702584768; l2 norm of gradients: 0.049865049457410084; l2 norm of weights: 0.42969720807263617\n","Iteration #4564  Loss: 0.49284538686372303; l2 norm of gradients: 0.0498113203594638; l2 norm of weights: 0.4296842906574436\n","Iteration #4565  Loss: 0.49283881012387554; l2 norm of gradients: 0.049757650452437414; l2 norm of weights: 0.4296713903210451\n","Iteration #4566  Loss: 0.4928322400269627; l2 norm of gradients: 0.04970403967127011; l2 norm of weights: 0.42965850703298697\n","Iteration #4567  Loss: 0.49282567656103843; l2 norm of gradients: 0.049650487950973286; l2 norm of weights: 0.42964564076287376\n","Iteration #4568  Loss: 0.49281911971418213; l2 norm of gradients: 0.049596995226630546; l2 norm of weights: 0.42963279148036837\n","Iteration #4569  Loss: 0.4928125694744984; l2 norm of gradients: 0.04954356143339752; l2 norm of weights: 0.42961995915519213\n","Iteration #4570  Loss: 0.49280602583011723; l2 norm of gradients: 0.04949018650650189; l2 norm of weights: 0.4296071437571243\n","Iteration #4571  Loss: 0.49279948876919377; l2 norm of gradients: 0.04943687038124324; l2 norm of weights: 0.42959434525600215\n","Iteration #4572  Loss: 0.49279295827990843; l2 norm of gradients: 0.04938361299299298; l2 norm of weights: 0.429581563621721\n","Iteration #4573  Loss: 0.49278643435046654; l2 norm of gradients: 0.04933041427719428; l2 norm of weights: 0.4295687988242336\n","Iteration #4574  Loss: 0.49277991696909873; l2 norm of gradients: 0.049277274169362006; l2 norm of weights: 0.42955605083355075\n","Iteration #4575  Loss: 0.4927734061240605; l2 norm of gradients: 0.049224192605082616; l2 norm of weights: 0.42954331961974046\n","Iteration #4576  Loss: 0.49276690180363214; l2 norm of gradients: 0.04917116952001405; l2 norm of weights: 0.4295306051529284\n","Iteration #4577  Loss: 0.4927604039961193; l2 norm of gradients: 0.049118204849885765; l2 norm of weights: 0.42951790740329754\n","Iteration #4578  Loss: 0.4927539126898519; l2 norm of gradients: 0.049065298530498486; l2 norm of weights: 0.429505226341088\n","Iteration #4579  Loss: 0.49274742787318504; l2 norm of gradients: 0.0490124504977243; l2 norm of weights: 0.42949256193659696\n","Iteration #4580  Loss: 0.4927409495344984; l2 norm of gradients: 0.0489596606875064; l2 norm of weights: 0.42947991416017867\n","Iteration #4581  Loss: 0.49273447766219636; l2 norm of gradients: 0.0489069290358592; l2 norm of weights: 0.4294672829822443\n","Iteration #4582  Loss: 0.49272801224470786; l2 norm of gradients: 0.04885425547886809; l2 norm of weights: 0.42945466837326174\n","Iteration #4583  Loss: 0.4927215532704865; l2 norm of gradients: 0.04880163995268943; l2 norm of weights: 0.42944207030375553\n","Iteration #4584  Loss: 0.49271510072801056; l2 norm of gradients: 0.04874908239355046; l2 norm of weights: 0.4294294887443069\n","Iteration #4585  Loss: 0.4927086546057823; l2 norm of gradients: 0.04869658273774926; l2 norm of weights: 0.42941692366555334\n","Iteration #4586  Loss: 0.49270221489232874; l2 norm of gradients: 0.048644140921654574; l2 norm of weights: 0.4294043750381887\n","Iteration #4587  Loss: 0.4926957815762013; l2 norm of gradients: 0.048591756881705875; l2 norm of weights: 0.42939184283296317\n","Iteration #4588  Loss: 0.49268935464597546; l2 norm of gradients: 0.04853943055441308; l2 norm of weights: 0.42937932702068315\n","Iteration #4589  Loss: 0.4926829340902512; l2 norm of gradients: 0.0484871618763567; l2 norm of weights: 0.4293668275722107\n","Iteration #4590  Loss: 0.4926765198976524; l2 norm of gradients: 0.0484349507841876; l2 norm of weights: 0.4293543444584641\n","Iteration #4591  Loss: 0.4926701120568274; l2 norm of gradients: 0.04838279721462702; l2 norm of weights: 0.4293418776504174\n","Iteration #4592  Loss: 0.49266371055644825; l2 norm of gradients: 0.048330701104466384; l2 norm of weights: 0.4293294271191001\n","Iteration #4593  Loss: 0.49265731538521157; l2 norm of gradients: 0.04827866239056734; l2 norm of weights: 0.4293169928355977\n","Iteration #4594  Loss: 0.49265092653183756; l2 norm of gradients: 0.04822668100986161; l2 norm of weights: 0.4293045747710508\n","Iteration #4595  Loss: 0.4926445439850701; l2 norm of gradients: 0.048174756899350954; l2 norm of weights: 0.4292921728966553\n","Iteration #4596  Loss: 0.4926381677336776; l2 norm of gradients: 0.04812288999610705; l2 norm of weights: 0.4292797871836628\n","Iteration #4597  Loss: 0.49263179776645205; l2 norm of gradients: 0.04807108023727143; l2 norm of weights: 0.42926741760337966\n","Iteration #4598  Loss: 0.49262543407220893; l2 norm of gradients: 0.04801932756005542; l2 norm of weights: 0.4292550641271676\n","Iteration #4599  Loss: 0.49261907663978777; l2 norm of gradients: 0.047967631901740033; l2 norm of weights: 0.42924272672644287\n","Iteration #4600  Loss: 0.4926127254580515; l2 norm of gradients: 0.04791599319967593; l2 norm of weights: 0.4292304053726771\n","Iteration #4601  Loss: 0.49260638051588707; l2 norm of gradients: 0.04786441139128332; l2 norm of weights: 0.429218100037396\n","Iteration #4602  Loss: 0.49260004180220474; l2 norm of gradients: 0.04781288641405186; l2 norm of weights: 0.42920581069218056\n","Iteration #4603  Loss: 0.4925937093059381; l2 norm of gradients: 0.04776141820554062; l2 norm of weights: 0.4291935373086659\n","Iteration #4604  Loss: 0.49258738301604454; l2 norm of gradients: 0.047710006703377966; l2 norm of weights: 0.4291812798585415\n","Iteration #4605  Loss: 0.4925810629215045; l2 norm of gradients: 0.047658651845261545; l2 norm of weights: 0.4291690383135514\n","Iteration #4606  Loss: 0.49257474901132225; l2 norm of gradients: 0.04760735356895811; l2 norm of weights: 0.4291568126454936\n","Iteration #4607  Loss: 0.4925684412745249; l2 norm of gradients: 0.04755611181230353; l2 norm of weights: 0.42914460282622047\n","Iteration #4608  Loss: 0.4925621397001634; l2 norm of gradients: 0.047504926513202664; l2 norm of weights: 0.42913240882763815\n","Iteration #4609  Loss: 0.49255584427731103; l2 norm of gradients: 0.0474537976096293; l2 norm of weights: 0.4291202306217067\n","Iteration #4610  Loss: 0.49254955499506503; l2 norm of gradients: 0.047402725039626115; l2 norm of weights: 0.42910806818044\n","Iteration #4611  Loss: 0.49254327184254554; l2 norm of gradients: 0.0473517087413045; l2 norm of weights: 0.42909592147590564\n","Iteration #4612  Loss: 0.4925369948088954; l2 norm of gradients: 0.0473007486528446; l2 norm of weights: 0.4290837904802248\n","Iteration #4613  Loss: 0.49253072388328095; l2 norm of gradients: 0.04724984471249518; l2 norm of weights: 0.42907167516557204\n","Iteration #4614  Loss: 0.4925244590548911; l2 norm of gradients: 0.04719899685857346; l2 norm of weights: 0.42905957550417523\n","Iteration #4615  Loss: 0.492518200312938; l2 norm of gradients: 0.04714820502946527; l2 norm of weights: 0.42904749146831583\n","Iteration #4616  Loss: 0.4925119476466563; l2 norm of gradients: 0.04709746916362473; l2 norm of weights: 0.42903542303032804\n","Iteration #4617  Loss: 0.4925057010453036; l2 norm of gradients: 0.047046789199574286; l2 norm of weights: 0.42902337016259945\n","Iteration #4618  Loss: 0.4924994604981604; l2 norm of gradients: 0.04699616507590468; l2 norm of weights: 0.42901133283757037\n","Iteration #4619  Loss: 0.4924932259945297; l2 norm of gradients: 0.046945596731274776; l2 norm of weights: 0.42899931102773425\n","Iteration #4620  Loss: 0.49248699752373726; l2 norm of gradients: 0.04689508410441152; l2 norm of weights: 0.42898730470563706\n","Iteration #4621  Loss: 0.4924807750751315; l2 norm of gradients: 0.04684462713410988; l2 norm of weights: 0.4289753138438774\n","Iteration #4622  Loss: 0.4924745586380832; l2 norm of gradients: 0.04679422575923276; l2 norm of weights: 0.42896333841510653\n","Iteration #4623  Loss: 0.4924683482019859; l2 norm of gradients: 0.04674387991871093; l2 norm of weights: 0.42895137839202807\n","Iteration #4624  Loss: 0.49246214375625524; l2 norm of gradients: 0.04669358955154295; l2 norm of weights: 0.4289394337473982\n","Iteration #4625  Loss: 0.4924559452903296; l2 norm of gradients: 0.04664335459679505; l2 norm of weights: 0.42892750445402505\n","Iteration #4626  Loss: 0.4924497527936697; l2 norm of gradients: 0.046593174993601126; l2 norm of weights: 0.42891559048476907\n","Iteration #4627  Loss: 0.49244356625575847; l2 norm of gradients: 0.046543050681162636; l2 norm of weights: 0.42890369181254273\n","Iteration #4628  Loss: 0.49243738566610096; l2 norm of gradients: 0.04649298159874848; l2 norm of weights: 0.4288918084103104\n","Iteration #4629  Loss: 0.49243121101422466; l2 norm of gradients: 0.046442967685694996; l2 norm of weights: 0.4288799402510883\n","Iteration #4630  Loss: 0.49242504228967915; l2 norm of gradients: 0.04639300888140588; l2 norm of weights: 0.42886808730794435\n","Iteration #4631  Loss: 0.4924188794820361; l2 norm of gradients: 0.046343105125352; l2 norm of weights: 0.42885624955399837\n","Iteration #4632  Loss: 0.4924127225808892; l2 norm of gradients: 0.04629325635707154; l2 norm of weights: 0.42884442696242125\n","Iteration #4633  Loss: 0.4924065715758543; l2 norm of gradients: 0.0462434625161696; l2 norm of weights: 0.4288326195064357\n","Iteration #4634  Loss: 0.4924004264565692; l2 norm of gradients: 0.0461937235423185; l2 norm of weights: 0.42882082715931574\n","Iteration #4635  Loss: 0.4923942872126933; l2 norm of gradients: 0.046144039375257385; l2 norm of weights: 0.4288090498943863\n","Iteration #4636  Loss: 0.4923881538339086; l2 norm of gradients: 0.04609440995479236; l2 norm of weights: 0.4287972876850239\n","Iteration #4637  Loss: 0.4923820263099179; l2 norm of gradients: 0.046044835220796276; l2 norm of weights: 0.42878554050465584\n","Iteration #4638  Loss: 0.4923759046304468; l2 norm of gradients: 0.04599531511320876; l2 norm of weights: 0.42877380832676043\n","Iteration #4639  Loss: 0.492369788785242; l2 norm of gradients: 0.045945849572036096; l2 norm of weights: 0.4287620911248668\n","Iteration #4640  Loss: 0.4923636787640719; l2 norm of gradients: 0.04589643853735112; l2 norm of weights: 0.42875038887255484\n","Iteration #4641  Loss: 0.492357574556727; l2 norm of gradients: 0.04584708194929319; l2 norm of weights: 0.42873870154345506\n","Iteration #4642  Loss: 0.4923514761530188; l2 norm of gradients: 0.04579777974806812; l2 norm of weights: 0.42872702911124866\n","Iteration #4643  Loss: 0.49234538354278073; l2 norm of gradients: 0.04574853187394808; l2 norm of weights: 0.428715371549667\n","Iteration #4644  Loss: 0.4923392967158676; l2 norm of gradients: 0.04569933826727148; l2 norm of weights: 0.4287037288324922\n","Iteration #4645  Loss: 0.49233321566215577; l2 norm of gradients: 0.04565019886844297; l2 norm of weights: 0.42869210093355625\n","Iteration #4646  Loss: 0.4923271403715428; l2 norm of gradients: 0.04560111361793338; l2 norm of weights: 0.4286804878267415\n","Iteration #4647  Loss: 0.49232107083394766; l2 norm of gradients: 0.04555208245627954; l2 norm of weights: 0.4286688894859803\n","Iteration #4648  Loss: 0.49231500703931075; l2 norm of gradients: 0.04550310532408433; l2 norm of weights: 0.4286573058852551\n","Iteration #4649  Loss: 0.4923089489775937; l2 norm of gradients: 0.04545418216201648; l2 norm of weights: 0.428645736998598\n","Iteration #4650  Loss: 0.492302896638779; l2 norm of gradients: 0.04540531291081063; l2 norm of weights: 0.42863418280009097\n","Iteration #4651  Loss: 0.49229685001287116; l2 norm of gradients: 0.04535649751126714; l2 norm of weights: 0.4286226432638658\n","Iteration #4652  Loss: 0.49229080908989475; l2 norm of gradients: 0.0453077359042521; l2 norm of weights: 0.4286111183641036\n","Iteration #4653  Loss: 0.49228477385989616; l2 norm of gradients: 0.0452590280306972; l2 norm of weights: 0.42859960807503494\n","Iteration #4654  Loss: 0.4922787443129426; l2 norm of gradients: 0.04521037383159969; l2 norm of weights: 0.4285881123709403\n","Iteration #4655  Loss: 0.49227272043912207; l2 norm of gradients: 0.04516177324802231; l2 norm of weights: 0.42857663122614875\n","Iteration #4656  Loss: 0.49226670222854396; l2 norm of gradients: 0.04511322622109317; l2 norm of weights: 0.4285651646150391\n","Iteration #4657  Loss: 0.49226068967133796; l2 norm of gradients: 0.04506473269200576; l2 norm of weights: 0.42855371251203883\n","Iteration #4658  Loss: 0.49225468275765494; l2 norm of gradients: 0.04501629260201877; l2 norm of weights: 0.4285422748916248\n","Iteration #4659  Loss: 0.49224868147766676; l2 norm of gradients: 0.04496790589245613; l2 norm of weights: 0.42853085172832245\n","Iteration #4660  Loss: 0.4922426858215657; l2 norm of gradients: 0.044919572504706844; l2 norm of weights: 0.42851944299670625\n","Iteration #4661  Loss: 0.49223669577956475; l2 norm of gradients: 0.04487129238022495; l2 norm of weights: 0.4285080486713994\n","Iteration #4662  Loss: 0.49223071134189783; l2 norm of gradients: 0.04482306546052952; l2 norm of weights: 0.42849666872707365\n","Iteration #4663  Loss: 0.49222473249881926; l2 norm of gradients: 0.04477489168720443; l2 norm of weights: 0.42848530313844924\n","Iteration #4664  Loss: 0.49221875924060404; l2 norm of gradients: 0.04472677100189841; l2 norm of weights: 0.42847395188029486\n","Iteration #4665  Loss: 0.4922127915575476; l2 norm of gradients: 0.04467870334632501; l2 norm of weights: 0.42846261492742765\n","Iteration #4666  Loss: 0.492206829439966; l2 norm of gradients: 0.04463068866226234; l2 norm of weights: 0.4284512922547129\n","Iteration #4667  Loss: 0.4922008728781955; l2 norm of gradients: 0.04458272689155318; l2 norm of weights: 0.4284399838370641\n","Iteration #4668  Loss: 0.49219492186259295; l2 norm of gradients: 0.044534817976104854; l2 norm of weights: 0.42842868964944275\n","Iteration #4669  Loss: 0.4921889763835356; l2 norm of gradients: 0.044486961857889126; l2 norm of weights: 0.42841740966685843\n","Iteration #4670  Loss: 0.4921830364314208; l2 norm of gradients: 0.044439158478942135; l2 norm of weights: 0.42840614386436837\n","Iteration #4671  Loss: 0.49217710199666637; l2 norm of gradients: 0.04439140778136438; l2 norm of weights: 0.4283948922170779\n","Iteration #4672  Loss: 0.49217117306971003; l2 norm of gradients: 0.04434370970732055; l2 norm of weights: 0.42838365470013967\n","Iteration #4673  Loss: 0.49216524964101005; l2 norm of gradients: 0.04429606419903955; l2 norm of weights: 0.4283724312887543\n","Iteration #4674  Loss: 0.49215933170104464; l2 norm of gradients: 0.0442484711988144; l2 norm of weights: 0.42836122195816967\n","Iteration #4675  Loss: 0.49215341924031203; l2 norm of gradients: 0.044200930649002074; l2 norm of weights: 0.428350026683681\n","Iteration #4676  Loss: 0.49214751224933073; l2 norm of gradients: 0.04415344249202359; l2 norm of weights: 0.4283388454406313\n","Iteration #4677  Loss: 0.492141610718639; l2 norm of gradients: 0.044106006670363816; l2 norm of weights: 0.42832767820441014\n","Iteration #4678  Loss: 0.49213571463879513; l2 norm of gradients: 0.04405862312657141; l2 norm of weights: 0.4283165249504548\n","Iteration #4679  Loss: 0.49212982400037725; l2 norm of gradients: 0.044011291803258856; l2 norm of weights: 0.42830538565424914\n","Iteration #4680  Loss: 0.49212393879398353; l2 norm of gradients: 0.04396401264310225; l2 norm of weights: 0.42829426029132445\n","Iteration #4681  Loss: 0.492118059010232; l2 norm of gradients: 0.04391678558884126; l2 norm of weights: 0.4282831488372586\n","Iteration #4682  Loss: 0.49211218463976003; l2 norm of gradients: 0.04386961058327918; l2 norm of weights: 0.42827205126767603\n","Iteration #4683  Loss: 0.4921063156732252; l2 norm of gradients: 0.043822487569282684; l2 norm of weights: 0.4282609675582482\n","Iteration #4684  Loss: 0.49210045210130465; l2 norm of gradients: 0.04377541648978189; l2 norm of weights: 0.4282498976846932\n","Iteration #4685  Loss: 0.492094593914695; l2 norm of gradients: 0.04372839728777018; l2 norm of weights: 0.42823884162277537\n","Iteration #4686  Loss: 0.4920887411041127; l2 norm of gradients: 0.04368142990630425; l2 norm of weights: 0.42822779934830557\n","Iteration #4687  Loss: 0.49208289366029373; l2 norm of gradients: 0.043634514288503894; l2 norm of weights: 0.428216770837141\n","Iteration #4688  Loss: 0.49207705157399334; l2 norm of gradients: 0.04358765037755211; l2 norm of weights: 0.42820575606518496\n","Iteration #4689  Loss: 0.4920712148359866; l2 norm of gradients: 0.04354083811669485; l2 norm of weights: 0.42819475500838705\n","Iteration #4690  Loss: 0.49206538343706774; l2 norm of gradients: 0.0434940774492411; l2 norm of weights: 0.42818376764274285\n","Iteration #4691  Loss: 0.4920595573680505; l2 norm of gradients: 0.043447368318562656; l2 norm of weights: 0.4281727939442938\n","Iteration #4692  Loss: 0.4920537366197682; l2 norm of gradients: 0.04340071066809426; l2 norm of weights: 0.4281618338891276\n","Iteration #4693  Loss: 0.49204792118307306; l2 norm of gradients: 0.04335410444133332; l2 norm of weights: 0.42815088745337726\n","Iteration #4694  Loss: 0.49204211104883666; l2 norm of gradients: 0.04330754958183995; l2 norm of weights: 0.4281399546132215\n","Iteration #4695  Loss: 0.49203630620795; l2 norm of gradients: 0.043261046033236876; l2 norm of weights: 0.42812903534488517\n","Iteration #4696  Loss: 0.4920305066513232; l2 norm of gradients: 0.043214593739209384; l2 norm of weights: 0.428118129624638\n","Iteration #4697  Loss: 0.4920247123698854; l2 norm of gradients: 0.04316819264350528; l2 norm of weights: 0.4281072374287955\n","Iteration #4698  Loss: 0.4920189233545848; l2 norm of gradients: 0.04312184268993465; l2 norm of weights: 0.4280963587337184\n","Iteration #4699  Loss: 0.492013139596389; l2 norm of gradients: 0.04307554382237005; l2 norm of weights: 0.4280854935158128\n","Iteration #4700  Loss: 0.4920073610862842; l2 norm of gradients: 0.043029295984746245; l2 norm of weights: 0.42807464175152976\n","Iteration #4701  Loss: 0.4920015878152757; l2 norm of gradients: 0.04298309912106018; l2 norm of weights: 0.4280638034173657\n","Iteration #4702  Loss: 0.4919958197743881; l2 norm of gradients: 0.042936953175371; l2 norm of weights: 0.4280529784898618\n","Iteration #4703  Loss: 0.49199005695466413; l2 norm of gradients: 0.04289085809179984; l2 norm of weights: 0.428042166945604\n","Iteration #4704  Loss: 0.4919842993471662; l2 norm of gradients: 0.04284481381452982; l2 norm of weights: 0.4280313687612234\n","Iteration #4705  Loss: 0.49197854694297494; l2 norm of gradients: 0.04279882028780606; l2 norm of weights: 0.4280205839133957\n","Iteration #4706  Loss: 0.49197279973318986; l2 norm of gradients: 0.04275287745593545; l2 norm of weights: 0.4280098123788411\n","Iteration #4707  Loss: 0.4919670577089295; l2 norm of gradients: 0.042706985263286715; l2 norm of weights: 0.4279990541343246\n","Iteration #4708  Loss: 0.4919613208613307; l2 norm of gradients: 0.042661143654290266; l2 norm of weights: 0.4279883091566555\n","Iteration #4709  Loss: 0.4919555891815492; l2 norm of gradients: 0.04261535257343815; l2 norm of weights: 0.42797757742268727\n","Iteration #4710  Loss: 0.4919498626607591; l2 norm of gradients: 0.04256961196528403; l2 norm of weights: 0.4279668589093181\n","Iteration #4711  Loss: 0.49194414129015335; l2 norm of gradients: 0.04252392177444304; l2 norm of weights: 0.42795615359349015\n","Iteration #4712  Loss: 0.4919384250609434; l2 norm of gradients: 0.0424782819455918; l2 norm of weights: 0.42794546145218976\n","Iteration #4713  Loss: 0.49193271396435856; l2 norm of gradients: 0.04243269242346823; l2 norm of weights: 0.4279347824624471\n","Iteration #4714  Loss: 0.49192700799164774; l2 norm of gradients: 0.04238715315287161; l2 norm of weights: 0.4279241166013367\n","Iteration #4715  Loss: 0.4919213071340771; l2 norm of gradients: 0.04234166407866244; l2 norm of weights: 0.4279134638459764\n","Iteration #4716  Loss: 0.49191561138293194; l2 norm of gradients: 0.04229622514576238; l2 norm of weights: 0.42790282417352843\n","Iteration #4717  Loss: 0.49190992072951534; l2 norm of gradients: 0.042250836299154174; l2 norm of weights: 0.4278921975611982\n","Iteration #4718  Loss: 0.49190423516514925; l2 norm of gradients: 0.04220549748388162; l2 norm of weights: 0.42788158398623505\n","Iteration #4719  Loss: 0.49189855468117316; l2 norm of gradients: 0.04216020864504947; l2 norm of weights: 0.42787098342593155\n","Iteration #4720  Loss: 0.49189287926894537; l2 norm of gradients: 0.042114969727823325; l2 norm of weights: 0.42786039585762403\n","Iteration #4721  Loss: 0.4918872089198421; l2 norm of gradients: 0.04206978067742971; l2 norm of weights: 0.4278498212586919\n","Iteration #4722  Loss: 0.4918815436252575; l2 norm of gradients: 0.042024641439155797; l2 norm of weights: 0.42783925960655805\n","Iteration #4723  Loss: 0.4918758833766041; l2 norm of gradients: 0.04197955195834953; l2 norm of weights: 0.42782871087868846\n","Iteration #4724  Loss: 0.49187022816531234; l2 norm of gradients: 0.04193451218041942; l2 norm of weights: 0.42781817505259206\n","Iteration #4725  Loss: 0.4918645779828307; l2 norm of gradients: 0.04188952205083457; l2 norm of weights: 0.4278076521058211\n","Iteration #4726  Loss: 0.49185893282062554; l2 norm of gradients: 0.04184458151512456; l2 norm of weights: 0.4277971420159705\n","Iteration #4727  Loss: 0.49185329267018124; l2 norm of gradients: 0.04179969051887934; l2 norm of weights: 0.4277866447606781\n","Iteration #4728  Loss: 0.4918476575230002; l2 norm of gradients: 0.0417548490077493; l2 norm of weights: 0.42777616031762467\n","Iteration #4729  Loss: 0.49184202737060223; l2 norm of gradients: 0.04171005692744503; l2 norm of weights: 0.42776568866453335\n","Iteration #4730  Loss: 0.4918364022045254; l2 norm of gradients: 0.04166531422373741; l2 norm of weights: 0.4277552297791701\n","Iteration #4731  Loss: 0.49183078201632513; l2 norm of gradients: 0.04162062084245741; l2 norm of weights: 0.4277447836393434\n","Iteration #4732  Loss: 0.491825166797575; l2 norm of gradients: 0.04157597672949613; l2 norm of weights: 0.4277343502229039\n","Iteration #4733  Loss: 0.4918195565398661; l2 norm of gradients: 0.04153138183080463; l2 norm of weights: 0.42772392950774496\n","Iteration #4734  Loss: 0.4918139512348071; l2 norm of gradients: 0.04148683609239398; l2 norm of weights: 0.42771352147180214\n","Iteration #4735  Loss: 0.4918083508740241; l2 norm of gradients: 0.041442339460335084; l2 norm of weights: 0.4277031260930529\n","Iteration #4736  Loss: 0.4918027554491614; l2 norm of gradients: 0.041397891880758673; l2 norm of weights: 0.427692743349517\n","Iteration #4737  Loss: 0.4917971649518802; l2 norm of gradients: 0.04135349329985525; l2 norm of weights: 0.4276823732192564\n","Iteration #4738  Loss: 0.49179157937385953; l2 norm of gradients: 0.04130914366387496; l2 norm of weights: 0.42767201568037455\n","Iteration #4739  Loss: 0.49178599870679574; l2 norm of gradients: 0.041264842919127624; l2 norm of weights: 0.42766167071101713\n","Iteration #4740  Loss: 0.4917804229424025; l2 norm of gradients: 0.04122059101198252; l2 norm of weights: 0.42765133828937146\n","Iteration #4741  Loss: 0.49177485207241156; l2 norm of gradients: 0.0411763878888685; l2 norm of weights: 0.4276410183936667\n","Iteration #4742  Loss: 0.4917692860885707; l2 norm of gradients: 0.04113223349627378; l2 norm of weights: 0.4276307110021732\n","Iteration #4743  Loss: 0.4917637249826463; l2 norm of gradients: 0.04108812778074592; l2 norm of weights: 0.4276204160932034\n","Iteration #4744  Loss: 0.4917581687464215; l2 norm of gradients: 0.041044070688891814; l2 norm of weights: 0.42761013364511086\n","Iteration #4745  Loss: 0.4917526173716963; l2 norm of gradients: 0.04100006216737751; l2 norm of weights: 0.42759986363629043\n","Iteration #4746  Loss: 0.4917470708502884; l2 norm of gradients: 0.04095610216292824; l2 norm of weights: 0.42758960604517854\n","Iteration #4747  Loss: 0.49174152917403274; l2 norm of gradients: 0.040912190622328315; l2 norm of weights: 0.42757936085025267\n","Iteration #4748  Loss: 0.49173599233478094; l2 norm of gradients: 0.04086832749242108; l2 norm of weights: 0.4275691280300313\n","Iteration #4749  Loss: 0.491730460324402; l2 norm of gradients: 0.04082451272010879; l2 norm of weights: 0.4275589075630742\n","Iteration #4750  Loss: 0.4917249331347816; l2 norm of gradients: 0.04078074625235265; l2 norm of weights: 0.4275486994279819\n","Iteration #4751  Loss: 0.49171941075782316; l2 norm of gradients: 0.04073702803617265; l2 norm of weights: 0.4275385036033961\n","Iteration #4752  Loss: 0.49171389318544617; l2 norm of gradients: 0.04069335801864752; l2 norm of weights: 0.427528320067999\n","Iteration #4753  Loss: 0.49170838040958786; l2 norm of gradients: 0.04064973614691471; l2 norm of weights: 0.42751814880051364\n","Iteration #4754  Loss: 0.4917028724222016; l2 norm of gradients: 0.04060616236817028; l2 norm of weights: 0.4275079897797037\n","Iteration #4755  Loss: 0.49169736921525853; l2 norm of gradients: 0.040562636629668865; l2 norm of weights: 0.42749784298437354\n","Iteration #4756  Loss: 0.4916918707807456; l2 norm of gradients: 0.040519158878723566; l2 norm of weights: 0.42748770839336786\n","Iteration #4757  Loss: 0.4916863771106673; l2 norm of gradients: 0.040475729062705953; l2 norm of weights: 0.4274775859855718\n","Iteration #4758  Loss: 0.4916808881970445; l2 norm of gradients: 0.04043234712904591; l2 norm of weights: 0.42746747573991084\n","Iteration #4759  Loss: 0.49167540403191484; l2 norm of gradients: 0.04038901302523167; l2 norm of weights: 0.42745737763535085\n","Iteration #4760  Loss: 0.49166992460733283; l2 norm of gradients: 0.040345726698809675; l2 norm of weights: 0.4274472916508977\n","Iteration #4761  Loss: 0.49166444991536945; l2 norm of gradients: 0.04030248809738453; l2 norm of weights: 0.4274372177655974\n","Iteration #4762  Loss: 0.4916589799481122; l2 norm of gradients: 0.04025929716861896; l2 norm of weights: 0.427427155958536\n","Iteration #4763  Loss: 0.4916535146976654; l2 norm of gradients: 0.04021615386023368; l2 norm of weights: 0.4274171062088395\n","Iteration #4764  Loss: 0.49164805415614954; l2 norm of gradients: 0.04017305812000748; l2 norm of weights: 0.4274070684956738\n","Iteration #4765  Loss: 0.491642598315702; l2 norm of gradients: 0.04013000989577697; l2 norm of weights: 0.4273970427982444\n","Iteration #4766  Loss: 0.4916371471684764; l2 norm of gradients: 0.04008700913543662; l2 norm of weights: 0.4273870290957968\n","Iteration #4767  Loss: 0.49163170070664286; l2 norm of gradients: 0.040044055786938725; l2 norm of weights: 0.4273770273676159\n","Iteration #4768  Loss: 0.49162625892238776; l2 norm of gradients: 0.04000114979829325; l2 norm of weights: 0.4273670375930262\n","Iteration #4769  Loss: 0.4916208218079139; l2 norm of gradients: 0.039958291117567823; l2 norm of weights: 0.42735705975139165\n","Iteration #4770  Loss: 0.4916153893554406; l2 norm of gradients: 0.039915479692887675; l2 norm of weights: 0.4273470938221158\n","Iteration #4771  Loss: 0.49160996155720293; l2 norm of gradients: 0.03987271547243553; l2 norm of weights: 0.4273371397846413\n","Iteration #4772  Loss: 0.4916045384054528; l2 norm of gradients: 0.039829998404451616; l2 norm of weights: 0.42732719761845\n","Iteration #4773  Loss: 0.4915991198924582; l2 norm of gradients: 0.0397873284372335; l2 norm of weights: 0.42731726730306324\n","Iteration #4774  Loss: 0.4915937060105029; l2 norm of gradients: 0.03974470551913615; l2 norm of weights: 0.42730734881804106\n","Iteration #4775  Loss: 0.49158829675188714; l2 norm of gradients: 0.03970212959857171; l2 norm of weights: 0.42729744214298276\n","Iteration #4776  Loss: 0.49158289210892736; l2 norm of gradients: 0.0396596006240096; l2 norm of weights: 0.4272875472575266\n","Iteration #4777  Loss: 0.4915774920739556; l2 norm of gradients: 0.03961711854397636; l2 norm of weights: 0.42727766414134954\n","Iteration #4778  Loss: 0.49157209663932044; l2 norm of gradients: 0.039574683307055596; l2 norm of weights: 0.4272677927741674\n","Iteration #4779  Loss: 0.4915667057973865; l2 norm of gradients: 0.039532294861887925; l2 norm of weights: 0.42725793313573485\n","Iteration #4780  Loss: 0.4915613195405337; l2 norm of gradients: 0.03948995315717095; l2 norm of weights: 0.42724808520584484\n","Iteration #4781  Loss: 0.4915559378611583; l2 norm of gradients: 0.039447658141659105; l2 norm of weights: 0.42723824896432927\n","Iteration #4782  Loss: 0.49155056075167286; l2 norm of gradients: 0.03940540976416364; l2 norm of weights: 0.42722842439105835\n","Iteration #4783  Loss: 0.49154518820450505; l2 norm of gradients: 0.039363207973552646; l2 norm of weights: 0.42721861146594065\n","Iteration #4784  Loss: 0.49153982021209885; l2 norm of gradients: 0.0393210527187508; l2 norm of weights: 0.4272088101689232\n","Iteration #4785  Loss: 0.4915344567669139; l2 norm of gradients: 0.03927894394873948; l2 norm of weights: 0.42719902047999125\n","Iteration #4786  Loss: 0.4915290978614255; l2 norm of gradients: 0.03923688161255662; l2 norm of weights: 0.42718924237916817\n","Iteration #4787  Loss: 0.49152374348812494; l2 norm of gradients: 0.03919486565929667; l2 norm of weights: 0.42717947584651544\n","Iteration #4788  Loss: 0.4915183936395188; l2 norm of gradients: 0.039152896038110466; l2 norm of weights: 0.4271697208621326\n","Iteration #4789  Loss: 0.4915130483081297; l2 norm of gradients: 0.039110972698205286; l2 norm of weights: 0.42715997740615735\n","Iteration #4790  Loss: 0.4915077074864956; l2 norm of gradients: 0.03906909558884472; l2 norm of weights: 0.427150245458765\n","Iteration #4791  Loss: 0.4915023711671701; l2 norm of gradients: 0.03902726465934853; l2 norm of weights: 0.42714052500016875\n","Iteration #4792  Loss: 0.4914970393427225; l2 norm of gradients: 0.03898547985909281; l2 norm of weights: 0.4271308160106196\n","Iteration #4793  Loss: 0.4914917120057375; l2 norm of gradients: 0.03894374113750966; l2 norm of weights: 0.4271211184704062\n","Iteration #4794  Loss: 0.49148638914881526; l2 norm of gradients: 0.03890204844408729; l2 norm of weights: 0.42711143235985477\n","Iteration #4795  Loss: 0.49148107076457137; l2 norm of gradients: 0.0388604017283699; l2 norm of weights: 0.427101757659329\n","Iteration #4796  Loss: 0.49147575684563705; l2 norm of gradients: 0.03881880093995766; l2 norm of weights: 0.42709209434923023\n","Iteration #4797  Loss: 0.49147044738465856; l2 norm of gradients: 0.0387772460285066; l2 norm of weights: 0.42708244240999693\n","Iteration #4798  Loss: 0.49146514237429784; l2 norm of gradients: 0.03873573694372854; l2 norm of weights: 0.42707280182210494\n","Iteration #4799  Loss: 0.4914598418072318; l2 norm of gradients: 0.03869427363539109; l2 norm of weights: 0.4270631725660674\n","Iteration #4800  Loss: 0.49145454567615304; l2 norm of gradients: 0.03865285605331756; l2 norm of weights: 0.4270535546224346\n","Iteration #4801  Loss: 0.4914492539737689; l2 norm of gradients: 0.03861148414738684; l2 norm of weights: 0.42704394797179374\n","Iteration #4802  Loss: 0.4914439666928025; l2 norm of gradients: 0.0385701578675334; l2 norm of weights: 0.4270343525947694\n","Iteration #4803  Loss: 0.49143868382599176; l2 norm of gradients: 0.038528877163747255; l2 norm of weights: 0.42702476847202264\n","Iteration #4804  Loss: 0.49143340536608976; l2 norm of gradients: 0.038487641986073844; l2 norm of weights: 0.42701519558425166\n","Iteration #4805  Loss: 0.4914281313058648; l2 norm of gradients: 0.03844645228461397; l2 norm of weights: 0.4270056339121915\n","Iteration #4806  Loss: 0.4914228616381004; l2 norm of gradients: 0.03840530800952376; l2 norm of weights: 0.42699608343661366\n","Iteration #4807  Loss: 0.4914175963555948; l2 norm of gradients: 0.03836420911101462; l2 norm of weights: 0.42698654413832665\n","Iteration #4808  Loss: 0.49141233545116153; l2 norm of gradients: 0.03832315553935312; l2 norm of weights: 0.4269770159981752\n","Iteration #4809  Loss: 0.49140707891762897; l2 norm of gradients: 0.038282147244861; l2 norm of weights: 0.42696749899704084\n","Iteration #4810  Loss: 0.49140182674784044; l2 norm of gradients: 0.03824118417791503; l2 norm of weights: 0.4269579931158413\n","Iteration #4811  Loss: 0.49139657893465416; l2 norm of gradients: 0.03820026628894704; l2 norm of weights: 0.42694849833553067\n","Iteration #4812  Loss: 0.4913913354709434; l2 norm of gradients: 0.03815939352844379; l2 norm of weights: 0.4269390146370997\n","Iteration #4813  Loss: 0.4913860963495962; l2 norm of gradients: 0.038118565846946874; l2 norm of weights: 0.426929542001575\n","Iteration #4814  Loss: 0.49138086156351524; l2 norm of gradients: 0.03807778319505282; l2 norm of weights: 0.42692008041001944\n","Iteration #4815  Loss: 0.4913756311056182; l2 norm of gradients: 0.03803704552341285; l2 norm of weights: 0.42691062984353184\n","Iteration #4816  Loss: 0.49137040496883755; l2 norm of gradients: 0.03799635278273293; l2 norm of weights: 0.4269011902832473\n","Iteration #4817  Loss: 0.49136518314612043; l2 norm of gradients: 0.03795570492377361; l2 norm of weights: 0.42689176171033666\n","Iteration #4818  Loss: 0.4913599656304284; l2 norm of gradients: 0.03791510189735008; l2 norm of weights: 0.4268823441060067\n","Iteration #4819  Loss: 0.491354752414738; l2 norm of gradients: 0.03787454365433206; l2 norm of weights: 0.4268729374515\n","Iteration #4820  Loss: 0.49134954349204035; l2 norm of gradients: 0.03783403014564366; l2 norm of weights: 0.4268635417280948\n","Iteration #4821  Loss: 0.49134433885534107; l2 norm of gradients: 0.037793561322263504; l2 norm of weights: 0.42685415691710504\n","Iteration #4822  Loss: 0.49133913849766053; l2 norm of gradients: 0.03775313713522442; l2 norm of weights: 0.4268447829998804\n","Iteration #4823  Loss: 0.4913339424120334; l2 norm of gradients: 0.03771275753561366; l2 norm of weights: 0.4268354199578058\n","Iteration #4824  Loss: 0.49132875059150893; l2 norm of gradients: 0.03767242247457255; l2 norm of weights: 0.4268260677723018\n","Iteration #4825  Loss: 0.4913235630291507; l2 norm of gradients: 0.03763213190329671; l2 norm of weights: 0.4268167264248243\n","Iteration #4826  Loss: 0.4913183797180373; l2 norm of gradients: 0.03759188577303576; l2 norm of weights: 0.42680739589686456\n","Iteration #4827  Loss: 0.49131320065126094; l2 norm of gradients: 0.03755168403509341; l2 norm of weights: 0.426798076169949\n","Iteration #4828  Loss: 0.49130802582192873; l2 norm of gradients: 0.037511526640827325; l2 norm of weights: 0.42678876722563924\n","Iteration #4829  Loss: 0.49130285522316186; l2 norm of gradients: 0.03747141354164911; l2 norm of weights: 0.42677946904553193\n","Iteration #4830  Loss: 0.49129768884809605; l2 norm of gradients: 0.03743134468902419; l2 norm of weights: 0.4267701816112589\n","Iteration #4831  Loss: 0.491292526689881; l2 norm of gradients: 0.03739132003447183; l2 norm of weights: 0.426760904904487\n","Iteration #4832  Loss: 0.49128736874168116; l2 norm of gradients: 0.03735133952956499; l2 norm of weights: 0.42675163890691775\n","Iteration #4833  Loss: 0.4912822149966747; l2 norm of gradients: 0.03731140312593035; l2 norm of weights: 0.4267423836002876\n","Iteration #4834  Loss: 0.491277065448054; l2 norm of gradients: 0.0372715107752482; l2 norm of weights: 0.4267331389663679\n","Iteration #4835  Loss: 0.4912719200890261; l2 norm of gradients: 0.03723166242925232; l2 norm of weights: 0.4267239049869645\n","Iteration #4836  Loss: 0.49126677891281134; l2 norm of gradients: 0.03719185803973011; l2 norm of weights: 0.4267146816439179\n","Iteration #4837  Loss: 0.49126164191264504; l2 norm of gradients: 0.037152097558522314; l2 norm of weights: 0.4267054689191034\n","Iteration #4838  Loss: 0.4912565090817762; l2 norm of gradients: 0.03711238093752309; l2 norm of weights: 0.4266962667944304\n","Iteration #4839  Loss: 0.49125138041346744; l2 norm of gradients: 0.0370727081286799; l2 norm of weights: 0.42668707525184313\n","Iteration #4840  Loss: 0.4912462559009962; l2 norm of gradients: 0.03703307908399347; l2 norm of weights: 0.4266778942733199\n","Iteration #4841  Loss: 0.49124113553765325; l2 norm of gradients: 0.036993493755517734; l2 norm of weights: 0.4266687238408734\n","Iteration #4842  Loss: 0.49123601931674343; l2 norm of gradients: 0.03695395209535979; l2 norm of weights: 0.4266595639365505\n","Iteration #4843  Loss: 0.4912309072315857; l2 norm of gradients: 0.03691445405567977; l2 norm of weights: 0.4266504145424324\n","Iteration #4844  Loss: 0.4912257992755129; l2 norm of gradients: 0.03687499958869085; l2 norm of weights: 0.42664127564063414\n","Iteration #4845  Loss: 0.49122069544187136; l2 norm of gradients: 0.03683558864665919; l2 norm of weights: 0.426632147213305\n","Iteration #4846  Loss: 0.49121559572402157; l2 norm of gradients: 0.0367962211819038; l2 norm of weights: 0.42662302924262796\n","Iteration #4847  Loss: 0.4912105001153376; l2 norm of gradients: 0.0367568971467966; l2 norm of weights: 0.42661392171082013\n","Iteration #4848  Loss: 0.4912054086092077; l2 norm of gradients: 0.03671761649376225; l2 norm of weights: 0.42660482460013244\n","Iteration #4849  Loss: 0.49120032119903323; l2 norm of gradients: 0.03667837917527818; l2 norm of weights: 0.4265957378928495\n","Iteration #4850  Loss: 0.4911952378782296; l2 norm of gradients: 0.03663918514387446; l2 norm of weights: 0.42658666157128944\n","Iteration #4851  Loss: 0.49119015864022597; l2 norm of gradients: 0.03660003435213375; l2 norm of weights: 0.4265775956178044\n","Iteration #4852  Loss: 0.49118508347846485; l2 norm of gradients: 0.03656092675269131; l2 norm of weights: 0.42656854001477973\n","Iteration #4853  Loss: 0.4911800123864025; l2 norm of gradients: 0.03652186229823487; l2 norm of weights: 0.4265594947446345\n","Iteration #4854  Loss: 0.49117494535750916; l2 norm of gradients: 0.03648284094150456; l2 norm of weights: 0.42655045978982103\n","Iteration #4855  Loss: 0.49116988238526776; l2 norm of gradients: 0.03644386263529298; l2 norm of weights: 0.4265414351328252\n","Iteration #4856  Loss: 0.4911648234631756; l2 norm of gradients: 0.03640492733244491; l2 norm of weights: 0.42653242075616604\n","Iteration #4857  Loss: 0.49115976858474303; l2 norm of gradients: 0.03636603498585755; l2 norm of weights: 0.42652341664239585\n","Iteration #4858  Loss: 0.49115471774349373; l2 norm of gradients: 0.036327185548480155; l2 norm of weights: 0.4265144227741001\n","Iteration #4859  Loss: 0.49114967093296524; l2 norm of gradients: 0.0362883789733142; l2 norm of weights: 0.42650543913389755\n","Iteration #4860  Loss: 0.4911446281467083; l2 norm of gradients: 0.03624961521341325; l2 norm of weights: 0.42649646570443955\n","Iteration #4861  Loss: 0.49113958937828706; l2 norm of gradients: 0.03621089422188287; l2 norm of weights: 0.42648750246841083\n","Iteration #4862  Loss: 0.4911345546212787; l2 norm of gradients: 0.03617221595188057; l2 norm of weights: 0.42647854940852886\n","Iteration #4863  Loss: 0.4911295238692742; l2 norm of gradients: 0.03613358035661583; l2 norm of weights: 0.426469606507544\n","Iteration #4864  Loss: 0.4911244971158776; l2 norm of gradients: 0.036094987389349936; l2 norm of weights: 0.42646067374823937\n","Iteration #4865  Loss: 0.49111947435470626; l2 norm of gradients: 0.03605643700339601; l2 norm of weights: 0.4264517511134309\n","Iteration #4866  Loss: 0.49111445557939065; l2 norm of gradients: 0.03601792915211883; l2 norm of weights: 0.42644283858596704\n","Iteration #4867  Loss: 0.4911094407835745; l2 norm of gradients: 0.03597946378893498; l2 norm of weights: 0.42643393614872876\n","Iteration #4868  Loss: 0.49110442996091475; l2 norm of gradients: 0.03594104086731257; l2 norm of weights: 0.42642504378462986\n","Iteration #4869  Loss: 0.4910994231050815; l2 norm of gradients: 0.035902660340771296; l2 norm of weights: 0.4264161614766163\n","Iteration #4870  Loss: 0.49109442020975785; l2 norm of gradients: 0.03586432216288235; l2 norm of weights: 0.4264072892076666\n","Iteration #4871  Loss: 0.49108942126864025; l2 norm of gradients: 0.03582602628726839; l2 norm of weights: 0.4263984269607915\n","Iteration #4872  Loss: 0.49108442627543786; l2 norm of gradients: 0.035787772667603496; l2 norm of weights: 0.42638957471903427\n","Iteration #4873  Loss: 0.4910794352238731; l2 norm of gradients: 0.03574956125761302; l2 norm of weights: 0.4263807324654699\n","Iteration #4874  Loss: 0.4910744481076812; l2 norm of gradients: 0.03571139201107364; l2 norm of weights: 0.42637190018320603\n","Iteration #4875  Loss: 0.49106946492061065; l2 norm of gradients: 0.03567326488181321; l2 norm of weights: 0.42636307785538213\n","Iteration #4876  Loss: 0.49106448565642274; l2 norm of gradients: 0.03563517982371081; l2 norm of weights: 0.42635426546516986\n","Iteration #4877  Loss: 0.49105951030889144; l2 norm of gradients: 0.03559713679069656; l2 norm of weights: 0.4263454629957724\n","Iteration #4878  Loss: 0.491054538871804; l2 norm of gradients: 0.035559135736751665; l2 norm of weights: 0.4263366704304254\n","Iteration #4879  Loss: 0.4910495713389603; l2 norm of gradients: 0.035521176615908305; l2 norm of weights: 0.42632788775239594\n","Iteration #4880  Loss: 0.4910446077041731; l2 norm of gradients: 0.03548325938224962; l2 norm of weights: 0.4263191149449831\n","Iteration #4881  Loss: 0.49103964796126803; l2 norm of gradients: 0.03544538398990961; l2 norm of weights: 0.42631035199151746\n","Iteration #4882  Loss: 0.4910346921040834; l2 norm of gradients: 0.03540755039307307; l2 norm of weights: 0.4263015988753615\n","Iteration #4883  Loss: 0.49102974012647027; l2 norm of gradients: 0.03536975854597565; l2 norm of weights: 0.426292855579909\n","Iteration #4884  Loss: 0.49102479202229243; l2 norm of gradients: 0.03533200840290361; l2 norm of weights: 0.42628412208858546\n","Iteration #4885  Loss: 0.4910198477854265; l2 norm of gradients: 0.0352942999181939; l2 norm of weights: 0.42627539838484785\n","Iteration #4886  Loss: 0.4910149074097616; l2 norm of gradients: 0.03525663304623405; l2 norm of weights: 0.42626668445218435\n","Iteration #4887  Loss: 0.4910099708891995; l2 norm of gradients: 0.03521900774146219; l2 norm of weights: 0.42625798027411466\n","Iteration #4888  Loss: 0.4910050382176548; l2 norm of gradients: 0.03518142395836685; l2 norm of weights: 0.42624928583418975\n","Iteration #4889  Loss: 0.49100010938905425; l2 norm of gradients: 0.03514388165148703; l2 norm of weights: 0.4262406011159916\n","Iteration #4890  Loss: 0.4909951843973376; l2 norm of gradients: 0.03510638077541211; l2 norm of weights: 0.4262319261031336\n","Iteration #4891  Loss: 0.4909902632364568; l2 norm of gradients: 0.03506892128478176; l2 norm of weights: 0.4262232607792601\n","Iteration #4892  Loss: 0.49098534590037657; l2 norm of gradients: 0.03503150313428591; l2 norm of weights: 0.4262146051280465\n","Iteration #4893  Loss: 0.49098043238307376; l2 norm of gradients: 0.03499412627866466; l2 norm of weights: 0.4262059591331991\n","Iteration #4894  Loss: 0.49097552267853817; l2 norm of gradients: 0.03495679067270838; l2 norm of weights: 0.42619732277845535\n","Iteration #4895  Loss: 0.4909706167807715; l2 norm of gradients: 0.03491949627125736; l2 norm of weights: 0.4261886960475832\n","Iteration #4896  Loss: 0.4909657146837882; l2 norm of gradients: 0.034882243029202065; l2 norm of weights: 0.42618007892438164\n","Iteration #4897  Loss: 0.4909608163816148; l2 norm of gradients: 0.03484503090148284; l2 norm of weights: 0.42617147139268036\n","Iteration #4898  Loss: 0.4909559218682905; l2 norm of gradients: 0.03480785984309001; l2 norm of weights: 0.4261628734363397\n","Iteration #4899  Loss: 0.4909510311378665; l2 norm of gradients: 0.03477072980906373; l2 norm of weights: 0.4261542850392504\n","Iteration #4900  Loss: 0.4909461441844066; l2 norm of gradients: 0.034733640754494026; l2 norm of weights: 0.4261457061853341\n","Iteration #4901  Loss: 0.4909412610019863; l2 norm of gradients: 0.034696592634520615; l2 norm of weights: 0.4261371368585427\n","Iteration #4902  Loss: 0.4909363815846942; l2 norm of gradients: 0.03465958540433291; l2 norm of weights: 0.4261285770428587\n","Iteration #4903  Loss: 0.49093150592663026; l2 norm of gradients: 0.03462261901917003; l2 norm of weights: 0.4261200267222947\n","Iteration #4904  Loss: 0.4909266340219071; l2 norm of gradients: 0.03458569343432065; l2 norm of weights: 0.426111485880894\n","Iteration #4905  Loss: 0.4909217658646493; l2 norm of gradients: 0.03454880860512298; l2 norm of weights: 0.42610295450272956\n","Iteration #4906  Loss: 0.4909169014489937; l2 norm of gradients: 0.0345119644869647; l2 norm of weights: 0.4260944325719052\n","Iteration #4907  Loss: 0.49091204076908906; l2 norm of gradients: 0.034475161035282906; l2 norm of weights: 0.42608592007255447\n","Iteration #4908  Loss: 0.49090718381909637; l2 norm of gradients: 0.03443839820556412; l2 norm of weights: 0.426077416988841\n","Iteration #4909  Loss: 0.49090233059318866; l2 norm of gradients: 0.03440167595334412; l2 norm of weights: 0.42606892330495866\n","Iteration #4910  Loss: 0.4908974810855508; l2 norm of gradients: 0.03436499423420794; l2 norm of weights: 0.4260604390051311\n","Iteration #4911  Loss: 0.49089263529038; l2 norm of gradients: 0.034328353003789856; l2 norm of weights: 0.42605196407361184\n","Iteration #4912  Loss: 0.490887793201885; l2 norm of gradients: 0.03429175221777326; l2 norm of weights: 0.4260434984946844\n","Iteration #4913  Loss: 0.49088295481428673; l2 norm of gradients: 0.034255191831890665; l2 norm of weights: 0.42603504225266187\n","Iteration #4914  Loss: 0.49087812012181825; l2 norm of gradients: 0.03421867180192362; l2 norm of weights: 0.42602659533188725\n","Iteration #4915  Loss: 0.49087328911872385; l2 norm of gradients: 0.034182192083702614; l2 norm of weights: 0.42601815771673307\n","Iteration #4916  Loss: 0.4908684617992604; l2 norm of gradients: 0.03414575263310714; l2 norm of weights: 0.42600972939160164\n","Iteration #4917  Loss: 0.49086363815769635; l2 norm of gradients: 0.03410935340606547; l2 norm of weights: 0.4260013103409246\n","Iteration #4918  Loss: 0.49085881818831173; l2 norm of gradients: 0.03407299435855484; l2 norm of weights: 0.42599290054916317\n","Iteration #4919  Loss: 0.4908540018853985; l2 norm of gradients: 0.03403667544660108; l2 norm of weights: 0.42598450000080795\n","Iteration #4920  Loss: 0.4908491892432607; l2 norm of gradients: 0.03400039662627887; l2 norm of weights: 0.4259761086803793\n","Iteration #4921  Loss: 0.49084438025621363; l2 norm of gradients: 0.03396415785371147; l2 norm of weights: 0.42596772657242615\n","Iteration #4922  Loss: 0.49083957491858454; l2 norm of gradients: 0.03392795908507081; l2 norm of weights: 0.4259593536615273\n","Iteration #4923  Loss: 0.49083477322471225; l2 norm of gradients: 0.0338918002765773; l2 norm of weights: 0.4259509899322906\n","Iteration #4924  Loss: 0.4908299751689475; l2 norm of gradients: 0.03385568138449987; l2 norm of weights: 0.42594263536935295\n","Iteration #4925  Loss: 0.4908251807456523; l2 norm of gradients: 0.033819602365155896; l2 norm of weights: 0.4259342899573804\n","Iteration #4926  Loss: 0.4908203899492005; l2 norm of gradients: 0.033783563174911144; l2 norm of weights: 0.42592595368106806\n","Iteration #4927  Loss: 0.4908156027739775; l2 norm of gradients: 0.033747563770179734; l2 norm of weights: 0.42591762652514\n","Iteration #4928  Loss: 0.49081081921438025; l2 norm of gradients: 0.033711604107424; l2 norm of weights: 0.425909308474349\n","Iteration #4929  Loss: 0.49080603926481714; l2 norm of gradients: 0.03367568414315456; l2 norm of weights: 0.42590099951347715\n","Iteration #4930  Loss: 0.49080126291970816; l2 norm of gradients: 0.03363980383393021; l2 norm of weights: 0.4258926996273349\n","Iteration #4931  Loss: 0.4907964901734848; l2 norm of gradients: 0.03360396313635781; l2 norm of weights: 0.42588440880076167\n","Iteration #4932  Loss: 0.4907917210205899; l2 norm of gradients: 0.03356816200709233; l2 norm of weights: 0.4258761270186256\n","Iteration #4933  Loss: 0.490786955455478; l2 norm of gradients: 0.03353240040283674; l2 norm of weights: 0.4258678542658233\n","Iteration #4934  Loss: 0.49078219347261454; l2 norm of gradients: 0.033496678280341946; l2 norm of weights: 0.42585959052728\n","Iteration #4935  Loss: 0.49077743506647686; l2 norm of gradients: 0.03346099559640677; l2 norm of weights: 0.4258513357879497\n","Iteration #4936  Loss: 0.49077268023155335; l2 norm of gradients: 0.03342535230787791; l2 norm of weights: 0.42584309003281445\n","Iteration #4937  Loss: 0.4907679289623439; l2 norm of gradients: 0.03338974837164983; l2 norm of weights: 0.4258348532468851\n","Iteration #4938  Loss: 0.49076318125335955; l2 norm of gradients: 0.033354183744664756; l2 norm of weights: 0.4258266254152006\n","Iteration #4939  Loss: 0.49075843709912287; l2 norm of gradients: 0.0333186583839126; l2 norm of weights: 0.4258184065228284\n","Iteration #4940  Loss: 0.49075369649416745; l2 norm of gradients: 0.03328317224643085; l2 norm of weights: 0.42581019655486385\n","Iteration #4941  Loss: 0.49074895943303815; l2 norm of gradients: 0.03324772528930473; l2 norm of weights: 0.425801995496431\n","Iteration #4942  Loss: 0.49074422591029115; l2 norm of gradients: 0.03321231746966683; l2 norm of weights: 0.4257938033326815\n","Iteration #4943  Loss: 0.4907394959204937; l2 norm of gradients: 0.033176948744697315; l2 norm of weights: 0.42578562004879555\n","Iteration #4944  Loss: 0.4907347694582243; l2 norm of gradients: 0.03314161907162375; l2 norm of weights: 0.4257774456299811\n","Iteration #4945  Loss: 0.4907300465180725; l2 norm of gradients: 0.033106328407721056; l2 norm of weights: 0.42576928006147396\n","Iteration #4946  Loss: 0.4907253270946391; l2 norm of gradients: 0.0330710767103115; l2 norm of weights: 0.42576112332853827\n","Iteration #4947  Loss: 0.49072061118253585; l2 norm of gradients: 0.033035863936764616; l2 norm of weights: 0.4257529754164656\n","Iteration #4948  Loss: 0.4907158987763856; l2 norm of gradients: 0.033000690044497114; l2 norm of weights: 0.42574483631057564\n","Iteration #4949  Loss: 0.49071118987082235; l2 norm of gradients: 0.03296555499097292; l2 norm of weights: 0.4257367059962156\n","Iteration #4950  Loss: 0.49070648446049087; l2 norm of gradients: 0.03293045873370302; l2 norm of weights: 0.4257285844587605\n","Iteration #4951  Loss: 0.4907017825400472; l2 norm of gradients: 0.03289540123024547; l2 norm of weights: 0.42572047168361304\n","Iteration #4952  Loss: 0.4906970841041581; l2 norm of gradients: 0.03286038243820537; l2 norm of weights: 0.42571236765620335\n","Iteration #4953  Loss: 0.49069238914750163; l2 norm of gradients: 0.032825402315234696; l2 norm of weights: 0.4257042723619893\n","Iteration #4954  Loss: 0.4906876976647663; l2 norm of gradients: 0.03279046081903241; l2 norm of weights: 0.42569618578645607\n","Iteration #4955  Loss: 0.49068300965065176; l2 norm of gradients: 0.032755557907344225; l2 norm of weights: 0.42568810791511635\n","Iteration #4956  Loss: 0.49067832509986875; l2 norm of gradients: 0.03272069353796273; l2 norm of weights: 0.42568003873351024\n","Iteration #4957  Loss: 0.4906736440071385; l2 norm of gradients: 0.03268586766872724; l2 norm of weights: 0.425671978227205\n","Iteration #4958  Loss: 0.49066896636719315; l2 norm of gradients: 0.03265108025752372; l2 norm of weights: 0.4256639263817953\n","Iteration #4959  Loss: 0.4906642921747757; l2 norm of gradients: 0.0326163312622848; l2 norm of weights: 0.425655883182903\n","Iteration #4960  Loss: 0.4906596214246401; l2 norm of gradients: 0.03258162064098972; l2 norm of weights: 0.42564784861617705\n","Iteration #4961  Loss: 0.4906549541115507; l2 norm of gradients: 0.0325469483516642; l2 norm of weights: 0.42563982266729367\n","Iteration #4962  Loss: 0.49065029023028295; l2 norm of gradients: 0.03251231435238048; l2 norm of weights: 0.4256318053219559\n","Iteration #4963  Loss: 0.49064562977562265; l2 norm of gradients: 0.032477718601257234; l2 norm of weights: 0.4256237965658939\n","Iteration #4964  Loss: 0.4906409727423665; l2 norm of gradients: 0.032443161056459495; l2 norm of weights: 0.4256157963848648\n","Iteration #4965  Loss: 0.4906363191253219; l2 norm of gradients: 0.03240864167619862; l2 norm of weights: 0.4256078047646526\n","Iteration #4966  Loss: 0.4906316689193067; l2 norm of gradients: 0.03237416041873227; l2 norm of weights: 0.4255998216910681\n","Iteration #4967  Loss: 0.49062702211914955; l2 norm of gradients: 0.03233971724236431; l2 norm of weights: 0.42559184714994897\n","Iteration #4968  Loss: 0.49062237871968956; l2 norm of gradients: 0.03230531210544477; l2 norm of weights: 0.4255838811271595\n","Iteration #4969  Loss: 0.49061773871577663; l2 norm of gradients: 0.03227094496636982; l2 norm of weights: 0.4255759236085908\n","Iteration #4970  Loss: 0.4906131021022708; l2 norm of gradients: 0.032236615783581675; l2 norm of weights: 0.4255679745801605\n","Iteration #4971  Loss: 0.4906084688740432; l2 norm of gradients: 0.032202324515568606; l2 norm of weights: 0.42556003402781295\n","Iteration #4972  Loss: 0.4906038390259748; l2 norm of gradients: 0.032168071120864825; l2 norm of weights: 0.42555210193751875\n","Iteration #4973  Loss: 0.4905992125529576; l2 norm of gradients: 0.03213385555805043; l2 norm of weights: 0.42554417829527524\n","Iteration #4974  Loss: 0.4905945894498938; l2 norm of gradients: 0.03209967778575144; l2 norm of weights: 0.4255362630871061\n","Iteration #4975  Loss: 0.4905899697116962; l2 norm of gradients: 0.03206553776263965; l2 norm of weights: 0.4255283562990614\n","Iteration #4976  Loss: 0.4905853533332877; l2 norm of gradients: 0.03203143544743266; l2 norm of weights: 0.4255204579172175\n","Iteration #4977  Loss: 0.49058074030960186; l2 norm of gradients: 0.03199737079889372; l2 norm of weights: 0.425512567927677\n","Iteration #4978  Loss: 0.4905761306355824; l2 norm of gradients: 0.031963343775831786; l2 norm of weights: 0.42550468631656874\n","Iteration #4979  Loss: 0.4905715243061838; l2 norm of gradients: 0.03192935433710138; l2 norm of weights: 0.4254968130700477\n","Iteration #4980  Loss: 0.4905669213163703; l2 norm of gradients: 0.031895402441602626; l2 norm of weights: 0.4254889481742952\n","Iteration #4981  Loss: 0.49056232166111685; l2 norm of gradients: 0.03186148804828114; l2 norm of weights: 0.4254810916155182\n","Iteration #4982  Loss: 0.49055772533540853; l2 norm of gradients: 0.031827611116128; l2 norm of weights: 0.42547324337995\n","Iteration #4983  Loss: 0.49055313233424036; l2 norm of gradients: 0.031793771604179684; l2 norm of weights: 0.4254654034538498\n","Iteration #4984  Loss: 0.49054854265261844; l2 norm of gradients: 0.03175996947151802; l2 norm of weights: 0.42545757182350263\n","Iteration #4985  Loss: 0.490543956285558; l2 norm of gradients: 0.031726204677270144; l2 norm of weights: 0.42544974847521944\n","Iteration #4986  Loss: 0.4905393732280855; l2 norm of gradients: 0.03169247718060843; l2 norm of weights: 0.4254419333953369\n","Iteration #4987  Loss: 0.49053479347523665; l2 norm of gradients: 0.0316587869407505; l2 norm of weights: 0.4254341265702176\n","Iteration #4988  Loss: 0.49053021702205796; l2 norm of gradients: 0.0316251339169591; l2 norm of weights: 0.4254263279862498\n","Iteration #4989  Loss: 0.49052564386360553; l2 norm of gradients: 0.03159151806854208; l2 norm of weights: 0.4254185376298471\n","Iteration #4990  Loss: 0.4905210739949461; l2 norm of gradients: 0.03155793935485232; l2 norm of weights: 0.4254107554874492\n","Iteration #4991  Loss: 0.4905165074111563; l2 norm of gradients: 0.03152439773528778; l2 norm of weights: 0.42540298154552114\n","Iteration #4992  Loss: 0.4905119441073226; l2 norm of gradients: 0.03149089316929128; l2 norm of weights: 0.42539521579055334\n","Iteration #4993  Loss: 0.49050738407854155; l2 norm of gradients: 0.0314574256163506; l2 norm of weights: 0.42538745820906193\n","Iteration #4994  Loss: 0.4905028273199199; l2 norm of gradients: 0.03142399503599834; l2 norm of weights: 0.425379708787588\n","Iteration #4995  Loss: 0.4904982738265742; l2 norm of gradients: 0.03139060138781194; l2 norm of weights: 0.42537196751269857\n","Iteration #4996  Loss: 0.49049372359363125; l2 norm of gradients: 0.031357244631413546; l2 norm of weights: 0.4253642343709857\n","Iteration #4997  Loss: 0.4904891766162275; l2 norm of gradients: 0.031323924726470026; l2 norm of weights: 0.42535650934906666\n","Iteration #4998  Loss: 0.4904846328895093; l2 norm of gradients: 0.031290641632692934; l2 norm of weights: 0.42534879243358387\n","Iteration #4999  Loss: 0.4904800924086332; l2 norm of gradients: 0.03125739530983837; l2 norm of weights: 0.4253410836112051\n","Iteration #5000  Loss: 0.4904755551687654; l2 norm of gradients: 0.031224185717707045; l2 norm of weights: 0.4253333828686232\n","Iteration #5001  Loss: 0.49047102116508196; l2 norm of gradients: 0.031191012816144108; l2 norm of weights: 0.42532569019255595\n","Iteration #5002  Loss: 0.490466490392769; l2 norm of gradients: 0.031157876565039207; l2 norm of weights: 0.4253180055697462\n","Iteration #5003  Loss: 0.49046196284702215; l2 norm of gradients: 0.031124776924326376; l2 norm of weights: 0.42531032898696186\n","Iteration #5004  Loss: 0.4904574385230469; l2 norm of gradients: 0.031091713853984015; l2 norm of weights: 0.4253026604309955\n","Iteration #5005  Loss: 0.4904529174160588; l2 norm of gradients: 0.03105868731403485; l2 norm of weights: 0.4252949998886649\n","Iteration #5006  Loss: 0.49044839952128283; l2 norm of gradients: 0.0310256972645458; l2 norm of weights: 0.4252873473468124\n","Iteration #5007  Loss: 0.4904438848339539; l2 norm of gradients: 0.030992743665628063; l2 norm of weights: 0.42527970279230526\n","Iteration #5008  Loss: 0.4904393733493163; l2 norm of gradients: 0.030959826477436916; l2 norm of weights: 0.4252720662120352\n","Iteration #5009  Loss: 0.4904348650626246; l2 norm of gradients: 0.030926945660171828; l2 norm of weights: 0.42526443759291904\n","Iteration #5010  Loss: 0.49043035996914247; l2 norm of gradients: 0.030894101174076267; l2 norm of weights: 0.4252568169218978\n","Iteration #5011  Loss: 0.4904258580641434; l2 norm of gradients: 0.03086129297943773; l2 norm of weights: 0.42524920418593737\n","Iteration #5012  Loss: 0.49042135934291065; l2 norm of gradients: 0.030828521036587675; l2 norm of weights: 0.42524159937202793\n","Iteration #5013  Loss: 0.4904168638007371; l2 norm of gradients: 0.030795785305901463; l2 norm of weights: 0.42523400246718435\n","Iteration #5014  Loss: 0.49041237143292493; l2 norm of gradients: 0.030763085747798333; l2 norm of weights: 0.42522641345844575\n","Iteration #5015  Loss: 0.4904078822347861; l2 norm of gradients: 0.030730422322741337; l2 norm of weights: 0.4252188323328758\n","Iteration #5016  Loss: 0.49040339620164203; l2 norm of gradients: 0.030697794991237252; l2 norm of weights: 0.4252112590775623\n","Iteration #5017  Loss: 0.49039891332882385; l2 norm of gradients: 0.030665203713836648; l2 norm of weights: 0.4252036936796176\n","Iteration #5018  Loss: 0.4903944336116719; l2 norm of gradients: 0.03063264845113369; l2 norm of weights: 0.425196136126178\n","Iteration #5019  Loss: 0.4903899570455362; l2 norm of gradients: 0.030600129163766216; l2 norm of weights: 0.42518858640440427\n","Iteration #5020  Loss: 0.49038548362577605; l2 norm of gradients: 0.030567645812415556; l2 norm of weights: 0.425181044501481\n","Iteration #5021  Loss: 0.4903810133477606; l2 norm of gradients: 0.030535198357806652; l2 norm of weights: 0.4251735104046173\n","Iteration #5022  Loss: 0.49037654620686777; l2 norm of gradients: 0.030502786760707872; l2 norm of weights: 0.42516598410104595\n","Iteration #5023  Loss: 0.4903720821984856; l2 norm of gradients: 0.03047041098193099; l2 norm of weights: 0.4251584655780239\n","Iteration #5024  Loss: 0.4903676213180108; l2 norm of gradients: 0.030438070982331224; l2 norm of weights: 0.425150954822832\n","Iteration #5025  Loss: 0.4903631635608501; l2 norm of gradients: 0.030405766722807048; l2 norm of weights: 0.4251434518227751\n","Iteration #5026  Loss: 0.49035870892241906; l2 norm of gradients: 0.030373498164300253; l2 norm of weights: 0.4251359565651818\n","Iteration #5027  Loss: 0.4903542573981427; l2 norm of gradients: 0.030341265267795837; l2 norm of weights: 0.42512846903740453\n","Iteration #5028  Loss: 0.49034980898345554; l2 norm of gradients: 0.030309067994321995; l2 norm of weights: 0.4251209892268197\n","Iteration #5029  Loss: 0.4903453636738012; l2 norm of gradients: 0.030276906304950096; l2 norm of weights: 0.42511351712082696\n","Iteration #5030  Loss: 0.49034092146463265; l2 norm of gradients: 0.03024478016079452; l2 norm of weights: 0.42510605270685026\n","Iteration #5031  Loss: 0.49033648235141175; l2 norm of gradients: 0.03021268952301274; l2 norm of weights: 0.4250985959723366\n","Iteration #5032  Loss: 0.49033204632961; l2 norm of gradients: 0.030180634352805182; l2 norm of weights: 0.4250911469047571\n","Iteration #5033  Loss: 0.4903276133947082; l2 norm of gradients: 0.030148614611415245; l2 norm of weights: 0.42508370549160585\n","Iteration #5034  Loss: 0.4903231835421956; l2 norm of gradients: 0.030116630260129205; l2 norm of weights: 0.4250762717204008\n","Iteration #5035  Loss: 0.49031875676757164; l2 norm of gradients: 0.0300846812602762; l2 norm of weights: 0.4250688455786833\n","Iteration #5036  Loss: 0.4903143330663437; l2 norm of gradients: 0.03005276757322815; l2 norm of weights: 0.42506142705401795\n","Iteration #5037  Loss: 0.4903099124340295; l2 norm of gradients: 0.030020889160399754; l2 norm of weights: 0.42505401613399313\n","Iteration #5038  Loss: 0.4903054948661549; l2 norm of gradients: 0.02998904598324836; l2 norm of weights: 0.4250466128062198\n","Iteration #5039  Loss: 0.49030108035825537; l2 norm of gradients: 0.029957238003274045; l2 norm of weights: 0.42503921705833275\n","Iteration #5040  Loss: 0.4902966689058751; l2 norm of gradients: 0.029925465182019433; l2 norm of weights: 0.42503182887798985\n","Iteration #5041  Loss: 0.4902922605045679; l2 norm of gradients: 0.02989372748106976; l2 norm of weights: 0.42502444825287217\n","Iteration #5042  Loss: 0.4902878551498957; l2 norm of gradients: 0.02986202486205269; l2 norm of weights: 0.4250170751706837\n","Iteration #5043  Loss: 0.49028345283743036; l2 norm of gradients: 0.02983035728663848; l2 norm of weights: 0.4250097096191517\n","Iteration #5044  Loss: 0.4902790535627519; l2 norm of gradients: 0.029798724716539677; l2 norm of weights: 0.42500235158602645\n","Iteration #5045  Loss: 0.4902746573214499; l2 norm of gradients: 0.0297671271135113; l2 norm of weights: 0.42499500105908117\n","Iteration #5046  Loss: 0.49027026410912267; l2 norm of gradients: 0.029735564439350638; l2 norm of weights: 0.42498765802611205\n","Iteration #5047  Loss: 0.4902658739213773; l2 norm of gradients: 0.029704036655897263; l2 norm of weights: 0.4249803224749381\n","Iteration #5048  Loss: 0.49026148675383; l2 norm of gradients: 0.029672543725032995; l2 norm of weights: 0.4249729943934013\n","Iteration #5049  Loss: 0.4902571026021059; l2 norm of gradients: 0.02964108560868183; l2 norm of weights: 0.42496567376936645\n","Iteration #5050  Loss: 0.4902527214618386; l2 norm of gradients: 0.029609662268809893; l2 norm of weights: 0.42495836059072106\n","Iteration #5051  Loss: 0.49024834332867107; l2 norm of gradients: 0.029578273667425374; l2 norm of weights: 0.42495105484537515\n","Iteration #5052  Loss: 0.49024396819825466; l2 norm of gradients: 0.02954691976657857; l2 norm of weights: 0.42494375652126193\n","Iteration #5053  Loss: 0.4902395960662499; l2 norm of gradients: 0.029515600528361705; l2 norm of weights: 0.42493646560633663\n","Iteration #5054  Loss: 0.49023522692832566; l2 norm of gradients: 0.02948431591490898; l2 norm of weights: 0.4249291820885775\n","Iteration #5055  Loss: 0.4902308607801599; l2 norm of gradients: 0.029453065888396524; l2 norm of weights: 0.4249219059559852\n","Iteration #5056  Loss: 0.4902264976174396; l2 norm of gradients: 0.029421850411042262; l2 norm of weights: 0.4249146371965831\n","Iteration #5057  Loss: 0.4902221374358597; l2 norm of gradients: 0.02939066944510599; l2 norm of weights: 0.4249073757984164\n","Iteration #5058  Loss: 0.4902177802311246; l2 norm of gradients: 0.029359522952889208; l2 norm of weights: 0.4249001217495535\n","Iteration #5059  Loss: 0.4902134259989468; l2 norm of gradients: 0.029328410896735168; l2 norm of weights: 0.4248928750380845\n","Iteration #5060  Loss: 0.4902090747350479; l2 norm of gradients: 0.02929733323902878; l2 norm of weights: 0.4248856356521222\n","Iteration #5061  Loss: 0.49020472643515794; l2 norm of gradients: 0.029266289942196565; l2 norm of weights: 0.42487840357980167\n","Iteration #5062  Loss: 0.49020038109501585; l2 norm of gradients: 0.029235280968706617; l2 norm of weights: 0.4248711788092801\n","Iteration #5063  Loss: 0.49019603871036876; l2 norm of gradients: 0.029204306281068595; l2 norm of weights: 0.4248639613287368\n","Iteration #5064  Loss: 0.49019169927697265; l2 norm of gradients: 0.029173365841833613; l2 norm of weights: 0.42485675112637356\n","Iteration #5065  Loss: 0.49018736279059216; l2 norm of gradients: 0.029142459613594216; l2 norm of weights: 0.4248495481904138\n","Iteration #5066  Loss: 0.4901830292470003; l2 norm of gradients: 0.029111587558984336; l2 norm of weights: 0.4248423525091035\n","Iteration #5067  Loss: 0.4901786986419787; l2 norm of gradients: 0.02908074964067927; l2 norm of weights: 0.42483516407071026\n","Iteration #5068  Loss: 0.4901743709713175; l2 norm of gradients: 0.02904994582139562; l2 norm of weights: 0.42482798286352375\n","Iteration #5069  Loss: 0.4901700462308154; l2 norm of gradients: 0.029019176063891212; l2 norm of weights: 0.42482080887585566\n","Iteration #5070  Loss: 0.49016572441627954; l2 norm of gradients: 0.0289884403309651; l2 norm of weights: 0.4248136420960396\n","Iteration #5071  Loss: 0.49016140552352555; l2 norm of gradients: 0.028957738585457454; l2 norm of weights: 0.424806482512431\n","Iteration #5072  Loss: 0.4901570895483774; l2 norm of gradients: 0.028927070790249608; l2 norm of weights: 0.4247993301134069\n","Iteration #5073  Loss: 0.49015277648666766; l2 norm of gradients: 0.028896436908263944; l2 norm of weights: 0.42479218488736653\n","Iteration #5074  Loss: 0.4901484663342372; l2 norm of gradients: 0.028865836902463894; l2 norm of weights: 0.4247850468227303\n","Iteration #5075  Loss: 0.49014415908693537; l2 norm of gradients: 0.028835270735853808; l2 norm of weights: 0.4247779159079406\n","Iteration #5076  Loss: 0.4901398547406198; l2 norm of gradients: 0.028804738371479007; l2 norm of weights: 0.4247707921314615\n","Iteration #5077  Loss: 0.4901355532911566; l2 norm of gradients: 0.02877423977242569; l2 norm of weights: 0.4247636754817785\n","Iteration #5078  Loss: 0.49013125473441993; l2 norm of gradients: 0.028743774901820906; l2 norm of weights: 0.42475656594739875\n","Iteration #5079  Loss: 0.4901269590662928; l2 norm of gradients: 0.028713343722832477; l2 norm of weights: 0.42474946351685094\n","Iteration #5080  Loss: 0.4901226662826659; l2 norm of gradients: 0.02868294619866901; l2 norm of weights: 0.42474236817868505\n","Iteration #5081  Loss: 0.4901183763794388; l2 norm of gradients: 0.028652582292579745; l2 norm of weights: 0.4247352799214727\n","Iteration #5082  Loss: 0.49011408935251877; l2 norm of gradients: 0.02862225196785466; l2 norm of weights: 0.4247281987338067\n","Iteration #5083  Loss: 0.4901098051978219; l2 norm of gradients: 0.028591955187824276; l2 norm of weights: 0.42472112460430134\n","Iteration #5084  Loss: 0.49010552391127205; l2 norm of gradients: 0.028561691915859753; l2 norm of weights: 0.42471405752159214\n","Iteration #5085  Loss: 0.49010124548880146; l2 norm of gradients: 0.02853146211537272; l2 norm of weights: 0.4247069974743358\n","Iteration #5086  Loss: 0.4900969699263507; l2 norm of gradients: 0.028501265749815284; l2 norm of weights: 0.42469994445121056\n","Iteration #5087  Loss: 0.49009269721986826; l2 norm of gradients: 0.028471102782680013; l2 norm of weights: 0.4246928984409153\n","Iteration #5088  Loss: 0.490088427365311; l2 norm of gradients: 0.028440973177499836; l2 norm of weights: 0.4246858594321705\n","Iteration #5089  Loss: 0.4900841603586439; l2 norm of gradients: 0.02841087689784804; l2 norm of weights: 0.42467882741371754\n","Iteration #5090  Loss: 0.4900798961958399; l2 norm of gradients: 0.028380813907338185; l2 norm of weights: 0.42467180237431884\n","Iteration #5091  Loss: 0.4900756348728803; l2 norm of gradients: 0.02835078416962412; l2 norm of weights: 0.4246647843027577\n","Iteration #5092  Loss: 0.49007137638575404; l2 norm of gradients: 0.028320787648399844; l2 norm of weights: 0.4246577731878387\n","Iteration #5093  Loss: 0.4900671207304589; l2 norm of gradients: 0.02829082430739959; l2 norm of weights: 0.4246507690183872\n","Iteration #5094  Loss: 0.490062867903; l2 norm of gradients: 0.028260894110397648; l2 norm of weights: 0.42464377178324936\n","Iteration #5095  Loss: 0.49005861789939087; l2 norm of gradients: 0.028230997021208383; l2 norm of weights: 0.4246367814712922\n","Iteration #5096  Loss: 0.4900543707156527; l2 norm of gradients: 0.028201133003686263; l2 norm of weights: 0.4246297980714036\n","Iteration #5097  Loss: 0.49005012634781525; l2 norm of gradients: 0.028171302021725637; l2 norm of weights: 0.4246228215724922\n","Iteration #5098  Loss: 0.49004588479191574; l2 norm of gradients: 0.02814150403926086; l2 norm of weights: 0.42461585196348733\n","Iteration #5099  Loss: 0.49004164604399986; l2 norm of gradients: 0.028111739020266143; l2 norm of weights: 0.42460888923333906\n","Iteration #5100  Loss: 0.4900374101001205; l2 norm of gradients: 0.028082006928755544; l2 norm of weights: 0.42460193337101804\n","Iteration #5101  Loss: 0.4900331769563392; l2 norm of gradients: 0.02805230772878299; l2 norm of weights: 0.4245949843655154\n","Iteration #5102  Loss: 0.49002894660872526; l2 norm of gradients: 0.02802264138444209; l2 norm of weights: 0.4245880422058431\n","Iteration #5103  Loss: 0.4900247190533556; l2 norm of gradients: 0.027993007859866206; l2 norm of weights: 0.4245811068810335\n","Iteration #5104  Loss: 0.4900204942863152; l2 norm of gradients: 0.027963407119228373; l2 norm of weights: 0.4245741783801394\n","Iteration #5105  Loss: 0.4900162723036971; l2 norm of gradients: 0.02793383912674122; l2 norm of weights: 0.42456725669223394\n","Iteration #5106  Loss: 0.4900120531016019; l2 norm of gradients: 0.027904303846657007; l2 norm of weights: 0.4245603418064109\n","Iteration #5107  Loss: 0.49000783667613795; l2 norm of gradients: 0.02787480124326752; l2 norm of weights: 0.42455343371178417\n","Iteration #5108  Loss: 0.4900036230234219; l2 norm of gradients: 0.02784533128090399; l2 norm of weights: 0.4245465323974882\n","Iteration #5109  Loss: 0.48999941213957765; l2 norm of gradients: 0.027815893923937192; l2 norm of weights: 0.42453963785267756\n","Iteration #5110  Loss: 0.48999520402073726; l2 norm of gradients: 0.027786489136777216; l2 norm of weights: 0.424532750066527\n","Iteration #5111  Loss: 0.4899909986630403; l2 norm of gradients: 0.02775711688387356; l2 norm of weights: 0.42452586902823175\n","Iteration #5112  Loss: 0.48998679606263423; l2 norm of gradients: 0.027727777129715074; l2 norm of weights: 0.4245189947270069\n","Iteration #5113  Loss: 0.4899825962156743; l2 norm of gradients: 0.0276984698388298; l2 norm of weights: 0.42451212715208775\n","Iteration #5114  Loss: 0.4899783991183233; l2 norm of gradients: 0.027669194975785072; l2 norm of weights: 0.42450526629272983\n","Iteration #5115  Loss: 0.48997420476675185; l2 norm of gradients: 0.027639952505187405; l2 norm of weights: 0.4244984121382084\n","Iteration #5116  Loss: 0.48997001315713823; l2 norm of gradients: 0.027610742391682434; l2 norm of weights: 0.42449156467781907\n","Iteration #5117  Loss: 0.48996582428566815; l2 norm of gradients: 0.027581564599954894; l2 norm of weights: 0.4244847239008771\n","Iteration #5118  Loss: 0.48996163814853544; l2 norm of gradients: 0.027552419094728643; l2 norm of weights: 0.4244778897967181\n","Iteration #5119  Loss: 0.4899574547419412; l2 norm of gradients: 0.02752330584076646; l2 norm of weights: 0.424471062354697\n","Iteration #5120  Loss: 0.4899532740620941; l2 norm of gradients: 0.02749422480287013; l2 norm of weights: 0.424464241564189\n","Iteration #5121  Loss: 0.4899490961052107; l2 norm of gradients: 0.02746517594588039; l2 norm of weights: 0.42445742741458903\n","Iteration #5122  Loss: 0.4899449208675149; l2 norm of gradients: 0.027436159234676828; l2 norm of weights: 0.42445061989531163\n","Iteration #5123  Loss: 0.48994074834523843; l2 norm of gradients: 0.02740717463417788; l2 norm of weights: 0.42444381899579126\n","Iteration #5124  Loss: 0.48993657853462014; l2 norm of gradients: 0.02737822210934077; l2 norm of weights: 0.4244370247054819\n","Iteration #5125  Loss: 0.48993241143190674; l2 norm of gradients: 0.0273493016251615; l2 norm of weights: 0.4244302370138574\n","Iteration #5126  Loss: 0.48992824703335236; l2 norm of gradients: 0.027320413146674754; l2 norm of weights: 0.4244234559104109\n","Iteration #5127  Loss: 0.48992408533521875; l2 norm of gradients: 0.02729155663895391; l2 norm of weights: 0.4244166813846555\n","Iteration #5128  Loss: 0.4899199263337749; l2 norm of gradients: 0.0272627320671109; l2 norm of weights: 0.4244099134261235\n","Iteration #5129  Loss: 0.4899157700252974; l2 norm of gradients: 0.027233939396296352; l2 norm of weights: 0.42440315202436685\n","Iteration #5130  Loss: 0.4899116164060705; l2 norm of gradients: 0.02720517859169933; l2 norm of weights: 0.4243963971689573\n","Iteration #5131  Loss: 0.4899074654723854; l2 norm of gradients: 0.02717644961854742; l2 norm of weights: 0.4243896488494851\n","Iteration #5132  Loss: 0.4899033172205411; l2 norm of gradients: 0.0271477524421067; l2 norm of weights: 0.4243829070555609\n","Iteration #5133  Loss: 0.489899171646844; l2 norm of gradients: 0.0271190870276816; l2 norm of weights: 0.4243761717768142\n","Iteration #5134  Loss: 0.4898950287476077; l2 norm of gradients: 0.02709045334061496; l2 norm of weights: 0.4243694430028937\n","Iteration #5135  Loss: 0.48989088851915313; l2 norm of gradients: 0.027061851346287892; l2 norm of weights: 0.4243627207234677\n","Iteration #5136  Loss: 0.4898867509578089; l2 norm of gradients: 0.02703328101011984; l2 norm of weights: 0.42435600492822356\n","Iteration #5137  Loss: 0.4898826160599106; l2 norm of gradients: 0.027004742297568458; l2 norm of weights: 0.42434929560686785\n","Iteration #5138  Loss: 0.4898784838218014; l2 norm of gradients: 0.026976235174129583; l2 norm of weights: 0.4243425927491262\n","Iteration #5139  Loss: 0.4898743542398316; l2 norm of gradients: 0.026947759605337237; l2 norm of weights: 0.42433589634474367\n","Iteration #5140  Loss: 0.489870227310359; l2 norm of gradients: 0.026919315556763527; l2 norm of weights: 0.424329206383484\n","Iteration #5141  Loss: 0.4898661030297484; l2 norm of gradients: 0.02689090299401863; l2 norm of weights: 0.4243225228551303\n","Iteration #5142  Loss: 0.489861981394372; l2 norm of gradients: 0.026862521882750755; l2 norm of weights: 0.4243158457494845\n","Iteration #5143  Loss: 0.4898578624006092; l2 norm of gradients: 0.02683417218864607; l2 norm of weights: 0.42430917505636756\n","Iteration #5144  Loss: 0.4898537460448468; l2 norm of gradients: 0.026805853877428728; l2 norm of weights: 0.4243025107656195\n","Iteration #5145  Loss: 0.48984963232347867; l2 norm of gradients: 0.026777566914860735; l2 norm of weights: 0.424295852867099\n","Iteration #5146  Loss: 0.4898455212329058; l2 norm of gradients: 0.026749311266741963; l2 norm of weights: 0.42428920135068376\n","Iteration #5147  Loss: 0.4898414127695365; l2 norm of gradients: 0.026721086898910097; l2 norm of weights: 0.4242825562062703\n","Iteration #5148  Loss: 0.48983730692978616; l2 norm of gradients: 0.026692893777240635; l2 norm of weights: 0.42427591742377385\n","Iteration #5149  Loss: 0.4898332037100773; l2 norm of gradients: 0.026664731867646712; l2 norm of weights: 0.4242692849931284\n","Iteration #5150  Loss: 0.4898291031068399; l2 norm of gradients: 0.026636601136079234; l2 norm of weights: 0.42426265890428677\n","Iteration #5151  Loss: 0.4898250051165105; l2 norm of gradients: 0.026608501548526692; l2 norm of weights: 0.4242560391472203\n","Iteration #5152  Loss: 0.48982090973553316; l2 norm of gradients: 0.026580433071015253; l2 norm of weights: 0.4242494257119192\n","Iteration #5153  Loss: 0.48981681696035884; l2 norm of gradients: 0.026552395669608538; l2 norm of weights: 0.42424281858839197\n","Iteration #5154  Loss: 0.4898127267874456; l2 norm of gradients: 0.026524389310407792; l2 norm of weights: 0.42423621776666587\n","Iteration #5155  Loss: 0.4898086392132587; l2 norm of gradients: 0.02649641395955165; l2 norm of weights: 0.42422962323678676\n","Iteration #5156  Loss: 0.4898045542342703; l2 norm of gradients: 0.026468469583216254; l2 norm of weights: 0.42422303498881875\n","Iteration #5157  Loss: 0.4898004718469595; l2 norm of gradients: 0.026440556147615087; l2 norm of weights: 0.42421645301284466\n","Iteration #5158  Loss: 0.4897963920478128; l2 norm of gradients: 0.02641267361899901; l2 norm of weights: 0.42420987729896553\n","Iteration #5159  Loss: 0.4897923148333231; l2 norm of gradients: 0.026384821963656175; l2 norm of weights: 0.424203307837301\n","Iteration #5160  Loss: 0.4897882401999908; l2 norm of gradients: 0.026357001147912024; l2 norm of weights: 0.42419674461798895\n","Iteration #5161  Loss: 0.4897841681443231; l2 norm of gradients: 0.026329211138129204; l2 norm of weights: 0.4241901876311855\n","Iteration #5162  Loss: 0.4897800986628339; l2 norm of gradients: 0.026301451900707554; l2 norm of weights: 0.4241836368670651\n","Iteration #5163  Loss: 0.4897760317520445; l2 norm of gradients: 0.026273723402084063; l2 norm of weights: 0.4241770923158205\n","Iteration #5164  Loss: 0.4897719674084828; l2 norm of gradients: 0.026246025608732803; l2 norm of weights: 0.4241705539676627\n","Iteration #5165  Loss: 0.4897679056286838; l2 norm of gradients: 0.026218358487164924; l2 norm of weights: 0.4241640218128206\n","Iteration #5166  Loss: 0.48976384640918913; l2 norm of gradients: 0.026190722003928614; l2 norm of weights: 0.4241574958415417\n","Iteration #5167  Loss: 0.4897597897465475; l2 norm of gradients: 0.026163116125608968; l2 norm of weights: 0.4241509760440912\n","Iteration #5168  Loss: 0.4897557356373144; l2 norm of gradients: 0.02613554081882811; l2 norm of weights: 0.42414446241075254\n","Iteration #5169  Loss: 0.4897516840780522; l2 norm of gradients: 0.026107996050244966; l2 norm of weights: 0.42413795493182704\n","Iteration #5170  Loss: 0.4897476350653302; l2 norm of gradients: 0.026080481786555414; l2 norm of weights: 0.4241314535976342\n","Iteration #5171  Loss: 0.4897435885957242; l2 norm of gradients: 0.026052997994492035; l2 norm of weights: 0.42412495839851144\n","Iteration #5172  Loss: 0.48973954466581726; l2 norm of gradients: 0.026025544640824273; l2 norm of weights: 0.42411846932481406\n","Iteration #5173  Loss: 0.48973550327219895; l2 norm of gradients: 0.025998121692358288; l2 norm of weights: 0.4241119863669152\n","Iteration #5174  Loss: 0.48973146441146553; l2 norm of gradients: 0.02597072911593686; l2 norm of weights: 0.42410550951520604\n","Iteration #5175  Loss: 0.4897274280802202; l2 norm of gradients: 0.025943366878439515; l2 norm of weights: 0.42409903876009536\n","Iteration #5176  Loss: 0.48972339427507283; l2 norm of gradients: 0.025916034946782303; l2 norm of weights: 0.4240925740920099\n","Iteration #5177  Loss: 0.48971936299263996; l2 norm of gradients: 0.02588873328791791; l2 norm of weights: 0.42408611550139397\n","Iteration #5178  Loss: 0.4897153342295451; l2 norm of gradients: 0.02586146186883549; l2 norm of weights: 0.4240796629787098\n","Iteration #5179  Loss: 0.489711307982418; l2 norm of gradients: 0.025834220656560714; l2 norm of weights: 0.42407321651443713\n","Iteration #5180  Loss: 0.4897072842478958; l2 norm of gradients: 0.02580700961815571; l2 norm of weights: 0.4240667760990735\n","Iteration #5181  Loss: 0.4897032630226214; l2 norm of gradients: 0.025779828720718986; l2 norm of weights: 0.42406034172313395\n","Iteration #5182  Loss: 0.48969924430324513; l2 norm of gradients: 0.02575267793138539; l2 norm of weights: 0.42405391337715115\n","Iteration #5183  Loss: 0.48969522808642374; l2 norm of gradients: 0.025725557217326153; l2 norm of weights: 0.4240474910516752\n","Iteration #5184  Loss: 0.4896912143688201; l2 norm of gradients: 0.02569846654574876; l2 norm of weights: 0.424041074737274\n","Iteration #5185  Loss: 0.4896872031471046; l2 norm of gradients: 0.02567140588389693; l2 norm of weights: 0.42403466442453247\n","Iteration #5186  Loss: 0.48968319441795366; l2 norm of gradients: 0.025644375199050547; l2 norm of weights: 0.4240282601040535\n","Iteration #5187  Loss: 0.48967918817805023; l2 norm of gradients: 0.025617374458525747; l2 norm of weights: 0.4240218617664569\n","Iteration #5188  Loss: 0.4896751844240842; l2 norm of gradients: 0.02559040362967473; l2 norm of weights: 0.4240154694023802\n","Iteration #5189  Loss: 0.48967118315275154; l2 norm of gradients: 0.025563462679885778; l2 norm of weights: 0.42400908300247814\n","Iteration #5190  Loss: 0.4896671843607553; l2 norm of gradients: 0.025536551576583202; l2 norm of weights: 0.4240027025574226\n","Iteration #5191  Loss: 0.4896631880448046; l2 norm of gradients: 0.025509670287227355; l2 norm of weights: 0.42399632805790316\n","Iteration #5192  Loss: 0.4896591942016153; l2 norm of gradients: 0.025482818779314482; l2 norm of weights: 0.42398995949462615\n","Iteration #5193  Loss: 0.48965520282790986; l2 norm of gradients: 0.025455997020376823; l2 norm of weights: 0.4239835968583155\n","Iteration #5194  Loss: 0.48965121392041694; l2 norm of gradients: 0.025429204977982456; l2 norm of weights: 0.42397724013971194\n","Iteration #5195  Loss: 0.4896472274758718; l2 norm of gradients: 0.0254024426197353; l2 norm of weights: 0.4239708893295738\n","Iteration #5196  Loss: 0.4896432434910162; l2 norm of gradients: 0.02537570991327508; l2 norm of weights: 0.4239645444186759\n","Iteration #5197  Loss: 0.4896392619625984; l2 norm of gradients: 0.025349006826277276; l2 norm of weights: 0.42395820539781076\n","Iteration #5198  Loss: 0.489635282887373; l2 norm of gradients: 0.025322333326453058; l2 norm of weights: 0.42395187225778763\n","Iteration #5199  Loss: 0.489631306262101; l2 norm of gradients: 0.02529568938154939; l2 norm of weights: 0.4239455449894327\n","Iteration #5200  Loss: 0.4896273320835498; l2 norm of gradients: 0.025269074959348697; l2 norm of weights: 0.4239392235835892\n","Iteration #5201  Loss: 0.48962336034849324; l2 norm of gradients: 0.025242490027669155; l2 norm of weights: 0.42393290803111744\n","Iteration #5202  Loss: 0.48961939105371155; l2 norm of gradients: 0.02521593455436442; l2 norm of weights: 0.42392659832289437\n","Iteration #5203  Loss: 0.48961542419599124; l2 norm of gradients: 0.0251894085073237; l2 norm of weights: 0.4239202944498141\n","Iteration #5204  Loss: 0.48961145977212517; l2 norm of gradients: 0.025162911854471687; l2 norm of weights: 0.42391399640278726\n","Iteration #5205  Loss: 0.4896074977789127; l2 norm of gradients: 0.025136444563768495; l2 norm of weights: 0.4239077041727416\n","Iteration #5206  Loss: 0.4896035382131594; l2 norm of gradients: 0.025110006603209646; l2 norm of weights: 0.4239014177506214\n","Iteration #5207  Loss: 0.4895995810716769; l2 norm of gradients: 0.025083597940826038; l2 norm of weights: 0.42389513712738774\n","Iteration #5208  Loss: 0.4895956263512836; l2 norm of gradients: 0.025057218544683846; l2 norm of weights: 0.4238888622940185\n","Iteration #5209  Loss: 0.48959167404880394; l2 norm of gradients: 0.025030868382884586; l2 norm of weights: 0.4238825932415083\n","Iteration #5210  Loss: 0.4895877241610685; l2 norm of gradients: 0.02500454742356501; l2 norm of weights: 0.42387632996086816\n","Iteration #5211  Loss: 0.48958377668491415; l2 norm of gradients: 0.024978255634897004; l2 norm of weights: 0.4238700724431257\n","Iteration #5212  Loss: 0.4895798316171843; l2 norm of gradients: 0.024951992985087743; l2 norm of weights: 0.4238638206793254\n","Iteration #5213  Loss: 0.4895758889547282; l2 norm of gradients: 0.024925759442379396; l2 norm of weights: 0.4238575746605282\n","Iteration #5214  Loss: 0.48957194869440157; l2 norm of gradients: 0.024899554975049303; l2 norm of weights: 0.4238513343778113\n","Iteration #5215  Loss: 0.4895680108330662; l2 norm of gradients: 0.024873379551409825; l2 norm of weights: 0.42384509982226864\n","Iteration #5216  Loss: 0.48956407536759017; l2 norm of gradients: 0.02484723313980835; l2 norm of weights: 0.4238388709850105\n","Iteration #5217  Loss: 0.48956014229484746; l2 norm of gradients: 0.0248211157086272; l2 norm of weights: 0.42383264785716357\n","Iteration #5218  Loss: 0.48955621161171875; l2 norm of gradients: 0.02479502722628367; l2 norm of weights: 0.4238264304298711\n","Iteration #5219  Loss: 0.4895522833150902; l2 norm of gradients: 0.024768967661229897; l2 norm of weights: 0.4238202186942925\n","Iteration #5220  Loss: 0.48954835740185454; l2 norm of gradients: 0.02474293698195288; l2 norm of weights: 0.42381401264160334\n","Iteration #5221  Loss: 0.48954443386891056; l2 norm of gradients: 0.02471693515697452; l2 norm of weights: 0.4238078122629959\n","Iteration #5222  Loss: 0.4895405127131631; l2 norm of gradients: 0.024690962154851354; l2 norm of weights: 0.42380161754967854\n","Iteration #5223  Loss: 0.48953659393152305; l2 norm of gradients: 0.024665017944174736; l2 norm of weights: 0.42379542849287555\n","Iteration #5224  Loss: 0.48953267752090746; l2 norm of gradients: 0.02463910249357072; l2 norm of weights: 0.423789245083828\n","Iteration #5225  Loss: 0.48952876347823926; l2 norm of gradients: 0.02461321577169997; l2 norm of weights: 0.42378306731379245\n","Iteration #5226  Loss: 0.4895248518004478; l2 norm of gradients: 0.024587357747257815; l2 norm of weights: 0.42377689517404216\n","Iteration #5227  Loss: 0.4895209424844681; l2 norm of gradients: 0.02456152838897416; l2 norm of weights: 0.4237707286558661\n","Iteration #5228  Loss: 0.48951703552724146; l2 norm of gradients: 0.02453572766561342; l2 norm of weights: 0.42376456775056964\n","Iteration #5229  Loss: 0.489513130925715; l2 norm of gradients: 0.024509955545974533; l2 norm of weights: 0.4237584124494739\n","Iteration #5230  Loss: 0.4895092286768418; l2 norm of gradients: 0.02448421199889092; l2 norm of weights: 0.423752262743916\n","Iteration #5231  Loss: 0.4895053287775814; l2 norm of gradients: 0.024458496993230407; l2 norm of weights: 0.42374611862524936\n","Iteration #5232  Loss: 0.4895014312248987; l2 norm of gradients: 0.02443281049789519; l2 norm of weights: 0.42373998008484304\n","Iteration #5233  Loss: 0.48949753601576507; l2 norm of gradients: 0.024407152481821865; l2 norm of weights: 0.42373384711408196\n","Iteration #5234  Loss: 0.48949364314715726; l2 norm of gradients: 0.02438152291398131; l2 norm of weights: 0.42372771970436723\n","Iteration #5235  Loss: 0.48948975261605854; l2 norm of gradients: 0.024355921763378655; l2 norm of weights: 0.42372159784711555\n","Iteration #5236  Loss: 0.48948586441945774; l2 norm of gradients: 0.02433034899905329; l2 norm of weights: 0.42371548153375954\n","Iteration #5237  Loss: 0.4894819785543497; l2 norm of gradients: 0.02430480459007882; l2 norm of weights: 0.42370937075574755\n","Iteration #5238  Loss: 0.4894780950177354; l2 norm of gradients: 0.024279288505562976; l2 norm of weights: 0.4237032655045438\n","Iteration #5239  Loss: 0.4894742138066212; l2 norm of gradients: 0.02425380071464761; l2 norm of weights: 0.423697165771628\n","Iteration #5240  Loss: 0.48947033491801983; l2 norm of gradients: 0.02422834118650867; l2 norm of weights: 0.42369107154849595\n","Iteration #5241  Loss: 0.48946645834894936; l2 norm of gradients: 0.024202909890356148; l2 norm of weights: 0.4236849828266587\n","Iteration #5242  Loss: 0.4894625840964342; l2 norm of gradients: 0.02417750679543403; l2 norm of weights: 0.42367889959764304\n","Iteration #5243  Loss: 0.48945871215750447; l2 norm of gradients: 0.024152131871020305; l2 norm of weights: 0.4236728218529916\n","Iteration #5244  Loss: 0.4894548425291959; l2 norm of gradients: 0.024126785086426825; l2 norm of weights: 0.42366674958426237\n","Iteration #5245  Loss: 0.48945097520855013; l2 norm of gradients: 0.024101466410999393; l2 norm of weights: 0.42366068278302893\n","Iteration #5246  Loss: 0.48944711019261483; l2 norm of gradients: 0.024076175814117626; l2 norm of weights: 0.42365462144088023\n","Iteration #5247  Loss: 0.4894432474784431; l2 norm of gradients: 0.024050913265195012; l2 norm of weights: 0.42364856554942093\n","Iteration #5248  Loss: 0.4894393870630941; l2 norm of gradients: 0.024025678733678755; l2 norm of weights: 0.42364251510027107\n","Iteration #5249  Loss: 0.4894355289436324; l2 norm of gradients: 0.02400047218904985; l2 norm of weights: 0.42363647008506605\n","Iteration #5250  Loss: 0.48943167311712876; l2 norm of gradients: 0.023975293600822976; l2 norm of weights: 0.4236304304954568\n","Iteration #5251  Loss: 0.4894278195806593; l2 norm of gradients: 0.023950142938546443; l2 norm of weights: 0.42362439632310933\n","Iteration #5252  Loss: 0.4894239683313062; l2 norm of gradients: 0.02392502017180223; l2 norm of weights: 0.4236183675597053\n","Iteration #5253  Loss: 0.4894201193661569; l2 norm of gradients: 0.023899925270205943; l2 norm of weights: 0.42361234419694144\n","Iteration #5254  Loss: 0.48941627268230503; l2 norm of gradients: 0.023874858203406638; l2 norm of weights: 0.42360632622652983\n","Iteration #5255  Loss: 0.4894124282768496; l2 norm of gradients: 0.023849818941086957; l2 norm of weights: 0.42360031364019796\n","Iteration #5256  Loss: 0.4894085861468953; l2 norm of gradients: 0.02382480745296304; l2 norm of weights: 0.42359430642968826\n","Iteration #5257  Loss: 0.4894047462895526; l2 norm of gradients: 0.023799823708784418; l2 norm of weights: 0.4235883045867584\n","Iteration #5258  Loss: 0.4894009087019376; l2 norm of gradients: 0.023774867678334; l2 norm of weights: 0.4235823081031813\n","Iteration #5259  Loss: 0.48939707338117194; l2 norm of gradients: 0.023749939331428164; l2 norm of weights: 0.4235763169707451\n","Iteration #5260  Loss: 0.489393240324383; l2 norm of gradients: 0.02372503863791652; l2 norm of weights: 0.4235703311812527\n","Iteration #5261  Loss: 0.48938940952870374; l2 norm of gradients: 0.02370016556768201; l2 norm of weights: 0.4235643507265223\n","Iteration #5262  Loss: 0.4893855809912727; l2 norm of gradients: 0.02367532009064082; l2 norm of weights: 0.4235583755983872\n","Iteration #5263  Loss: 0.48938175470923384; l2 norm of gradients: 0.02365050217674237; l2 norm of weights: 0.4235524057886955\n","Iteration #5264  Loss: 0.4893779306797371; l2 norm of gradients: 0.02362571179596926; l2 norm of weights: 0.42354644128931046\n","Iteration #5265  Loss: 0.4893741088999376; l2 norm of gradients: 0.023600948918337188; l2 norm of weights: 0.4235404820921101\n","Iteration #5266  Loss: 0.4893702893669961; l2 norm of gradients: 0.023576213513894998; l2 norm of weights: 0.42353452818898746\n","Iteration #5267  Loss: 0.4893664720780792; l2 norm of gradients: 0.023551505552724606; l2 norm of weights: 0.4235285795718505\n","Iteration #5268  Loss: 0.4893626570303586; l2 norm of gradients: 0.02352682500494095; l2 norm of weights: 0.4235226362326221\n","Iteration #5269  Loss: 0.4893588442210117; l2 norm of gradients: 0.023502171840691942; l2 norm of weights: 0.4235166981632397\n","Iteration #5270  Loss: 0.4893550336472215; l2 norm of gradients: 0.02347754603015849; l2 norm of weights: 0.4235107653556559\n","Iteration #5271  Loss: 0.48935122530617625; l2 norm of gradients: 0.023452947543554405; l2 norm of weights: 0.42350483780183773\n","Iteration #5272  Loss: 0.4893474191950701; l2 norm of gradients: 0.023428376351126372; l2 norm of weights: 0.4234989154937673\n","Iteration #5273  Loss: 0.48934361531110204; l2 norm of gradients: 0.023403832423153945; l2 norm of weights: 0.423492998423441\n","Iteration #5274  Loss: 0.4893398136514771; l2 norm of gradients: 0.023379315729949474; l2 norm of weights: 0.42348708658287043\n","Iteration #5275  Loss: 0.48933601421340556; l2 norm of gradients: 0.023354826241858095; l2 norm of weights: 0.4234811799640814\n","Iteration #5276  Loss: 0.4893322169941029; l2 norm of gradients: 0.023330363929257657; l2 norm of weights: 0.4234752785591147\n","Iteration #5277  Loss: 0.48932842199079035; l2 norm of gradients: 0.02330592876255876; l2 norm of weights: 0.42346938236002546\n","Iteration #5278  Loss: 0.48932462920069447; l2 norm of gradients: 0.023281520712204566; l2 norm of weights: 0.4234634913588835\n","Iteration #5279  Loss: 0.48932083862104714; l2 norm of gradients: 0.023257139748671034; l2 norm of weights: 0.4234576055477732\n","Iteration #5280  Loss: 0.48931705024908556; l2 norm of gradients: 0.023232785842466526; l2 norm of weights: 0.4234517249187933\n","Iteration #5281  Loss: 0.4893132640820525; l2 norm of gradients: 0.023208458964132112; l2 norm of weights: 0.4234458494640572\n","Iteration #5282  Loss: 0.489309480117196; l2 norm of gradients: 0.023184159084241283; l2 norm of weights: 0.42343997917569276\n","Iteration #5283  Loss: 0.4893056983517693; l2 norm of gradients: 0.023159886173400056; l2 norm of weights: 0.4234341140458422\n","Iteration #5284  Loss: 0.48930191878303136; l2 norm of gradients: 0.023135640202246902; l2 norm of weights: 0.4234282540666621\n","Iteration #5285  Loss: 0.489298141408246; l2 norm of gradients: 0.02311142114145266; l2 norm of weights: 0.42342239923032354\n","Iteration #5286  Loss: 0.48929436622468275; l2 norm of gradients: 0.023087228961720593; l2 norm of weights: 0.4234165495290118\n","Iteration #5287  Loss: 0.4892905932296163; l2 norm of gradients: 0.023063063633786258; l2 norm of weights: 0.4234107049549267\n","Iteration #5288  Loss: 0.4892868224203265; l2 norm of gradients: 0.02303892512841754; l2 norm of weights: 0.4234048655002821\n","Iteration #5289  Loss: 0.4892830537940988; l2 norm of gradients: 0.02301481341641458; l2 norm of weights: 0.4233990311573063\n","Iteration #5290  Loss: 0.48927928734822357; l2 norm of gradients: 0.022990728468609754; l2 norm of weights: 0.4233932019182417\n","Iteration #5291  Loss: 0.48927552307999655; l2 norm of gradients: 0.022966670255867648; l2 norm of weights: 0.42338737777534496\n","Iteration #5292  Loss: 0.48927176098671893; l2 norm of gradients: 0.02294263874908492; l2 norm of weights: 0.42338155872088706\n","Iteration #5293  Loss: 0.4892680010656969; l2 norm of gradients: 0.022918633919190478; l2 norm of weights: 0.4233757447471529\n","Iteration #5294  Loss: 0.48926424331424195; l2 norm of gradients: 0.02289465573714523; l2 norm of weights: 0.4233699358464416\n","Iteration #5295  Loss: 0.4892604877296708; l2 norm of gradients: 0.02287070417394214; l2 norm of weights: 0.42336413201106654\n","Iteration #5296  Loss: 0.48925673430930544; l2 norm of gradients: 0.022846779200606217; l2 norm of weights: 0.4233583332333549\n","Iteration #5297  Loss: 0.489252983050473; l2 norm of gradients: 0.022822880788194434; l2 norm of weights: 0.42335253950564816\n","Iteration #5298  Loss: 0.48924923395050557; l2 norm of gradients: 0.022799008907795692; l2 norm of weights: 0.4233467508203015\n","Iteration #5299  Loss: 0.4892454870067408; l2 norm of gradients: 0.022775163530530815; l2 norm of weights: 0.42334096716968433\n","Iteration #5300  Loss: 0.4892417422165213; l2 norm of gradients: 0.022751344627552482; l2 norm of weights: 0.4233351885461801\n","Iteration #5301  Loss: 0.4892379995771948; l2 norm of gradients: 0.022727552170045243; l2 norm of weights: 0.4233294149421858\n","Iteration #5302  Loss: 0.4892342590861143; l2 norm of gradients: 0.022703786129225405; l2 norm of weights: 0.42332364635011277\n","Iteration #5303  Loss: 0.4892305207406376; l2 norm of gradients: 0.022680046476341045; l2 norm of weights: 0.42331788276238597\n","Iteration #5304  Loss: 0.48922678453812796; l2 norm of gradients: 0.022656333182671986; l2 norm of weights: 0.42331212417144426\n","Iteration #5305  Loss: 0.4892230504759538; l2 norm of gradients: 0.022632646219529748; l2 norm of weights: 0.42330637056974046\n","Iteration #5306  Loss: 0.4892193185514883; l2 norm of gradients: 0.0226089855582575; l2 norm of weights: 0.42330062194974094\n","Iteration #5307  Loss: 0.4892155887621099; l2 norm of gradients: 0.02258535117023002; l2 norm of weights: 0.4232948783039261\n","Iteration #5308  Loss: 0.4892118611052022; l2 norm of gradients: 0.022561743026853678; l2 norm of weights: 0.4232891396247898\n","Iteration #5309  Loss: 0.4892081355781536; l2 norm of gradients: 0.022538161099566436; l2 norm of weights: 0.42328340590484\n","Iteration #5310  Loss: 0.48920441217835786; l2 norm of gradients: 0.02251460535983769; l2 norm of weights: 0.4232776771365979\n","Iteration #5311  Loss: 0.4892006909032134; l2 norm of gradients: 0.02249107577916842; l2 norm of weights: 0.4232719533125987\n","Iteration #5312  Loss: 0.4891969717501242; l2 norm of gradients: 0.02246757232909093; l2 norm of weights: 0.4232662344253912\n","Iteration #5313  Loss: 0.4891932547164988; l2 norm of gradients: 0.02244409498116906; l2 norm of weights: 0.4232605204675377\n","Iteration #5314  Loss: 0.4891895397997509; l2 norm of gradients: 0.022420643706997947; l2 norm of weights: 0.42325481143161414\n","Iteration #5315  Loss: 0.4891858269972992; l2 norm of gradients: 0.02239721847820407; l2 norm of weights: 0.42324910731021\n","Iteration #5316  Loss: 0.48918211630656727; l2 norm of gradients: 0.022373819266445247; l2 norm of weights: 0.4232434080959282\n","Iteration #5317  Loss: 0.489178407724984; l2 norm of gradients: 0.022350446043410556; l2 norm of weights: 0.4232377137813854\n","Iteration #5318  Loss: 0.48917470124998275; l2 norm of gradients: 0.022327098780820294; l2 norm of weights: 0.4232320243592117\n","Iteration #5319  Loss: 0.4891709968790021; l2 norm of gradients: 0.022303777450425975; l2 norm of weights: 0.4232263398220503\n","Iteration #5320  Loss: 0.48916729460948577; l2 norm of gradients: 0.022280482024010298; l2 norm of weights: 0.42322066016255827\n","Iteration #5321  Loss: 0.48916359443888213; l2 norm of gradients: 0.022257212473387036; l2 norm of weights: 0.42321498537340585\n","Iteration #5322  Loss: 0.48915989636464435; l2 norm of gradients: 0.02223396877040109; l2 norm of weights: 0.4232093154472767\n","Iteration #5323  Loss: 0.48915620038423085; l2 norm of gradients: 0.02221075088692842; l2 norm of weights: 0.4232036503768678\n","Iteration #5324  Loss: 0.4891525064951049; l2 norm of gradients: 0.022187558794876036; l2 norm of weights: 0.4231979901548896\n","Iteration #5325  Loss: 0.48914881469473437; l2 norm of gradients: 0.022164392466181917; l2 norm of weights: 0.42319233477406576\n","Iteration #5326  Loss: 0.48914512498059215; l2 norm of gradients: 0.022141251872814984; l2 norm of weights: 0.42318668422713296\n","Iteration #5327  Loss: 0.48914143735015625; l2 norm of gradients: 0.022118136986775117; l2 norm of weights: 0.4231810385068416\n","Iteration #5328  Loss: 0.4891377518009091; l2 norm of gradients: 0.022095047780093042; l2 norm of weights: 0.4231753976059548\n","Iteration #5329  Loss: 0.48913406833033857; l2 norm of gradients: 0.022071984224830376; l2 norm of weights: 0.4231697615172494\n","Iteration #5330  Loss: 0.48913038693593663; l2 norm of gradients: 0.022048946293079522; l2 norm of weights: 0.42316413023351496\n","Iteration #5331  Loss: 0.4891267076152005; l2 norm of gradients: 0.022025933956963697; l2 norm of weights: 0.42315850374755437\n","Iteration #5332  Loss: 0.4891230303656323; l2 norm of gradients: 0.022002947188636837; l2 norm of weights: 0.4231528820521837\n","Iteration #5333  Loss: 0.4891193551847387; l2 norm of gradients: 0.02197998596028366; l2 norm of weights: 0.42314726514023204\n","Iteration #5334  Loss: 0.4891156820700314; l2 norm of gradients: 0.021957050244119453; l2 norm of weights: 0.4231416530045415\n","Iteration #5335  Loss: 0.4891120110190267; l2 norm of gradients: 0.021934140012390257; l2 norm of weights: 0.4231360456379673\n","Iteration #5336  Loss: 0.48910834202924575; l2 norm of gradients: 0.021911255237372675; l2 norm of weights: 0.42313044303337766\n","Iteration #5337  Loss: 0.4891046750982143; l2 norm of gradients: 0.021888395891373875; l2 norm of weights: 0.4231248451836536\n","Iteration #5338  Loss: 0.4891010102234634; l2 norm of gradients: 0.02186556194673161; l2 norm of weights: 0.4231192520816896\n","Iteration #5339  Loss: 0.48909734740252814; l2 norm of gradients: 0.021842753375814105; l2 norm of weights: 0.42311366372039255\n","Iteration #5340  Loss: 0.4890936866329486; l2 norm of gradients: 0.021819970151020083; l2 norm of weights: 0.42310808009268247\n","Iteration #5341  Loss: 0.48909002791226996; l2 norm of gradients: 0.0217972122447787; l2 norm of weights: 0.42310250119149245\n","Iteration #5342  Loss: 0.48908637123804155; l2 norm of gradients: 0.021774479629549526; l2 norm of weights: 0.4230969270097682\n","Iteration #5343  Loss: 0.48908271660781755; l2 norm of gradients: 0.021751772277822493; l2 norm of weights: 0.42309135754046806\n","Iteration #5344  Loss: 0.48907906401915724; l2 norm of gradients: 0.021729090162117912; l2 norm of weights: 0.4230857927765639\n","Iteration #5345  Loss: 0.48907541346962413; l2 norm of gradients: 0.021706433254986352; l2 norm of weights: 0.42308023271103967\n","Iteration #5346  Loss: 0.4890717649567864; l2 norm of gradients: 0.021683801529008655; l2 norm of weights: 0.42307467733689236\n","Iteration #5347  Loss: 0.4890681184782171; l2 norm of gradients: 0.021661194956795922; l2 norm of weights: 0.42306912664713175\n","Iteration #5348  Loss: 0.48906447403149406; l2 norm of gradients: 0.02163861351098947; l2 norm of weights: 0.4230635806347803\n","Iteration #5349  Loss: 0.48906083161419933; l2 norm of gradients: 0.021616057164260762; l2 norm of weights: 0.42305803929287317\n","Iteration #5350  Loss: 0.48905719122392; l2 norm of gradients: 0.021593525889311384; l2 norm of weights: 0.42305250261445804\n","Iteration #5351  Loss: 0.4890535528582475; l2 norm of gradients: 0.02157101965887305; l2 norm of weights: 0.4230469705925955\n","Iteration #5352  Loss: 0.48904991651477797; l2 norm of gradients: 0.021548538445707532; l2 norm of weights: 0.4230414432203584\n","Iteration #5353  Loss: 0.4890462821911122; l2 norm of gradients: 0.021526082222606685; l2 norm of weights: 0.4230359204908326\n","Iteration #5354  Loss: 0.4890426498848555; l2 norm of gradients: 0.021503650962392266; l2 norm of weights: 0.4230304023971162\n","Iteration #5355  Loss: 0.48903901959361784; l2 norm of gradients: 0.02148124463791611; l2 norm of weights: 0.42302488893232\n","Iteration #5356  Loss: 0.4890353913150137; l2 norm of gradients: 0.021458863222059893; l2 norm of weights: 0.42301938008956724\n","Iteration #5357  Loss: 0.48903176504666224; l2 norm of gradients: 0.021436506687735255; l2 norm of weights: 0.4230138758619938\n","Iteration #5358  Loss: 0.48902814078618684; l2 norm of gradients: 0.021414175007883684; l2 norm of weights: 0.42300837624274773\n","Iteration #5359  Loss: 0.4890245185312159; l2 norm of gradients: 0.021391868155476494; l2 norm of weights: 0.4230028812249899\n","Iteration #5360  Loss: 0.489020898279382; l2 norm of gradients: 0.021369586103514793; l2 norm of weights: 0.42299739080189336\n","Iteration #5361  Loss: 0.4890172800283224; l2 norm of gradients: 0.02134732882502949; l2 norm of weights: 0.42299190496664363\n","Iteration #5362  Loss: 0.48901366377567884; l2 norm of gradients: 0.02132509629308121; l2 norm of weights: 0.42298642371243855\n","Iteration #5363  Loss: 0.4890100495190975; l2 norm of gradients: 0.021302888480760267; l2 norm of weights: 0.4229809470324884\n","Iteration #5364  Loss: 0.48900643725622917; l2 norm of gradients: 0.021280705361186633; l2 norm of weights: 0.42297547492001575\n","Iteration #5365  Loss: 0.4890028269847291; l2 norm of gradients: 0.021258546907509977; l2 norm of weights: 0.42297000736825546\n","Iteration #5366  Loss: 0.4889992187022569; l2 norm of gradients: 0.021236413092909506; l2 norm of weights: 0.42296454437045466\n","Iteration #5367  Loss: 0.48899561240647665; l2 norm of gradients: 0.021214303890594012; l2 norm of weights: 0.42295908591987275\n","Iteration #5368  Loss: 0.4889920080950572; l2 norm of gradients: 0.021192219273801823; l2 norm of weights: 0.4229536320097813\n","Iteration #5369  Loss: 0.4889884057656715; l2 norm of gradients: 0.021170159215800774; l2 norm of weights: 0.42294818263346423\n","Iteration #5370  Loss: 0.4889848054159971; l2 norm of gradients: 0.02114812368988817; l2 norm of weights: 0.4229427377842176\n","Iteration #5371  Loss: 0.48898120704371584; l2 norm of gradients: 0.021126112669390754; l2 norm of weights: 0.4229372974553494\n","Iteration #5372  Loss: 0.4889776106465142; l2 norm of gradients: 0.021104126127664654; l2 norm of weights: 0.4229318616401801\n","Iteration #5373  Loss: 0.48897401622208275; l2 norm of gradients: 0.021082164038095392; l2 norm of weights: 0.422926430332042\n","Iteration #5374  Loss: 0.4889704237681168; l2 norm of gradients: 0.021060226374097806; l2 norm of weights: 0.4229210035242798\n","Iteration #5375  Loss: 0.4889668332823158; l2 norm of gradients: 0.021038313109116046; l2 norm of weights: 0.42291558121024986\n","Iteration #5376  Loss: 0.48896324476238384; l2 norm of gradients: 0.021016424216623555; l2 norm of weights: 0.422910163383321\n","Iteration #5377  Loss: 0.488959658206029; l2 norm of gradients: 0.020994559670122995; l2 norm of weights: 0.42290475003687367\n","Iteration #5378  Loss: 0.48895607361096416; l2 norm of gradients: 0.020972719443146208; l2 norm of weights: 0.4228993411643007\n","Iteration #5379  Loss: 0.4889524909749062; l2 norm of gradients: 0.02095090350925427; l2 norm of weights: 0.42289393675900655\n","Iteration #5380  Loss: 0.48894891029557647; l2 norm of gradients: 0.02092911184203733; l2 norm of weights: 0.42288853681440786\n","Iteration #5381  Loss: 0.48894533157070086; l2 norm of gradients: 0.020907344415114694; l2 norm of weights: 0.42288314132393306\n","Iteration #5382  Loss: 0.48894175479800905; l2 norm of gradients: 0.020885601202134733; l2 norm of weights: 0.42287775028102254\n","Iteration #5383  Loss: 0.4889381799752357; l2 norm of gradients: 0.020863882176774862; l2 norm of weights: 0.42287236367912867\n","Iteration #5384  Loss: 0.48893460710011943; l2 norm of gradients: 0.020842187312741463; l2 norm of weights: 0.42286698151171537\n","Iteration #5385  Loss: 0.4889310361704027; l2 norm of gradients: 0.020820516583769984; l2 norm of weights: 0.42286160377225873\n","Iteration #5386  Loss: 0.4889274671838334; l2 norm of gradients: 0.020798869963624723; l2 norm of weights: 0.4228562304542464\n","Iteration #5387  Loss: 0.48892390013816267; l2 norm of gradients: 0.02077724742609896; l2 norm of weights: 0.4228508615511782\n","Iteration #5388  Loss: 0.48892033503114624; l2 norm of gradients: 0.020755648945014846; l2 norm of weights: 0.4228454970565651\n","Iteration #5389  Loss: 0.48891677186054433; l2 norm of gradients: 0.020734074494223327; l2 norm of weights: 0.4228401369639304\n","Iteration #5390  Loss: 0.4889132106241211; l2 norm of gradients: 0.020712524047604236; l2 norm of weights: 0.42283478126680885\n","Iteration #5391  Loss: 0.4889096513196451; l2 norm of gradients: 0.02069099757906611; l2 norm of weights: 0.4228294299587467\n","Iteration #5392  Loss: 0.48890609394488904; l2 norm of gradients: 0.02066949506254635; l2 norm of weights: 0.42282408303330243\n","Iteration #5393  Loss: 0.48890253849763005; l2 norm of gradients: 0.02064801647201097; l2 norm of weights: 0.42281874048404555\n","Iteration #5394  Loss: 0.48889898497564915; l2 norm of gradients: 0.020626561781454708; l2 norm of weights: 0.4228134023045577\n","Iteration #5395  Loss: 0.4888954333767318; l2 norm of gradients: 0.020605130964900985; l2 norm of weights: 0.42280806848843183\n","Iteration #5396  Loss: 0.48889188369866776; l2 norm of gradients: 0.02058372399640182; l2 norm of weights: 0.42280273902927257\n","Iteration #5397  Loss: 0.4888883359392505; l2 norm of gradients: 0.020562340850037795; l2 norm of weights: 0.422797413920696\n","Iteration #5398  Loss: 0.4888847900962782; l2 norm of gradients: 0.020540981499918103; l2 norm of weights: 0.4227920931563299\n","Iteration #5399  Loss: 0.48888124616755296; l2 norm of gradients: 0.020519645920180428; l2 norm of weights: 0.42278677672981363\n","Iteration #5400  Loss: 0.4888777041508811; l2 norm of gradients: 0.020498334084990955; l2 norm of weights: 0.4227814646347977\n","Iteration #5401  Loss: 0.488874164044073; l2 norm of gradients: 0.020477045968544362; l2 norm of weights: 0.42277615686494446\n","Iteration #5402  Loss: 0.48887062584494334; l2 norm of gradients: 0.020455781545063702; l2 norm of weights: 0.4227708534139275\n","Iteration #5403  Loss: 0.48886708955131075; l2 norm of gradients: 0.02043454078880049; l2 norm of weights: 0.4227655542754319\n","Iteration #5404  Loss: 0.4888635551609982; l2 norm of gradients: 0.020413323674034554; l2 norm of weights: 0.4227602594431542\n","Iteration #5405  Loss: 0.4888600226718324; l2 norm of gradients: 0.02039213017507409; l2 norm of weights: 0.4227549689108021\n","Iteration #5406  Loss: 0.48885649208164467; l2 norm of gradients: 0.02037096026625558; l2 norm of weights: 0.42274968267209506\n","Iteration #5407  Loss: 0.4888529633882701; l2 norm of gradients: 0.020349813921943793; l2 norm of weights: 0.4227444007207634\n","Iteration #5408  Loss: 0.4888494365895479; l2 norm of gradients: 0.020328691116531725; l2 norm of weights: 0.42273912305054917\n","Iteration #5409  Loss: 0.4888459116833214; l2 norm of gradients: 0.0203075918244406; l2 norm of weights: 0.42273384965520544\n","Iteration #5410  Loss: 0.4888423886674383; l2 norm of gradients: 0.0202865160201198; l2 norm of weights: 0.42272858052849677\n","Iteration #5411  Loss: 0.48883886753974964; l2 norm of gradients: 0.020265463678046852; l2 norm of weights: 0.4227233156641988\n","Iteration #5412  Loss: 0.48883534829811115; l2 norm of gradients: 0.0202444347727274; l2 norm of weights: 0.42271805505609833\n","Iteration #5413  Loss: 0.48883183094038263; l2 norm of gradients: 0.020223429278695183; l2 norm of weights: 0.4227127986979937\n","Iteration #5414  Loss: 0.48882831546442734; l2 norm of gradients: 0.02020244717051197; l2 norm of weights: 0.42270754658369425\n","Iteration #5415  Loss: 0.4888248018681131; l2 norm of gradients: 0.020181488422767575; l2 norm of weights: 0.42270229870702025\n","Iteration #5416  Loss: 0.4888212901493115; l2 norm of gradients: 0.020160553010079796; l2 norm of weights: 0.4226970550618035\n","Iteration #5417  Loss: 0.48881778030589834; l2 norm of gradients: 0.020139640907094365; l2 norm of weights: 0.4226918156418866\n","Iteration #5418  Loss: 0.4888142723357532; l2 norm of gradients: 0.020118752088484956; l2 norm of weights: 0.42268658044112367\n","Iteration #5419  Loss: 0.4888107662367598; l2 norm of gradients: 0.020097886528953123; l2 norm of weights: 0.4226813494533793\n","Iteration #5420  Loss: 0.4888072620068057; l2 norm of gradients: 0.020077044203228298; l2 norm of weights: 0.42267612267252974\n","Iteration #5421  Loss: 0.48880375964378264; l2 norm of gradients: 0.020056225086067754; l2 norm of weights: 0.42267090009246194\n","Iteration #5422  Loss: 0.4888002591455863; l2 norm of gradients: 0.020035429152256538; l2 norm of weights: 0.4226656817070739\n","Iteration #5423  Loss: 0.48879676051011606; l2 norm of gradients: 0.020014656376607497; l2 norm of weights: 0.42266046751027464\n","Iteration #5424  Loss: 0.4887932637352756; l2 norm of gradients: 0.019993906733961172; l2 norm of weights: 0.4226552574959842\n","Iteration #5425  Loss: 0.4887897688189722; l2 norm of gradients: 0.019973180199185838; l2 norm of weights: 0.4226500516581335\n","Iteration #5426  Loss: 0.48878627575911754; l2 norm of gradients: 0.01995247674717747; l2 norm of weights: 0.4226448499906645\n","Iteration #5427  Loss: 0.48878278455362656; l2 norm of gradients: 0.01993179635285966; l2 norm of weights: 0.4226396524875298\n","Iteration #5428  Loss: 0.48877929520041896; l2 norm of gradients: 0.01991113899118359; l2 norm of weights: 0.42263445914269326\n","Iteration #5429  Loss: 0.4887758076974175; l2 norm of gradients: 0.019890504637128062; l2 norm of weights: 0.42262926995012934\n","Iteration #5430  Loss: 0.48877232204254933; l2 norm of gradients: 0.019869893265699445; l2 norm of weights: 0.4226240849038234\n","Iteration #5431  Loss: 0.48876883823374556; l2 norm of gradients: 0.019849304851931603; l2 norm of weights: 0.4226189039977717\n","Iteration #5432  Loss: 0.4887653562689409; l2 norm of gradients: 0.01982873937088586; l2 norm of weights: 0.42261372722598123\n","Iteration #5433  Loss: 0.48876187614607414; l2 norm of gradients: 0.019808196797651043; l2 norm of weights: 0.4226085545824698\n","Iteration #5434  Loss: 0.48875839786308767; l2 norm of gradients: 0.019787677107343405; l2 norm of weights: 0.4226033860612659\n","Iteration #5435  Loss: 0.488754921417928; l2 norm of gradients: 0.019767180275106575; l2 norm of weights: 0.42259822165640887\n","Iteration #5436  Loss: 0.4887514468085456; l2 norm of gradients: 0.019746706276111567; l2 norm of weights: 0.42259306136194874\n","Iteration #5437  Loss: 0.48874797403289433; l2 norm of gradients: 0.019726255085556733; l2 norm of weights: 0.4225879051719462\n","Iteration #5438  Loss: 0.4887445030889324; l2 norm of gradients: 0.01970582667866774; l2 norm of weights: 0.42258275308047266\n","Iteration #5439  Loss: 0.4887410339746214; l2 norm of gradients: 0.019685421030697467; l2 norm of weights: 0.4225776050816101\n","Iteration #5440  Loss: 0.488737566687927; l2 norm of gradients: 0.01966503811692614; l2 norm of weights: 0.4225724611694513\n","Iteration #5441  Loss: 0.4887341012268187; l2 norm of gradients: 0.019644677912661116; l2 norm of weights: 0.42256732133809943\n","Iteration #5442  Loss: 0.48873063758926966; l2 norm of gradients: 0.019624340393236947; l2 norm of weights: 0.4225621855816685\n","Iteration #5443  Loss: 0.4887271757732569; l2 norm of gradients: 0.0196040255340154; l2 norm of weights: 0.422557053894283\n","Iteration #5444  Loss: 0.4887237157767613; l2 norm of gradients: 0.019583733310385265; l2 norm of weights: 0.42255192627007776\n","Iteration #5445  Loss: 0.4887202575977673; l2 norm of gradients: 0.01956346369776254; l2 norm of weights: 0.42254680270319855\n","Iteration #5446  Loss: 0.4887168012342635; l2 norm of gradients: 0.01954321667159017; l2 norm of weights: 0.42254168318780144\n","Iteration #5447  Loss: 0.4887133466842416; l2 norm of gradients: 0.019522992207338212; l2 norm of weights: 0.42253656771805276\n","Iteration #5448  Loss: 0.48870989394569786; l2 norm of gradients: 0.019502790280503703; l2 norm of weights: 0.4225314562881298\n","Iteration #5449  Loss: 0.4887064430166317; l2 norm of gradients: 0.019482610866610642; l2 norm of weights: 0.42252634889221996\n","Iteration #5450  Loss: 0.4887029938950465; l2 norm of gradients: 0.01946245394120994; l2 norm of weights: 0.4225212455245212\n","Iteration #5451  Loss: 0.48869954657894943; l2 norm of gradients: 0.01944231947987948; l2 norm of weights: 0.4225161461792417\n","Iteration #5452  Loss: 0.48869610106635114; l2 norm of gradients: 0.019422207458223997; l2 norm of weights: 0.42251105085060037\n","Iteration #5453  Loss: 0.4886926573552663; l2 norm of gradients: 0.01940211785187505; l2 norm of weights: 0.4225059595328263\n","Iteration #5454  Loss: 0.488689215443713; l2 norm of gradients: 0.019382050636491052; l2 norm of weights: 0.42250087222015886\n","Iteration #5455  Loss: 0.4886857753297133; l2 norm of gradients: 0.0193620057877572; l2 norm of weights: 0.4224957889068479\n","Iteration #5456  Loss: 0.4886823370112926; l2 norm of gradients: 0.019341983281385453; l2 norm of weights: 0.4224907095871535\n","Iteration #5457  Loss: 0.48867890048648044; l2 norm of gradients: 0.019321983093114482; l2 norm of weights: 0.4224856342553461\n","Iteration #5458  Loss: 0.48867546575330967; l2 norm of gradients: 0.019302005198709678; l2 norm of weights: 0.42248056290570624\n","Iteration #5459  Loss: 0.4886720328098169; l2 norm of gradients: 0.019282049573963074; l2 norm of weights: 0.4224754955325249\n","Iteration #5460  Loss: 0.4886686016540424; l2 norm of gradients: 0.01926211619469338; l2 norm of weights: 0.4224704321301033\n","Iteration #5461  Loss: 0.4886651722840304; l2 norm of gradients: 0.019242205036745882; l2 norm of weights: 0.42246537269275275\n","Iteration #5462  Loss: 0.48866174469782786; l2 norm of gradients: 0.01922231607599248; l2 norm of weights: 0.4224603172147947\n","Iteration #5463  Loss: 0.48865831889348665; l2 norm of gradients: 0.019202449288331588; l2 norm of weights: 0.4224552656905611\n","Iteration #5464  Loss: 0.48865489486906133; l2 norm of gradients: 0.019182604649688167; l2 norm of weights: 0.42245021811439365\n","Iteration #5465  Loss: 0.4886514726226104; l2 norm of gradients: 0.019162782136013634; l2 norm of weights: 0.4224451744806445\n","Iteration #5466  Loss: 0.488648052152196; l2 norm of gradients: 0.01914298172328593; l2 norm of weights: 0.4224401347836757\n","Iteration #5467  Loss: 0.4886446334558835; l2 norm of gradients: 0.019123203387509388; l2 norm of weights: 0.42243509901785953\n","Iteration #5468  Loss: 0.4886412165317427; l2 norm of gradients: 0.019103447104714733; l2 norm of weights: 0.4224300671775782\n","Iteration #5469  Loss: 0.48863780137784607; l2 norm of gradients: 0.01908371285095903; l2 norm of weights: 0.42242503925722413\n","Iteration #5470  Loss: 0.4886343879922702; l2 norm of gradients: 0.019064000602325777; l2 norm of weights: 0.4224200152511998\n","Iteration #5471  Loss: 0.4886309763730952; l2 norm of gradients: 0.01904431033492474; l2 norm of weights: 0.42241499515391756\n","Iteration #5472  Loss: 0.4886275665184045; l2 norm of gradients: 0.01902464202489193; l2 norm of weights: 0.42240997895979987\n","Iteration #5473  Loss: 0.48862415842628526; l2 norm of gradients: 0.019004995648389686; l2 norm of weights: 0.422404966663279\n","Iteration #5474  Loss: 0.4886207520948284; l2 norm of gradients: 0.018985371181606498; l2 norm of weights: 0.4223999582587974\n","Iteration #5475  Loss: 0.48861734752212793; l2 norm of gradients: 0.018965768600757077; l2 norm of weights: 0.4223949537408074\n","Iteration #5476  Loss: 0.4886139447062817; l2 norm of gradients: 0.01894618788208236; l2 norm of weights: 0.4223899531037711\n","Iteration #5477  Loss: 0.4886105436453909; l2 norm of gradients: 0.0189266290018493; l2 norm of weights: 0.42238495634216067\n","Iteration #5478  Loss: 0.4886071443375606; l2 norm of gradients: 0.018907091936351067; l2 norm of weights: 0.4223799634504582\n","Iteration #5479  Loss: 0.48860374678089896; l2 norm of gradients: 0.018887576661906845; l2 norm of weights: 0.42237497442315547\n","Iteration #5480  Loss: 0.48860035097351784; l2 norm of gradients: 0.018868083154861888; l2 norm of weights: 0.42236998925475416\n","Iteration #5481  Loss: 0.4885969569135328; l2 norm of gradients: 0.018848611391587475; l2 norm of weights: 0.42236500793976584\n","Iteration #5482  Loss: 0.4885935645990623; l2 norm of gradients: 0.018829161348480852; l2 norm of weights: 0.42236003047271187\n","Iteration #5483  Loss: 0.4885901740282289; l2 norm of gradients: 0.018809733001965246; l2 norm of weights: 0.4223550568481234\n","Iteration #5484  Loss: 0.4885867851991584; l2 norm of gradients: 0.018790326328489797; l2 norm of weights: 0.42235008706054133\n","Iteration #5485  Loss: 0.4885833981099799; l2 norm of gradients: 0.01877094130452958; l2 norm of weights: 0.4223451211045163\n","Iteration #5486  Loss: 0.48858001275882623; l2 norm of gradients: 0.0187515779065855; l2 norm of weights: 0.4223401589746086\n","Iteration #5487  Loss: 0.4885766291438336; l2 norm of gradients: 0.018732236111184335; l2 norm of weights: 0.4223352006653884\n","Iteration #5488  Loss: 0.48857324726314155; l2 norm of gradients: 0.01871291589487868; l2 norm of weights: 0.4223302461714356\n","Iteration #5489  Loss: 0.4885698671148932; l2 norm of gradients: 0.01869361723424686; l2 norm of weights: 0.4223252954873395\n","Iteration #5490  Loss: 0.48856648869723496; l2 norm of gradients: 0.01867434010589305; l2 norm of weights: 0.4223203486076993\n","Iteration #5491  Loss: 0.4885631120083168; l2 norm of gradients: 0.018655084486447072; l2 norm of weights: 0.4223154055271237\n","Iteration #5492  Loss: 0.4885597370462921; l2 norm of gradients: 0.01863585035256452; l2 norm of weights: 0.42231046624023116\n","Iteration #5493  Loss: 0.48855636380931744; l2 norm of gradients: 0.018616637680926574; l2 norm of weights: 0.4223055307416496\n","Iteration #5494  Loss: 0.48855299229555316; l2 norm of gradients: 0.018597446448240115; l2 norm of weights: 0.42230059902601663\n","Iteration #5495  Loss: 0.4885496225031626; l2 norm of gradients: 0.018578276631237633; l2 norm of weights: 0.4222956710879794\n","Iteration #5496  Loss: 0.4885462544303128; l2 norm of gradients: 0.018559128206677166; l2 norm of weights: 0.4222907469221944\n","Iteration #5497  Loss: 0.48854288807517404; l2 norm of gradients: 0.01854000115134236; l2 norm of weights: 0.4222858265233282\n","Iteration #5498  Loss: 0.4885395234359201; l2 norm of gradients: 0.01852089544204233; l2 norm of weights: 0.42228090988605604\n","Iteration #5499  Loss: 0.48853616051072773; l2 norm of gradients: 0.018501811055611724; l2 norm of weights: 0.42227599700506346\n","Iteration #5500  Loss: 0.4885327992977777; l2 norm of gradients: 0.018482747968910654; l2 norm of weights: 0.422271087875045\n","Iteration #5501  Loss: 0.4885294397952535; l2 norm of gradients: 0.01846370615882467; l2 norm of weights: 0.4222661824907049\n","Iteration #5502  Loss: 0.48852608200134245; l2 norm of gradients: 0.01844468560226473; l2 norm of weights: 0.42226128084675657\n","Iteration #5503  Loss: 0.4885227259142346; l2 norm of gradients: 0.018425686276167176; l2 norm of weights: 0.422256382937923\n","Iteration #5504  Loss: 0.4885193715321242; l2 norm of gradients: 0.018406708157493717; l2 norm of weights: 0.4222514887589367\n","Iteration #5505  Loss: 0.48851601885320795; l2 norm of gradients: 0.01838775122323136; l2 norm of weights: 0.4222465983045393\n","Iteration #5506  Loss: 0.48851266787568653; l2 norm of gradients: 0.01836881545039245; l2 norm of weights: 0.4222417115694821\n","Iteration #5507  Loss: 0.4885093185977636; l2 norm of gradients: 0.01834990081601456; l2 norm of weights: 0.4222368285485253\n","Iteration #5508  Loss: 0.4885059710176459; l2 norm of gradients: 0.01833100729716054; l2 norm of weights: 0.4222319492364388\n","Iteration #5509  Loss: 0.488502625133544; l2 norm of gradients: 0.018312134870918415; l2 norm of weights: 0.4222270736280019\n","Iteration #5510  Loss: 0.48849928094367145; l2 norm of gradients: 0.018293283514401418; l2 norm of weights: 0.4222222017180028\n","Iteration #5511  Loss: 0.4884959384462451; l2 norm of gradients: 0.01827445320474795; l2 norm of weights: 0.422217333501239\n","Iteration #5512  Loss: 0.48849259763948516; l2 norm of gradients: 0.018255643919121517; l2 norm of weights: 0.42221246897251785\n","Iteration #5513  Loss: 0.48848925852161507; l2 norm of gradients: 0.018236855634710714; l2 norm of weights: 0.4222076081266552\n","Iteration #5514  Loss: 0.48848592109086125; l2 norm of gradients: 0.018218088328729228; l2 norm of weights: 0.42220275095847654\n","Iteration #5515  Loss: 0.488482585345454; l2 norm of gradients: 0.01819934197841579; l2 norm of weights: 0.4221978974628165\n","Iteration #5516  Loss: 0.48847925128362635; l2 norm of gradients: 0.018180616561034157; l2 norm of weights: 0.4221930476345187\n","Iteration #5517  Loss: 0.4884759189036145; l2 norm of gradients: 0.018161912053873006; l2 norm of weights: 0.42218820146843616\n","Iteration #5518  Loss: 0.48847258820365846; l2 norm of gradients: 0.018143228434246074; l2 norm of weights: 0.42218335895943093\n","Iteration #5519  Loss: 0.4884692591820009; l2 norm of gradients: 0.018124565679491958; l2 norm of weights: 0.4221785201023744\n","Iteration #5520  Loss: 0.488465931836888; l2 norm of gradients: 0.018105923766974164; l2 norm of weights: 0.42217368489214663\n","Iteration #5521  Loss: 0.48846260616656895; l2 norm of gradients: 0.0180873026740811; l2 norm of weights: 0.42216885332363724\n","Iteration #5522  Loss: 0.4884592821692962; l2 norm of gradients: 0.018068702378226; l2 norm of weights: 0.4221640253917446\n","Iteration #5523  Loss: 0.4884559598433257; l2 norm of gradients: 0.0180501228568469; l2 norm of weights: 0.4221592010913764\n","Iteration #5524  Loss: 0.4884526391869162; l2 norm of gradients: 0.018031564087406676; l2 norm of weights: 0.4221543804174491\n","Iteration #5525  Loss: 0.4884493201983299; l2 norm of gradients: 0.01801302604739293; l2 norm of weights: 0.4221495633648885\n","Iteration #5526  Loss: 0.4884460028758318; l2 norm of gradients: 0.017994508714318003; l2 norm of weights: 0.4221447499286292\n","Iteration #5527  Loss: 0.4884426872176906; l2 norm of gradients: 0.01797601206571893; l2 norm of weights: 0.42213994010361483\n","Iteration #5528  Loss: 0.4884393732221777; l2 norm of gradients: 0.01795753607915747; l2 norm of weights: 0.4221351338847979\n","Iteration #5529  Loss: 0.4884360608875679; l2 norm of gradients: 0.017939080732219993; l2 norm of weights: 0.4221303312671401\n","Iteration #5530  Loss: 0.4884327502121392; l2 norm of gradients: 0.017920646002517495; l2 norm of weights: 0.4221255322456119\n","Iteration #5531  Loss: 0.48842944119417264; l2 norm of gradients: 0.017902231867685584; l2 norm of weights: 0.42212073681519274\n","Iteration #5532  Loss: 0.4884261338319522; l2 norm of gradients: 0.017883838305384436; l2 norm of weights: 0.4221159449708709\n","Iteration #5533  Loss: 0.48842282812376536; l2 norm of gradients: 0.017865465293298754; l2 norm of weights: 0.4221111567076437\n","Iteration #5534  Loss: 0.4884195240679026; l2 norm of gradients: 0.017847112809137766; l2 norm of weights: 0.4221063720205172\n","Iteration #5535  Loss: 0.4884162216626574; l2 norm of gradients: 0.017828780830635164; l2 norm of weights: 0.42210159090450644\n","Iteration #5536  Loss: 0.4884129209063263; l2 norm of gradients: 0.01781046933554914; l2 norm of weights: 0.422096813354635\n","Iteration #5537  Loss: 0.48840962179720926; l2 norm of gradients: 0.017792178301662267; l2 norm of weights: 0.42209203936593565\n","Iteration #5538  Loss: 0.4884063243336091; l2 norm of gradients: 0.017773907706781583; l2 norm of weights: 0.4220872689334497\n","Iteration #5539  Loss: 0.4884030285138317; l2 norm of gradients: 0.0177556575287384; l2 norm of weights: 0.4220825020522275\n","Iteration #5540  Loss: 0.48839973433618616; l2 norm of gradients: 0.017737427745388508; l2 norm of weights: 0.42207773871732795\n","Iteration #5541  Loss: 0.4883964417989845; l2 norm of gradients: 0.017719218334611906; l2 norm of weights: 0.4220729789238187\n","Iteration #5542  Loss: 0.4883931509005421; l2 norm of gradients: 0.017701029274312954; l2 norm of weights: 0.4220682226667763\n","Iteration #5543  Loss: 0.48838986163917697; l2 norm of gradients: 0.017682860542420232; l2 norm of weights: 0.42206346994128574\n","Iteration #5544  Loss: 0.4883865740132107; l2 norm of gradients: 0.017664712116886597; l2 norm of weights: 0.4220587207424412\n","Iteration #5545  Loss: 0.4883832880209673; l2 norm of gradients: 0.0176465839756891; l2 norm of weights: 0.42205397506534503\n","Iteration #5546  Loss: 0.4883800036607745; l2 norm of gradients: 0.017628476096828966; l2 norm of weights: 0.4220492329051086\n","Iteration #5547  Loss: 0.4883767209309625; l2 norm of gradients: 0.017610388458331596; l2 norm of weights: 0.4220444942568516\n","Iteration #5548  Loss: 0.48837343982986503; l2 norm of gradients: 0.017592321038246504; l2 norm of weights: 0.42203975911570274\n","Iteration #5549  Loss: 0.48837016035581843; l2 norm of gradients: 0.017574273814647316; l2 norm of weights: 0.4220350274767991\n","Iteration #5550  Loss: 0.4883668825071622; l2 norm of gradients: 0.017556246765631757; l2 norm of weights: 0.4220302993352864\n","Iteration #5551  Loss: 0.4883636062822388; l2 norm of gradients: 0.01753823986932155; l2 norm of weights: 0.42202557468631896\n","Iteration #5552  Loss: 0.488360331679394; l2 norm of gradients: 0.01752025310386248; l2 norm of weights: 0.4220208535250598\n","Iteration #5553  Loss: 0.4883570586969762; l2 norm of gradients: 0.01750228644742429; l2 norm of weights: 0.4220161358466802\n","Iteration #5554  Loss: 0.48835378733333684; l2 norm of gradients: 0.01748433987820072; l2 norm of weights: 0.42201142164636024\n","Iteration #5555  Loss: 0.4883505175868306; l2 norm of gradients: 0.017466413374409444; l2 norm of weights: 0.4220067109192884\n","Iteration #5556  Loss: 0.4883472494558149; l2 norm of gradients: 0.017448506914292054; l2 norm of weights: 0.4220020036606617\n","Iteration #5557  Loss: 0.4883439829386502; l2 norm of gradients: 0.017430620476114; l2 norm of weights: 0.4219972998656857\n","Iteration #5558  Loss: 0.48834071803370005; l2 norm of gradients: 0.01741275403816462; l2 norm of weights: 0.4219925995295743\n","Iteration #5559  Loss: 0.4883374547393308; l2 norm of gradients: 0.017394907578757064; l2 norm of weights: 0.42198790264754993\n","Iteration #5560  Loss: 0.4883341930539118; l2 norm of gradients: 0.017377081076228307; l2 norm of weights: 0.42198320921484345\n","Iteration #5561  Loss: 0.48833093297581515; l2 norm of gradients: 0.01735927450893909; l2 norm of weights: 0.4219785192266942\n","Iteration #5562  Loss: 0.4883276745034164; l2 norm of gradients: 0.01734148785527392; l2 norm of weights: 0.42197383267834987\n","Iteration #5563  Loss: 0.4883244176350937; l2 norm of gradients: 0.017323721093640988; l2 norm of weights: 0.4219691495650665\n","Iteration #5564  Loss: 0.48832116236922807; l2 norm of gradients: 0.017305974202472253; l2 norm of weights: 0.4219644698821085\n","Iteration #5565  Loss: 0.4883179087042035; l2 norm of gradients: 0.01728824716022329; l2 norm of weights: 0.42195979362474884\n","Iteration #5566  Loss: 0.488314656638407; l2 norm of gradients: 0.01727053994537334; l2 norm of weights: 0.42195512078826847\n","Iteration #5567  Loss: 0.4883114061702285; l2 norm of gradients: 0.017252852536425257; l2 norm of weights: 0.42195045136795706\n","Iteration #5568  Loss: 0.48830815729806076; l2 norm of gradients: 0.017235184911905465; l2 norm of weights: 0.42194578535911237\n","Iteration #5569  Loss: 0.48830491002029947; l2 norm of gradients: 0.017217537050364006; l2 norm of weights: 0.4219411227570404\n","Iteration #5570  Loss: 0.48830166433534317; l2 norm of gradients: 0.017199908930374413; l2 norm of weights: 0.42193646355705555\n","Iteration #5571  Loss: 0.48829842024159326; l2 norm of gradients: 0.017182300530533774; l2 norm of weights: 0.4219318077544805\n","Iteration #5572  Loss: 0.48829517773745407; l2 norm of gradients: 0.0171647118294626; l2 norm of weights: 0.4219271553446461\n","Iteration #5573  Loss: 0.488291936821333; l2 norm of gradients: 0.01714714280580492; l2 norm of weights: 0.4219225063228914\n","Iteration #5574  Loss: 0.48828869749163983; l2 norm of gradients: 0.01712959343822817; l2 norm of weights: 0.4219178606845638\n","Iteration #5575  Loss: 0.4882854597467878; l2 norm of gradients: 0.01711206370542322; l2 norm of weights: 0.4219132184250188\n","Iteration #5576  Loss: 0.48828222358519235; l2 norm of gradients: 0.01709455358610424; l2 norm of weights: 0.42190857953962\n","Iteration #5577  Loss: 0.48827898900527267; l2 norm of gradients: 0.01707706305900886; l2 norm of weights: 0.4219039440237395\n","Iteration #5578  Loss: 0.4882757560054497; l2 norm of gradients: 0.017059592102897966; l2 norm of weights: 0.421899311872757\n","Iteration #5579  Loss: 0.48827252458414816; l2 norm of gradients: 0.017042140696555776; l2 norm of weights: 0.42189468308206096\n","Iteration #5580  Loss: 0.4882692947397949; l2 norm of gradients: 0.017024708818789765; l2 norm of weights: 0.42189005764704746\n","Iteration #5581  Loss: 0.4882660664708203; l2 norm of gradients: 0.017007296448430683; l2 norm of weights: 0.421885435563121\n","Iteration #5582  Loss: 0.48826283977565677; l2 norm of gradients: 0.01698990356433244; l2 norm of weights: 0.42188081682569406\n","Iteration #5583  Loss: 0.48825961465274015; l2 norm of gradients: 0.016972530145372207; l2 norm of weights: 0.42187620143018706\n","Iteration #5584  Loss: 0.4882563911005088; l2 norm of gradients: 0.016955176170450326; l2 norm of weights: 0.4218715893720287\n","Iteration #5585  Loss: 0.488253169117404; l2 norm of gradients: 0.01693784161849023; l2 norm of weights: 0.4218669806466556\n","Iteration #5586  Loss: 0.4882499487018699; l2 norm of gradients: 0.01692052646843851; l2 norm of weights: 0.42186237524951253\n","Iteration #5587  Loss: 0.48824672985235296; l2 norm of gradients: 0.01690323069926483; l2 norm of weights: 0.421857773176052\n","Iteration #5588  Loss: 0.48824351256730314; l2 norm of gradients: 0.016885954289961935; l2 norm of weights: 0.4218531744217348\n","Iteration #5589  Loss: 0.4882402968451727; l2 norm of gradients: 0.0168686972195456; l2 norm of weights: 0.42184857898202965\n","Iteration #5590  Loss: 0.4882370826844168; l2 norm of gradients: 0.01685145946705461; l2 norm of weights: 0.42184398685241314\n","Iteration #5591  Loss: 0.4882338700834933; l2 norm of gradients: 0.01683424101155076; l2 norm of weights: 0.4218393980283698\n","Iteration #5592  Loss: 0.48823065904086294; l2 norm of gradients: 0.016817041832118768; l2 norm of weights: 0.42183481250539223\n","Iteration #5593  Loss: 0.4882274495549892; l2 norm of gradients: 0.01679986190786632; l2 norm of weights: 0.4218302302789808\n","Iteration #5594  Loss: 0.48822424162433825; l2 norm of gradients: 0.01678270121792401; l2 norm of weights: 0.42182565134464395\n","Iteration #5595  Loss: 0.488221035247379; l2 norm of gradients: 0.016765559741445276; l2 norm of weights: 0.42182107569789784\n","Iteration #5596  Loss: 0.48821783042258315; l2 norm of gradients: 0.016748437457606475; l2 norm of weights: 0.42181650333426657\n","Iteration #5597  Loss: 0.48821462714842506; l2 norm of gradients: 0.016731334345606752; l2 norm of weights: 0.42181193424928215\n","Iteration #5598  Loss: 0.48821142542338186; l2 norm of gradients: 0.01671425038466808; l2 norm of weights: 0.42180736843848443\n","Iteration #5599  Loss: 0.48820822524593355; l2 norm of gradients: 0.016697185554035213; l2 norm of weights: 0.42180280589742103\n","Iteration #5600  Loss: 0.48820502661456233; l2 norm of gradients: 0.01668013983297562; l2 norm of weights: 0.4217982466216474\n","Iteration #5601  Loss: 0.4882018295277539; l2 norm of gradients: 0.01666311320077956; l2 norm of weights: 0.42179369060672683\n","Iteration #5602  Loss: 0.48819863398399604; l2 norm of gradients: 0.016646105636759972; l2 norm of weights: 0.4217891378482304\n","Iteration #5603  Loss: 0.48819543998177933; l2 norm of gradients: 0.01662911712025247; l2 norm of weights: 0.42178458834173677\n","Iteration #5604  Loss: 0.48819224751959733; l2 norm of gradients: 0.016612147630615304; l2 norm of weights: 0.42178004208283265\n","Iteration #5605  Loss: 0.48818905659594597; l2 norm of gradients: 0.01659519714722937; l2 norm of weights: 0.4217754990671124\n","Iteration #5606  Loss: 0.4881858672093239; l2 norm of gradients: 0.016578265649498165; l2 norm of weights: 0.42177095929017794\n","Iteration #5607  Loss: 0.4881826793582326; l2 norm of gradients: 0.016561353116847782; l2 norm of weights: 0.4217664227476391\n","Iteration #5608  Loss: 0.4881794930411763; l2 norm of gradients: 0.016544459528726824; l2 norm of weights: 0.4217618894351133\n","Iteration #5609  Loss: 0.48817630825666153; l2 norm of gradients: 0.016527584864606423; l2 norm of weights: 0.4217573593482257\n","Iteration #5610  Loss: 0.4881731250031977; l2 norm of gradients: 0.016510729103980257; l2 norm of weights: 0.4217528324826091\n","Iteration #5611  Loss: 0.488169943279297; l2 norm of gradients: 0.016493892226364402; l2 norm of weights: 0.42174830883390385\n","Iteration #5612  Loss: 0.4881667630834739; l2 norm of gradients: 0.01647707421129746; l2 norm of weights: 0.42174378839775817\n","Iteration #5613  Loss: 0.488163584414246; l2 norm of gradients: 0.01646027503834043; l2 norm of weights: 0.4217392711698277\n","Iteration #5614  Loss: 0.4881604072701331; l2 norm of gradients: 0.01644349468707668; l2 norm of weights: 0.4217347571457757\n","Iteration #5615  Loss: 0.48815723164965774; l2 norm of gradients: 0.016426733137112; l2 norm of weights: 0.4217302463212732\n","Iteration #5616  Loss: 0.4881540575513453; l2 norm of gradients: 0.016409990368074476; l2 norm of weights: 0.4217257386919986\n","Iteration #5617  Loss: 0.4881508849737235; l2 norm of gradients: 0.016393266359614554; l2 norm of weights: 0.421721234253638\n","Iteration #5618  Loss: 0.48814771391532275; l2 norm of gradients: 0.016376561091404947; l2 norm of weights: 0.42171673300188495\n","Iteration #5619  Loss: 0.48814454437467625; l2 norm of gradients: 0.016359874543140685; l2 norm of weights: 0.4217122349324407\n","Iteration #5620  Loss: 0.4881413763503196; l2 norm of gradients: 0.016343206694539005; l2 norm of weights: 0.42170774004101386\n","Iteration #5621  Loss: 0.4881382098407911; l2 norm of gradients: 0.016326557525339375; l2 norm of weights: 0.4217032483233205\n","Iteration #5622  Loss: 0.48813504484463144; l2 norm of gradients: 0.016309927015303493; l2 norm of weights: 0.4216987597750845\n","Iteration #5623  Loss: 0.4881318813603843; l2 norm of gradients: 0.01629331514421514; l2 norm of weights: 0.42169427439203677\n","Iteration #5624  Loss: 0.4881287193865954; l2 norm of gradients: 0.01627672189188036; l2 norm of weights: 0.42168979216991603\n","Iteration #5625  Loss: 0.48812555892181353; l2 norm of gradients: 0.016260147238127232; l2 norm of weights: 0.4216853131044684\n","Iteration #5626  Loss: 0.4881223999645898; l2 norm of gradients: 0.016243591162805982; l2 norm of weights: 0.42168083719144717\n","Iteration #5627  Loss: 0.4881192425134779; l2 norm of gradients: 0.016227053645788848; l2 norm of weights: 0.4216763644266135\n","Iteration #5628  Loss: 0.4881160865670341; l2 norm of gradients: 0.016210534666970196; l2 norm of weights: 0.42167189480573547\n","Iteration #5629  Loss: 0.48811293212381723; l2 norm of gradients: 0.016194034206266333; l2 norm of weights: 0.4216674283245889\n","Iteration #5630  Loss: 0.4881097791823887; l2 norm of gradients: 0.016177552243615636; l2 norm of weights: 0.42166296497895694\n","Iteration #5631  Loss: 0.4881066277413124; l2 norm of gradients: 0.01616108875897838; l2 norm of weights: 0.42165850476463007\n","Iteration #5632  Loss: 0.48810347779915497; l2 norm of gradients: 0.01614464373233684; l2 norm of weights: 0.4216540476774059\n","Iteration #5633  Loss: 0.4881003293544849; l2 norm of gradients: 0.016128217143695212; l2 norm of weights: 0.4216495937130897\n","Iteration #5634  Loss: 0.4880971824058742; l2 norm of gradients: 0.01611180897307956; l2 norm of weights: 0.4216451428674939\n","Iteration #5635  Loss: 0.4880940369518967; l2 norm of gradients: 0.01609541920053783; l2 norm of weights: 0.4216406951364383\n","Iteration #5636  Loss: 0.4880908929911289; l2 norm of gradients: 0.016079047806139828; l2 norm of weights: 0.42163625051574993\n","Iteration #5637  Loss: 0.4880877505221498; l2 norm of gradients: 0.016062694769977202; l2 norm of weights: 0.4216318090012632\n","Iteration #5638  Loss: 0.48808460954354127; l2 norm of gradients: 0.01604636007216336; l2 norm of weights: 0.42162737058881966\n","Iteration #5639  Loss: 0.488081470053887; l2 norm of gradients: 0.016030043692833482; l2 norm of weights: 0.4216229352742683\n","Iteration #5640  Loss: 0.488078332051774; l2 norm of gradients: 0.016013745612144536; l2 norm of weights: 0.42161850305346515\n","Iteration #5641  Loss: 0.48807519553579065; l2 norm of gradients: 0.01599746581027517; l2 norm of weights: 0.4216140739222735\n","Iteration #5642  Loss: 0.488072060504529; l2 norm of gradients: 0.01598120426742579; l2 norm of weights: 0.4216096478765639\n","Iteration #5643  Loss: 0.4880689269565828; l2 norm of gradients: 0.015964960963818404; l2 norm of weights: 0.4216052249122142\n","Iteration #5644  Loss: 0.4880657948905487; l2 norm of gradients: 0.01594873587969673; l2 norm of weights: 0.42160080502510916\n","Iteration #5645  Loss: 0.48806266430502543; l2 norm of gradients: 0.015932528995326087; l2 norm of weights: 0.421596388211141\n","Iteration #5646  Loss: 0.48805953519861467; l2 norm of gradients: 0.015916340290993432; l2 norm of weights: 0.421591974466209\n","Iteration #5647  Loss: 0.48805640756991997; l2 norm of gradients: 0.015900169747007212; l2 norm of weights: 0.42158756378621937\n","Iteration #5648  Loss: 0.4880532814175479; l2 norm of gradients: 0.01588401734369753; l2 norm of weights: 0.42158315616708586\n","Iteration #5649  Loss: 0.48805015674010704; l2 norm of gradients: 0.015867883061415962; l2 norm of weights: 0.421578751604729\n","Iteration #5650  Loss: 0.48804703353620865; l2 norm of gradients: 0.01585176688053557; l2 norm of weights: 0.4215743500950766\n","Iteration #5651  Loss: 0.4880439118044663; l2 norm of gradients: 0.01583566878145097; l2 norm of weights: 0.42156995163406336\n","Iteration #5652  Loss: 0.4880407915434963; l2 norm of gradients: 0.015819588744578176; l2 norm of weights: 0.4215655562176313\n","Iteration #5653  Loss: 0.48803767275191684; l2 norm of gradients: 0.015803526750354634; l2 norm of weights: 0.4215611638417294\n","Iteration #5654  Loss: 0.488034555428349; l2 norm of gradients: 0.015787482779239234; l2 norm of weights: 0.42155677450231355\n","Iteration #5655  Loss: 0.48803143957141604; l2 norm of gradients: 0.015771456811712225; l2 norm of weights: 0.421552388195347\n","Iteration #5656  Loss: 0.4880283251797437; l2 norm of gradients: 0.015755448828275193; l2 norm of weights: 0.4215480049167996\n","Iteration #5657  Loss: 0.48802521225196016; l2 norm of gradients: 0.015739458809451105; l2 norm of weights: 0.4215436246626486\n","Iteration #5658  Loss: 0.48802210078669594; l2 norm of gradients: 0.0157234867357842; l2 norm of weights: 0.42153924742887805\n","Iteration #5659  Loss: 0.48801899078258404; l2 norm of gradients: 0.015707532587840037; l2 norm of weights: 0.42153487321147887\n","Iteration #5660  Loss: 0.4880158822382598; l2 norm of gradients: 0.01569159634620541; l2 norm of weights: 0.4215305020064493\n","Iteration #5661  Loss: 0.4880127751523608; l2 norm of gradients: 0.015675677991488376; l2 norm of weights: 0.4215261338097941\n","Iteration #5662  Loss: 0.48800966952352726; l2 norm of gradients: 0.015659777504318167; l2 norm of weights: 0.4215217686175253\n","Iteration #5663  Loss: 0.48800656535040166; l2 norm of gradients: 0.015643894865345252; l2 norm of weights: 0.4215174064256619\n","Iteration #5664  Loss: 0.48800346263162864; l2 norm of gradients: 0.01562803005524124; l2 norm of weights: 0.42151304723022937\n","Iteration #5665  Loss: 0.4880003613658558; l2 norm of gradients: 0.015612183054698886; l2 norm of weights: 0.4215086910272605\n","Iteration #5666  Loss: 0.4879972615517323; l2 norm of gradients: 0.015596353844432066; l2 norm of weights: 0.42150433781279495\n","Iteration #5667  Loss: 0.4879941631879103; l2 norm of gradients: 0.015580542405175778; l2 norm of weights: 0.421499987582879\n","Iteration #5668  Loss: 0.4879910662730439; l2 norm of gradients: 0.015564748717686022; l2 norm of weights: 0.42149564033356596\n","Iteration #5669  Loss: 0.48798797080578993; l2 norm of gradients: 0.015548972762739904; l2 norm of weights: 0.4214912960609161\n","Iteration #5670  Loss: 0.4879848767848072; l2 norm of gradients: 0.015533214521135538; l2 norm of weights: 0.42148695476099624\n","Iteration #5671  Loss: 0.48798178420875704; l2 norm of gradients: 0.015517473973692028; l2 norm of weights: 0.4214826164298803\n","Iteration #5672  Loss: 0.4879786930763031; l2 norm of gradients: 0.015501751101249473; l2 norm of weights: 0.4214782810636487\n","Iteration #5673  Loss: 0.48797560338611123; l2 norm of gradients: 0.015486045884668887; l2 norm of weights: 0.4214739486583891\n","Iteration #5674  Loss: 0.4879725151368498; l2 norm of gradients: 0.01547035830483228; l2 norm of weights: 0.4214696192101955\n","Iteration #5675  Loss: 0.4879694283271894; l2 norm of gradients: 0.015454688342642468; l2 norm of weights: 0.4214652927151691\n","Iteration #5676  Loss: 0.48796634295580293; l2 norm of gradients: 0.015439035979023241; l2 norm of weights: 0.42146096916941733\n","Iteration #5677  Loss: 0.4879632590213655; l2 norm of gradients: 0.015423401194919178; l2 norm of weights: 0.42145664856905474\n","Iteration #5678  Loss: 0.48796017652255474; l2 norm of gradients: 0.015407783971295753; l2 norm of weights: 0.4214523309102027\n","Iteration #5679  Loss: 0.48795709545805044; l2 norm of gradients: 0.015392184289139188; l2 norm of weights: 0.4214480161889889\n","Iteration #5680  Loss: 0.48795401582653447; l2 norm of gradients: 0.015376602129456535; l2 norm of weights: 0.42144370440154805\n","Iteration #5681  Loss: 0.4879509376266915; l2 norm of gradients: 0.015361037473275588; l2 norm of weights: 0.4214393955440216\n","Iteration #5682  Loss: 0.48794786085720804; l2 norm of gradients: 0.015345490301644898; l2 norm of weights: 0.4214350896125574\n","Iteration #5683  Loss: 0.487944785516773; l2 norm of gradients: 0.015329960595633704; l2 norm of weights: 0.4214307866033101\n","Iteration #5684  Loss: 0.4879417116040777; l2 norm of gradients: 0.01531444833633197; l2 norm of weights: 0.4214264865124412\n","Iteration #5685  Loss: 0.4879386391178156; l2 norm of gradients: 0.015298953504850284; l2 norm of weights: 0.4214221893361185\n","Iteration #5686  Loss: 0.4879355680566823; l2 norm of gradients: 0.015283476082319931; l2 norm of weights: 0.42141789507051663\n","Iteration #5687  Loss: 0.487932498419376; l2 norm of gradients: 0.015268016049892776; l2 norm of weights: 0.421413603711817\n","Iteration #5688  Loss: 0.4879294302045967; l2 norm of gradients: 0.015252573388741314; l2 norm of weights: 0.4214093152562072\n","Iteration #5689  Loss: 0.4879263634110471; l2 norm of gradients: 0.015237148080058598; l2 norm of weights: 0.42140502969988164\n","Iteration #5690  Loss: 0.48792329803743184; l2 norm of gradients: 0.01522174010505823; l2 norm of weights: 0.42140074703904146\n","Iteration #5691  Loss: 0.487920234082458; l2 norm of gradients: 0.015206349444974347; l2 norm of weights: 0.4213964672698942\n","Iteration #5692  Loss: 0.4879171715448347; l2 norm of gradients: 0.015190976081061583; l2 norm of weights: 0.421392190388654\n","Iteration #5693  Loss: 0.4879141104232735; l2 norm of gradients: 0.015175619994595085; l2 norm of weights: 0.4213879163915415\n","Iteration #5694  Loss: 0.4879110507164878; l2 norm of gradients: 0.0151602811668704; l2 norm of weights: 0.4213836452747838\n","Iteration #5695  Loss: 0.4879079924231938; l2 norm of gradients: 0.015144959579203573; l2 norm of weights: 0.4213793770346147\n","Iteration #5696  Loss: 0.48790493554210945; l2 norm of gradients: 0.015129655212931055; l2 norm of weights: 0.4213751116672745\n","Iteration #5697  Loss: 0.4879018800719552; l2 norm of gradients: 0.015114368049409592; l2 norm of weights: 0.42137084916900985\n","Iteration #5698  Loss: 0.48789882601145335; l2 norm of gradients: 0.015099098070016446; l2 norm of weights: 0.4213665895360739\n","Iteration #5699  Loss: 0.4878957733593287; l2 norm of gradients: 0.015083845256149106; l2 norm of weights: 0.4213623327647264\n","Iteration #5700  Loss: 0.48789272211430806; l2 norm of gradients: 0.015068609589225429; l2 norm of weights: 0.4213580788512333\n","Iteration #5701  Loss: 0.4878896722751209; l2 norm of gradients: 0.015053391050683538; l2 norm of weights: 0.42135382779186736\n","Iteration #5702  Loss: 0.48788662384049825; l2 norm of gradients: 0.015038189621981883; l2 norm of weights: 0.42134957958290753\n","Iteration #5703  Loss: 0.4878835768091736; l2 norm of gradients: 0.01502300528459911; l2 norm of weights: 0.42134533422063913\n","Iteration #5704  Loss: 0.48788053117988267; l2 norm of gradients: 0.015007838020034144; l2 norm of weights: 0.42134109170135414\n","Iteration #5705  Loss: 0.48787748695136324; l2 norm of gradients: 0.014992687809806055; l2 norm of weights: 0.4213368520213507\n","Iteration #5706  Loss: 0.48787444412235537; l2 norm of gradients: 0.01497755463545414; l2 norm of weights: 0.4213326151769334\n","Iteration #5707  Loss: 0.48787140269160106; l2 norm of gradients: 0.01496243847853785; l2 norm of weights: 0.42132838116441307\n","Iteration #5708  Loss: 0.48786836265784495; l2 norm of gradients: 0.014947339320636747; l2 norm of weights: 0.4213241499801074\n","Iteration #5709  Loss: 0.48786532401983324; l2 norm of gradients: 0.014932257143350542; l2 norm of weights: 0.42131992162033977\n","Iteration #5710  Loss: 0.4878622867763147; l2 norm of gradients: 0.01491719192829901; l2 norm of weights: 0.42131569608144037\n","Iteration #5711  Loss: 0.48785925092604; l2 norm of gradients: 0.014902143657122006; l2 norm of weights: 0.4213114733597454\n","Iteration #5712  Loss: 0.4878562164677621; l2 norm of gradients: 0.01488711231147942; l2 norm of weights: 0.4213072534515976\n","Iteration #5713  Loss: 0.4878531834002362; l2 norm of gradients: 0.014872097873051182; l2 norm of weights: 0.4213030363533459\n","Iteration #5714  Loss: 0.48785015172221924; l2 norm of gradients: 0.014857100323537183; l2 norm of weights: 0.42129882206134556\n","Iteration #5715  Loss: 0.4878471214324707; l2 norm of gradients: 0.01484211964465733; l2 norm of weights: 0.421294610571958\n","Iteration #5716  Loss: 0.487844092529752; l2 norm of gradients: 0.014827155818151496; l2 norm of weights: 0.42129040188155104\n","Iteration #5717  Loss: 0.4878410650128268; l2 norm of gradients: 0.014812208825779426; l2 norm of weights: 0.42128619598649875\n","Iteration #5718  Loss: 0.48783803888046046; l2 norm of gradients: 0.014797278649320825; l2 norm of weights: 0.4212819928831813\n","Iteration #5719  Loss: 0.48783501413142116; l2 norm of gradients: 0.01478236527057526; l2 norm of weights: 0.42127779256798537\n","Iteration #5720  Loss: 0.48783199076447853; l2 norm of gradients: 0.0147674686713622; l2 norm of weights: 0.4212735950373035\n","Iteration #5721  Loss: 0.48782896877840465; l2 norm of gradients: 0.014752588833520876; l2 norm of weights: 0.42126940028753457\n","Iteration #5722  Loss: 0.4878259481719737; l2 norm of gradients: 0.01473772573891041; l2 norm of weights: 0.42126520831508385\n","Iteration #5723  Loss: 0.4878229289439617; l2 norm of gradients: 0.01472287936940971; l2 norm of weights: 0.4212610191163625\n","Iteration #5724  Loss: 0.48781991109314693; l2 norm of gradients: 0.01470804970691743; l2 norm of weights: 0.421256832687788\n","Iteration #5725  Loss: 0.48781689461830985; l2 norm of gradients: 0.014693236733351966; l2 norm of weights: 0.42125264902578397\n","Iteration #5726  Loss: 0.48781387951823285; l2 norm of gradients: 0.014678440430651495; l2 norm of weights: 0.42124846812678013\n","Iteration #5727  Loss: 0.4878108657917004; l2 norm of gradients: 0.014663660780773845; l2 norm of weights: 0.42124428998721236\n","Iteration #5728  Loss: 0.4878078534374992; l2 norm of gradients: 0.014648897765696577; l2 norm of weights: 0.4212401146035227\n","Iteration #5729  Loss: 0.48780484245441785; l2 norm of gradients: 0.014634151367416868; l2 norm of weights: 0.42123594197215936\n","Iteration #5730  Loss: 0.487801832841247; l2 norm of gradients: 0.014619421567951582; l2 norm of weights: 0.4212317720895764\n","Iteration #5731  Loss: 0.48779882459677937; l2 norm of gradients: 0.014604708349337144; l2 norm of weights: 0.4212276049522342\n","Iteration #5732  Loss: 0.48779581771980995; l2 norm of gradients: 0.0145900116936296; l2 norm of weights: 0.4212234405565991\n","Iteration #5733  Loss: 0.4877928122091356; l2 norm of gradients: 0.014575331582904624; l2 norm of weights: 0.4212192788991437\n","Iteration #5734  Loss: 0.4877898080635551; l2 norm of gradients: 0.014560667999257324; l2 norm of weights: 0.4212151199763463\n","Iteration #5735  Loss: 0.4877868052818694; l2 norm of gradients: 0.014546020924802417; l2 norm of weights: 0.42121096378469164\n","Iteration #5736  Loss: 0.48778380386288167; l2 norm of gradients: 0.01453139034167411; l2 norm of weights: 0.42120681032067\n","Iteration #5737  Loss: 0.48778080380539685; l2 norm of gradients: 0.014516776232026104; l2 norm of weights: 0.4212026595807783\n","Iteration #5738  Loss: 0.48777780510822183; l2 norm of gradients: 0.014502178578031518; l2 norm of weights: 0.42119851156151883\n","Iteration #5739  Loss: 0.487774807770166; l2 norm of gradients: 0.014487597361882954; l2 norm of weights: 0.42119436625940043\n","Iteration #5740  Loss: 0.4877718117900402; l2 norm of gradients: 0.014473032565792432; l2 norm of weights: 0.42119022367093745\n","Iteration #5741  Loss: 0.4877688171666576; l2 norm of gradients: 0.014458484171991332; l2 norm of weights: 0.42118608379265066\n","Iteration #5742  Loss: 0.4877658238988334; l2 norm of gradients: 0.014443952162730435; l2 norm of weights: 0.4211819466210665\n","Iteration #5743  Loss: 0.4877628319853847; l2 norm of gradients: 0.01442943652027983; l2 norm of weights: 0.42117781215271743\n","Iteration #5744  Loss: 0.4877598414251306; l2 norm of gradients: 0.014414937226929039; l2 norm of weights: 0.4211736803841417\n","Iteration #5745  Loss: 0.48775685221689213; l2 norm of gradients: 0.014400454264986762; l2 norm of weights: 0.42116955131188377\n","Iteration #5746  Loss: 0.48775386435949275; l2 norm of gradients: 0.014385987616781067; l2 norm of weights: 0.4211654249324939\n","Iteration #5747  Loss: 0.4877508778517571; l2 norm of gradients: 0.014371537264659229; l2 norm of weights: 0.4211613012425282\n","Iteration #5748  Loss: 0.4877478926925126; l2 norm of gradients: 0.014357103190987857; l2 norm of weights: 0.42115718023854876\n","Iteration #5749  Loss: 0.4877449088805882; l2 norm of gradients: 0.014342685378152644; l2 norm of weights: 0.4211530619171233\n","Iteration #5750  Loss: 0.4877419264148151; l2 norm of gradients: 0.014328283808558604; l2 norm of weights: 0.42114894627482596\n","Iteration #5751  Loss: 0.4877389452940262; l2 norm of gradients: 0.014313898464629866; l2 norm of weights: 0.4211448333082361\n","Iteration #5752  Loss: 0.48773596551705645; l2 norm of gradients: 0.014299529328809698; l2 norm of weights: 0.42114072301393934\n","Iteration #5753  Loss: 0.48773298708274304; l2 norm of gradients: 0.014285176383560557; l2 norm of weights: 0.42113661538852704\n","Iteration #5754  Loss: 0.48773000998992466; l2 norm of gradients: 0.014270839611363972; l2 norm of weights: 0.4211325104285964\n","Iteration #5755  Loss: 0.4877270342374423; l2 norm of gradients: 0.014256518994720552; l2 norm of weights: 0.4211284081307503\n","Iteration #5756  Loss: 0.4877240598241388; l2 norm of gradients: 0.014242214516149989; l2 norm of weights: 0.42112430849159765\n","Iteration #5757  Loss: 0.48772108674885883; l2 norm of gradients: 0.014227926158190999; l2 norm of weights: 0.421120211507753\n","Iteration #5758  Loss: 0.48771811501044937; l2 norm of gradients: 0.014213653903401376; l2 norm of weights: 0.4211161171758367\n","Iteration #5759  Loss: 0.48771514460775867; l2 norm of gradients: 0.014199397734357867; l2 norm of weights: 0.42111202549247484\n","Iteration #5760  Loss: 0.48771217553963775; l2 norm of gradients: 0.014185157633656192; l2 norm of weights: 0.4211079364542995\n","Iteration #5761  Loss: 0.48770920780493887; l2 norm of gradients: 0.014170933583911069; l2 norm of weights: 0.42110385005794815\n","Iteration #5762  Loss: 0.48770624140251667; l2 norm of gradients: 0.014156725567756117; l2 norm of weights: 0.42109976630006435\n","Iteration #5763  Loss: 0.4877032763312274; l2 norm of gradients: 0.0141425335678439; l2 norm of weights: 0.42109568517729706\n","Iteration #5764  Loss: 0.4877003125899295; l2 norm of gradients: 0.014128357566845842; l2 norm of weights: 0.4210916066863013\n","Iteration #5765  Loss: 0.48769735017748317; l2 norm of gradients: 0.014114197547452257; l2 norm of weights: 0.4210875308237375\n","Iteration #5766  Loss: 0.4876943890927504; l2 norm of gradients: 0.014100053492372328; l2 norm of weights: 0.4210834575862719\n","Iteration #5767  Loss: 0.48769142933459525; l2 norm of gradients: 0.014085925384334046; l2 norm of weights: 0.4210793869705765\n","Iteration #5768  Loss: 0.4876884709018839; l2 norm of gradients: 0.014071813206084194; l2 norm of weights: 0.4210753189733289\n","Iteration #5769  Loss: 0.48768551379348396; l2 norm of gradients: 0.014057716940388373; l2 norm of weights: 0.4210712535912124\n","Iteration #5770  Loss: 0.4876825580082653; l2 norm of gradients: 0.014043636570030923; l2 norm of weights: 0.4210671908209158\n","Iteration #5771  Loss: 0.4876796035450994; l2 norm of gradients: 0.014029572077814969; l2 norm of weights: 0.42106313065913387\n","Iteration #5772  Loss: 0.4876766504028599; l2 norm of gradients: 0.014015523446562287; l2 norm of weights: 0.4210590731025666\n","Iteration #5773  Loss: 0.4876736985804224; l2 norm of gradients: 0.014001490659113441; l2 norm of weights: 0.42105501814792\n","Iteration #5774  Loss: 0.4876707480766638; l2 norm of gradients: 0.013987473698327584; l2 norm of weights: 0.4210509657919054\n","Iteration #5775  Loss: 0.48766779889046347; l2 norm of gradients: 0.0139734725470826; l2 norm of weights: 0.4210469160312398\n","Iteration #5776  Loss: 0.4876648510207024; l2 norm of gradients: 0.013959487188274977; l2 norm of weights: 0.4210428688626459\n","Iteration #5777  Loss: 0.4876619044662635; l2 norm of gradients: 0.01394551760481983; l2 norm of weights: 0.4210388242828519\n","Iteration #5778  Loss: 0.48765895922603153; l2 norm of gradients: 0.013931563779650847; l2 norm of weights: 0.4210347822885916\n","Iteration #5779  Loss: 0.4876560152988933; l2 norm of gradients: 0.013917625695720307; l2 norm of weights: 0.4210307428766043\n","Iteration #5780  Loss: 0.4876530726837369; l2 norm of gradients: 0.013903703335999051; l2 norm of weights: 0.4210267060436349\n","Iteration #5781  Loss: 0.48765013137945296; l2 norm of gradients: 0.013889796683476431; l2 norm of weights: 0.4210226717864337\n","Iteration #5782  Loss: 0.4876471913849336; l2 norm of gradients: 0.013875905721160313; l2 norm of weights: 0.42101864010175677\n","Iteration #5783  Loss: 0.4876442526990729; l2 norm of gradients: 0.013862030432077063; l2 norm of weights: 0.42101461098636556\n","Iteration #5784  Loss: 0.48764131532076666; l2 norm of gradients: 0.013848170799271496; l2 norm of weights: 0.42101058443702705\n","Iteration #5785  Loss: 0.4876383792489127; l2 norm of gradients: 0.013834326805806886; l2 norm of weights: 0.4210065604505136\n","Iteration #5786  Loss: 0.48763544448241036; l2 norm of gradients: 0.013820498434764952; l2 norm of weights: 0.42100253902360324\n","Iteration #5787  Loss: 0.48763251102016125; l2 norm of gradients: 0.013806685669245755; l2 norm of weights: 0.42099852015307937\n","Iteration #5788  Loss: 0.4876295788610683; l2 norm of gradients: 0.013792888492367802; l2 norm of weights: 0.4209945038357309\n","Iteration #5789  Loss: 0.48762664800403677; l2 norm of gradients: 0.013779106887267956; l2 norm of weights: 0.42099049006835215\n","Iteration #5790  Loss: 0.4876237184479734; l2 norm of gradients: 0.013765340837101374; l2 norm of weights: 0.420986478847743\n","Iteration #5791  Loss: 0.4876207901917868; l2 norm of gradients: 0.013751590325041588; l2 norm of weights: 0.42098247017070844\n","Iteration #5792  Loss: 0.48761786323438755; l2 norm of gradients: 0.013737855334280409; l2 norm of weights: 0.4209784640340593\n","Iteration #5793  Loss: 0.4876149375746878; l2 norm of gradients: 0.013724135848027938; l2 norm of weights: 0.42097446043461156\n","Iteration #5794  Loss: 0.4876120132116018; l2 norm of gradients: 0.0137104318495125; l2 norm of weights: 0.4209704593691868\n","Iteration #5795  Loss: 0.4876090901440452; l2 norm of gradients: 0.01369674332198066; l2 norm of weights: 0.42096646083461164\n","Iteration #5796  Loss: 0.4876061683709358; l2 norm of gradients: 0.013683070248697266; l2 norm of weights: 0.4209624648277186\n","Iteration #5797  Loss: 0.48760324789119297; l2 norm of gradients: 0.013669412612945287; l2 norm of weights: 0.42095847134534503\n","Iteration #5798  Loss: 0.48760032870373804; l2 norm of gradients: 0.013655770398025896; l2 norm of weights: 0.4209544803843341\n","Iteration #5799  Loss: 0.4875974108074941; l2 norm of gradients: 0.013642143587258414; l2 norm of weights: 0.4209504919415339\n","Iteration #5800  Loss: 0.4875944942013858; l2 norm of gradients: 0.013628532163980306; l2 norm of weights: 0.4209465060137984\n","Iteration #5801  Loss: 0.48759157888433985; l2 norm of gradients: 0.013614936111547156; l2 norm of weights: 0.42094252259798626\n","Iteration #5802  Loss: 0.4875886648552845; l2 norm of gradients: 0.0136013554133326; l2 norm of weights: 0.420938541690962\n","Iteration #5803  Loss: 0.4875857521131501; l2 norm of gradients: 0.013587790052728371; l2 norm of weights: 0.4209345632895953\n","Iteration #5804  Loss: 0.4875828406568683; l2 norm of gradients: 0.01357424001314428; l2 norm of weights: 0.42093058739076084\n","Iteration #5805  Loss: 0.48757993048537285; l2 norm of gradients: 0.01356070527800812; l2 norm of weights: 0.4209266139913391\n","Iteration #5806  Loss: 0.4875770215975992; l2 norm of gradients: 0.013547185830765739; l2 norm of weights: 0.42092264308821553\n","Iteration #5807  Loss: 0.4875741139924844; l2 norm of gradients: 0.013533681654880926; l2 norm of weights: 0.4209186746782808\n","Iteration #5808  Loss: 0.48757120766896767; l2 norm of gradients: 0.01352019273383547; l2 norm of weights: 0.420914708758431\n","Iteration #5809  Loss: 0.4875683026259894; l2 norm of gradients: 0.013506719051129103; l2 norm of weights: 0.4209107453255675\n","Iteration #5810  Loss: 0.48756539886249195; l2 norm of gradients: 0.013493260590279473; l2 norm of weights: 0.4209067843765967\n","Iteration #5811  Loss: 0.4875624963774199; l2 norm of gradients: 0.013479817334822158; l2 norm of weights: 0.4209028259084305\n","Iteration #5812  Loss: 0.4875595951697187; l2 norm of gradients: 0.01346638926831062; l2 norm of weights: 0.4208988699179858\n","Iteration #5813  Loss: 0.48755669523833606; l2 norm of gradients: 0.013452976374316136; l2 norm of weights: 0.42089491640218485\n","Iteration #5814  Loss: 0.48755379658222137; l2 norm of gradients: 0.013439578636427883; l2 norm of weights: 0.42089096535795495\n","Iteration #5815  Loss: 0.4875508992003259; l2 norm of gradients: 0.013426196038252874; l2 norm of weights: 0.4208870167822289\n","Iteration #5816  Loss: 0.4875480030916021; l2 norm of gradients: 0.013412828563415866; l2 norm of weights: 0.4208830706719443\n","Iteration #5817  Loss: 0.4875451082550048; l2 norm of gradients: 0.013399476195559465; l2 norm of weights: 0.4208791270240441\n","Iteration #5818  Loss: 0.4875422146894901; l2 norm of gradients: 0.013386138918343978; l2 norm of weights: 0.4208751858354766\n","Iteration #5819  Loss: 0.487539322394016; l2 norm of gradients: 0.013372816715447508; l2 norm of weights: 0.42087124710319496\n","Iteration #5820  Loss: 0.487536431367542; l2 norm of gradients: 0.013359509570565859; l2 norm of weights: 0.42086731082415757\n","Iteration #5821  Loss: 0.4875335416090296; l2 norm of gradients: 0.013346217467412552; l2 norm of weights: 0.420863376995328\n","Iteration #5822  Loss: 0.4875306531174418; l2 norm of gradients: 0.013332940389718769; l2 norm of weights: 0.4208594456136749\n","Iteration #5823  Loss: 0.4875277658917433; l2 norm of gradients: 0.013319678321233368; l2 norm of weights: 0.42085551667617216\n","Iteration #5824  Loss: 0.4875248799309007; l2 norm of gradients: 0.013306431245722842; l2 norm of weights: 0.42085159017979856\n","Iteration #5825  Loss: 0.4875219952338821; l2 norm of gradients: 0.013293199146971335; l2 norm of weights: 0.4208476661215383\n","Iteration #5826  Loss: 0.4875191117996572; l2 norm of gradients: 0.013279982008780541; l2 norm of weights: 0.42084374449838013\n","Iteration #5827  Loss: 0.4875162296271976; l2 norm of gradients: 0.013266779814969806; l2 norm of weights: 0.4208398253073185\n","Iteration #5828  Loss: 0.48751334871547647; l2 norm of gradients: 0.013253592549375953; l2 norm of weights: 0.4208359085453525\n","Iteration #5829  Loss: 0.4875104690634687; l2 norm of gradients: 0.01324042019585342; l2 norm of weights: 0.4208319942094865\n","Iteration #5830  Loss: 0.48750759067015076; l2 norm of gradients: 0.01322726273827413; l2 norm of weights: 0.4208280822967298\n","Iteration #5831  Loss: 0.48750471353450087; l2 norm of gradients: 0.01321412016052752; l2 norm of weights: 0.42082417280409673\n","Iteration #5832  Loss: 0.4875018376554989; l2 norm of gradients: 0.01320099244652051; l2 norm of weights: 0.4208202657286067\n","Iteration #5833  Loss: 0.4874989630321264; l2 norm of gradients: 0.013187879580177472; l2 norm of weights: 0.42081636106728426\n","Iteration #5834  Loss: 0.48749608966336655; l2 norm of gradients: 0.01317478154544021; l2 norm of weights: 0.4208124588171586\n","Iteration #5835  Loss: 0.48749321754820424; l2 norm of gradients: 0.013161698326267998; l2 norm of weights: 0.42080855897526437\n","Iteration #5836  Loss: 0.4874903466856259; l2 norm of gradients: 0.013148629906637451; l2 norm of weights: 0.42080466153864093\n","Iteration #5837  Loss: 0.48748747707461965; l2 norm of gradients: 0.013135576270542624; l2 norm of weights: 0.42080076650433257\n","Iteration #5838  Loss: 0.48748460871417537; l2 norm of gradients: 0.013122537401994882; l2 norm of weights: 0.4207968738693889\n","Iteration #5839  Loss: 0.4874817416032843; l2 norm of gradients: 0.013109513285022956; l2 norm of weights: 0.42079298363086415\n","Iteration #5840  Loss: 0.48747887574093984; l2 norm of gradients: 0.013096503903672905; l2 norm of weights: 0.4207890957858176\n","Iteration #5841  Loss: 0.4874760111261363; l2 norm of gradients: 0.013083509242008082; l2 norm of weights: 0.42078521033131344\n","Iteration #5842  Loss: 0.4874731477578703; l2 norm of gradients: 0.013070529284109143; l2 norm of weights: 0.420781327264421\n","Iteration #5843  Loss: 0.4874702856351397; l2 norm of gradients: 0.013057564014073937; l2 norm of weights: 0.42077744658221433\n","Iteration #5844  Loss: 0.4874674247569441; l2 norm of gradients: 0.01304461341601767; l2 norm of weights: 0.42077356828177237\n","Iteration #5845  Loss: 0.48746456512228453; l2 norm of gradients: 0.013031677474072655; l2 norm of weights: 0.42076969236017914\n","Iteration #5846  Loss: 0.48746170673016426; l2 norm of gradients: 0.0130187561723885; l2 norm of weights: 0.4207658188145236\n","Iteration #5847  Loss: 0.4874588495795872; l2 norm of gradients: 0.01300584949513195; l2 norm of weights: 0.42076194764189917\n","Iteration #5848  Loss: 0.4874559936695597; l2 norm of gradients: 0.012992957426486916; l2 norm of weights: 0.4207580788394047\n","Iteration #5849  Loss: 0.4874531389990894; l2 norm of gradients: 0.012980079950654496; l2 norm of weights: 0.42075421240414357\n","Iteration #5850  Loss: 0.48745028556718545; l2 norm of gradients: 0.012967217051852829; l2 norm of weights: 0.4207503483332241\n","Iteration #5851  Loss: 0.4874474333728589; l2 norm of gradients: 0.012954368714317235; l2 norm of weights: 0.4207464866237596\n","Iteration #5852  Loss: 0.4874445824151219; l2 norm of gradients: 0.012941534922300107; l2 norm of weights: 0.42074262727286793\n","Iteration #5853  Loss: 0.48744173269298874; l2 norm of gradients: 0.012928715660070868; l2 norm of weights: 0.42073877027767204\n","Iteration #5854  Loss: 0.487438884205475; l2 norm of gradients: 0.012915910911916028; l2 norm of weights: 0.42073491563529963\n","Iteration #5855  Loss: 0.487436036951598; l2 norm of gradients: 0.012903120662139098; l2 norm of weights: 0.4207310633428833\n","Iteration #5856  Loss: 0.4874331909303763; l2 norm of gradients: 0.012890344895060614; l2 norm of weights: 0.4207272133975602\n","Iteration #5857  Loss: 0.4874303461408306; l2 norm of gradients: 0.012877583595018092; l2 norm of weights: 0.4207233657964725\n","Iteration #5858  Loss: 0.48742750258198264; l2 norm of gradients: 0.01286483674636605; l2 norm of weights: 0.42071952053676726\n","Iteration #5859  Loss: 0.487424660252856; l2 norm of gradients: 0.01285210433347586; l2 norm of weights: 0.420715677615596\n","Iteration #5860  Loss: 0.48742181915247595; l2 norm of gradients: 0.012839386340735906; l2 norm of weights: 0.42071183703011517\n","Iteration #5861  Loss: 0.48741897927986905; l2 norm of gradients: 0.012826682752551485; l2 norm of weights: 0.42070799877748616\n","Iteration #5862  Loss: 0.48741614063406347; l2 norm of gradients: 0.012813993553344762; l2 norm of weights: 0.4207041628548747\n","Iteration #5863  Loss: 0.4874133032140894; l2 norm of gradients: 0.01280131872755477; l2 norm of weights: 0.42070032925945183\n","Iteration #5864  Loss: 0.4874104670189777; l2 norm of gradients: 0.012788658259637382; l2 norm of weights: 0.4206964979883928\n","Iteration #5865  Loss: 0.4874076320477615; l2 norm of gradients: 0.012776012134065328; l2 norm of weights: 0.4206926690388778\n","Iteration #5866  Loss: 0.48740479829947536; l2 norm of gradients: 0.012763380335328162; l2 norm of weights: 0.4206888424080918\n","Iteration #5867  Loss: 0.4874019657731552; l2 norm of gradients: 0.012750762847932213; l2 norm of weights: 0.42068501809322434\n","Iteration #5868  Loss: 0.48739913446783867; l2 norm of gradients: 0.012738159656400544; l2 norm of weights: 0.4206811960914698\n","Iteration #5869  Loss: 0.48739630438256487; l2 norm of gradients: 0.012725570745273099; l2 norm of weights: 0.4206773764000272\n","Iteration #5870  Loss: 0.48739347551637446; l2 norm of gradients: 0.01271299609910642; l2 norm of weights: 0.42067355901610026\n","Iteration #5871  Loss: 0.48739064786830955; l2 norm of gradients: 0.012700435702473816; l2 norm of weights: 0.4206697439368972\n","Iteration #5872  Loss: 0.48738782143741394; l2 norm of gradients: 0.012687889539965328; l2 norm of weights: 0.42066593115963125\n","Iteration #5873  Loss: 0.48738499622273274; l2 norm of gradients: 0.012675357596187667; l2 norm of weights: 0.4206621206815199\n","Iteration #5874  Loss: 0.4873821722233128; l2 norm of gradients: 0.012662839855764139; l2 norm of weights: 0.4206583124997856\n","Iteration #5875  Loss: 0.4873793494382025; l2 norm of gradients: 0.01265033630333479; l2 norm of weights: 0.4206545066116554\n","Iteration #5876  Loss: 0.48737652786645147; l2 norm of gradients: 0.012637846923556207; l2 norm of weights: 0.42065070301436064\n","Iteration #5877  Loss: 0.48737370750711123; l2 norm of gradients: 0.01262537170110162; l2 norm of weights: 0.4206469017051378\n","Iteration #5878  Loss: 0.4873708883592345; l2 norm of gradients: 0.012612910620660826; l2 norm of weights: 0.4206431026812277\n","Iteration #5879  Loss: 0.48736807042187574; l2 norm of gradients: 0.012600463666940186; l2 norm of weights: 0.42063930593987564\n","Iteration #5880  Loss: 0.48736525369409067; l2 norm of gradients: 0.012588030824662623; l2 norm of weights: 0.4206355114783318\n","Iteration #5881  Loss: 0.4873624381749368; l2 norm of gradients: 0.012575612078567585; l2 norm of weights: 0.42063171929385074\n","Iteration #5882  Loss: 0.487359623863473; l2 norm of gradients: 0.012563207413411022; l2 norm of weights: 0.42062792938369176\n","Iteration #5883  Loss: 0.4873568107587594; l2 norm of gradients: 0.012550816813965337; l2 norm of weights: 0.4206241417451186\n","Iteration #5884  Loss: 0.4873539988598581; l2 norm of gradients: 0.012538440265019483; l2 norm of weights: 0.42062035637539946\n","Iteration #5885  Loss: 0.48735118816583234; l2 norm of gradients: 0.012526077751378772; l2 norm of weights: 0.4206165732718074\n","Iteration #5886  Loss: 0.4873483786757471; l2 norm of gradients: 0.01251372925786502; l2 norm of weights: 0.42061279243161986\n","Iteration #5887  Loss: 0.4873455703886686; l2 norm of gradients: 0.01250139476931642; l2 norm of weights: 0.42060901385211885\n","Iteration #5888  Loss: 0.48734276330366455; l2 norm of gradients: 0.012489074270587592; l2 norm of weights: 0.42060523753059076\n","Iteration #5889  Loss: 0.48733995741980424; l2 norm of gradients: 0.012476767746549487; l2 norm of weights: 0.4206014634643266\n","Iteration #5890  Loss: 0.48733715273615874; l2 norm of gradients: 0.01246447518208944; l2 norm of weights: 0.42059769165062205\n","Iteration #5891  Loss: 0.48733434925179997; l2 norm of gradients: 0.012452196562111105; l2 norm of weights: 0.42059392208677704\n","Iteration #5892  Loss: 0.48733154696580183; l2 norm of gradients: 0.012439931871534476; l2 norm of weights: 0.42059015477009615\n","Iteration #5893  Loss: 0.4873287458772392; l2 norm of gradients: 0.012427681095295854; l2 norm of weights: 0.42058638969788836\n","Iteration #5894  Loss: 0.48732594598518897; l2 norm of gradients: 0.012415444218347779; l2 norm of weights: 0.4205826268674671\n","Iteration #5895  Loss: 0.4873231472887293; l2 norm of gradients: 0.012403221225659086; l2 norm of weights: 0.4205788662761506\n","Iteration #5896  Loss: 0.4873203497869395; l2 norm of gradients: 0.01239101210221485; l2 norm of weights: 0.420575107921261\n","Iteration #5897  Loss: 0.4873175534789007; l2 norm of gradients: 0.012378816833016398; l2 norm of weights: 0.42057135180012534\n","Iteration #5898  Loss: 0.48731475836369537; l2 norm of gradients: 0.012366635403081205; l2 norm of weights: 0.4205675979100749\n","Iteration #5899  Loss: 0.4873119644404074; l2 norm of gradients: 0.012354467797442955; l2 norm of weights: 0.42056384624844545\n","Iteration #5900  Loss: 0.48730917170812227; l2 norm of gradients: 0.012342314001151562; l2 norm of weights: 0.42056009681257717\n","Iteration #5901  Loss: 0.48730638016592664; l2 norm of gradients: 0.012330173999273006; l2 norm of weights: 0.42055634959981464\n","Iteration #5902  Loss: 0.4873035898129088; l2 norm of gradients: 0.012318047776889432; l2 norm of weights: 0.4205526046075068\n","Iteration #5903  Loss: 0.48730080064815817; l2 norm of gradients: 0.012305935319099114; l2 norm of weights: 0.42054886183300727\n","Iteration #5904  Loss: 0.48729801267076633; l2 norm of gradients: 0.012293836611016391; l2 norm of weights: 0.42054512127367377\n","Iteration #5905  Loss: 0.48729522587982554; l2 norm of gradients: 0.012281751637771718; l2 norm of weights: 0.42054138292686843\n","Iteration #5906  Loss: 0.48729244027442986; l2 norm of gradients: 0.01226968038451156; l2 norm of weights: 0.4205376467899579\n","Iteration #5907  Loss: 0.4872896558536745; l2 norm of gradients: 0.012257622836398478; l2 norm of weights: 0.42053391286031316\n","Iteration #5908  Loss: 0.4872868726166564; l2 norm of gradients: 0.012245578978611015; l2 norm of weights: 0.42053018113530943\n","Iteration #5909  Loss: 0.48728409056247396; l2 norm of gradients: 0.012233548796343712; l2 norm of weights: 0.42052645161232644\n","Iteration #5910  Loss: 0.48728130969022665; l2 norm of gradients: 0.01222153227480712; l2 norm of weights: 0.42052272428874815\n","Iteration #5911  Loss: 0.4872785299990154; l2 norm of gradients: 0.01220952939922773; l2 norm of weights: 0.4205189991619629\n","Iteration #5912  Loss: 0.48727575148794294; l2 norm of gradients: 0.012197540154848042; l2 norm of weights: 0.42051527622936347\n","Iteration #5913  Loss: 0.4872729741561131; l2 norm of gradients: 0.012185564526926403; l2 norm of weights: 0.4205115554883468\n","Iteration #5914  Loss: 0.48727019800263116; l2 norm of gradients: 0.012173602500737091; l2 norm of weights: 0.42050783693631416\n","Iteration #5915  Loss: 0.4872674230266036; l2 norm of gradients: 0.01216165406157035; l2 norm of weights: 0.42050412057067116\n","Iteration #5916  Loss: 0.4872646492271389; l2 norm of gradients: 0.012149719194732192; l2 norm of weights: 0.4205004063888278\n","Iteration #5917  Loss: 0.4872618766033461; l2 norm of gradients: 0.012137797885544548; l2 norm of weights: 0.4204966943881982\n","Iteration #5918  Loss: 0.48725910515433646; l2 norm of gradients: 0.012125890119345176; l2 norm of weights: 0.4204929845662008\n","Iteration #5919  Loss: 0.48725633487922204; l2 norm of gradients: 0.01211399588148765; l2 norm of weights: 0.4204892769202585\n","Iteration #5920  Loss: 0.4872535657771166; l2 norm of gradients: 0.012102115157341348; l2 norm of weights: 0.42048557144779825\n","Iteration #5921  Loss: 0.48725079784713504; l2 norm of gradients: 0.012090247932291456; l2 norm of weights: 0.4204818681462513\n","Iteration #5922  Loss: 0.487248031088394; l2 norm of gradients: 0.012078394191738855; l2 norm of weights: 0.4204781670130533\n","Iteration #5923  Loss: 0.4872452655000109; l2 norm of gradients: 0.01206655392110025; l2 norm of weights: 0.4204744680456438\n","Iteration #5924  Loss: 0.4872425010811053; l2 norm of gradients: 0.012054727105808034; l2 norm of weights: 0.420470771241467\n","Iteration #5925  Loss: 0.48723973783079755; l2 norm of gradients: 0.012042913731310298; l2 norm of weights: 0.42046707659797106\n","Iteration #5926  Loss: 0.48723697574820973; l2 norm of gradients: 0.012031113783070879; l2 norm of weights: 0.42046338411260836\n","Iteration #5927  Loss: 0.48723421483246493; l2 norm of gradients: 0.012019327246569239; l2 norm of weights: 0.42045969378283565\n","Iteration #5928  Loss: 0.48723145508268784; l2 norm of gradients: 0.012007554107300507; l2 norm of weights: 0.4204560056061138\n","Iteration #5929  Loss: 0.4872286964980045; l2 norm of gradients: 0.011995794350775463; l2 norm of weights: 0.4204523195799077\n","Iteration #5930  Loss: 0.48722593907754236; l2 norm of gradients: 0.011984047962520518; l2 norm of weights: 0.4204486357016868\n","Iteration #5931  Loss: 0.48722318282043003; l2 norm of gradients: 0.011972314928077633; l2 norm of weights: 0.42044495396892434\n","Iteration #5932  Loss: 0.48722042772579754; l2 norm of gradients: 0.011960595233004434; l2 norm of weights: 0.420441274379098\n","Iteration #5933  Loss: 0.4872176737927766; l2 norm of gradients: 0.011948888862874037; l2 norm of weights: 0.42043759692968957\n","Iteration #5934  Loss: 0.48721492102049974; l2 norm of gradients: 0.01193719580327515; l2 norm of weights: 0.42043392161818494\n","Iteration #5935  Loss: 0.487212169408101; l2 norm of gradients: 0.011925516039812015; l2 norm of weights: 0.4204302484420741\n","Iteration #5936  Loss: 0.4872094189547162; l2 norm of gradients: 0.011913849558104308; l2 norm of weights: 0.42042657739885125\n","Iteration #5937  Loss: 0.4872066696594818; l2 norm of gradients: 0.01190219634378735; l2 norm of weights: 0.4204229084860147\n","Iteration #5938  Loss: 0.487203921521536; l2 norm of gradients: 0.011890556382511798; l2 norm of weights: 0.420419241701067\n","Iteration #5939  Loss: 0.48720117454001854; l2 norm of gradients: 0.011878929659943843; l2 norm of weights: 0.42041557704151467\n","Iteration #5940  Loss: 0.4871984287140698; l2 norm of gradients: 0.011867316161765094; l2 norm of weights: 0.4204119145048683\n","Iteration #5941  Loss: 0.48719568404283237; l2 norm of gradients: 0.011855715873672578; l2 norm of weights: 0.42040825408864285\n","Iteration #5942  Loss: 0.4871929405254493; l2 norm of gradients: 0.01184412878137873; l2 norm of weights: 0.420404595790357\n","Iteration #5943  Loss: 0.4871901981610657; l2 norm of gradients: 0.011832554870611389; l2 norm of weights: 0.42040093960753383\n","Iteration #5944  Loss: 0.48718745694882737; l2 norm of gradients: 0.01182099412711375; l2 norm of weights: 0.4203972855377003\n","Iteration #5945  Loss: 0.487184716887882; l2 norm of gradients: 0.01180944653664438; l2 norm of weights: 0.4203936335783875\n","Iteration #5946  Loss: 0.4871819779773783; l2 norm of gradients: 0.01179791208497715; l2 norm of weights: 0.42038998372713077\n","Iteration #5947  Loss: 0.4871792402164659; l2 norm of gradients: 0.011786390757901263; l2 norm of weights: 0.4203863359814692\n","Iteration #5948  Loss: 0.48717650360429676; l2 norm of gradients: 0.011774882541221224; l2 norm of weights: 0.42038269033894604\n","Iteration #5949  Loss: 0.4871737681400232; l2 norm of gradients: 0.011763387420756808; l2 norm of weights: 0.42037904679710864\n","Iteration #5950  Loss: 0.4871710338227992; l2 norm of gradients: 0.011751905382343077; l2 norm of weights: 0.42037540535350837\n","Iteration #5951  Loss: 0.48716830065178013; l2 norm of gradients: 0.011740436411830334; l2 norm of weights: 0.4203717660057007\n","Iteration #5952  Loss: 0.4871655686261225; l2 norm of gradients: 0.011728980495084081; l2 norm of weights: 0.42036812875124474\n","Iteration #5953  Loss: 0.48716283774498415; l2 norm of gradients: 0.011717537617985035; l2 norm of weights: 0.4203644935877042\n","Iteration #5954  Loss: 0.48716010800752424; l2 norm of gradients: 0.011706107766429177; l2 norm of weights: 0.42036086051264643\n","Iteration #5955  Loss: 0.48715737941290327; l2 norm of gradients: 0.011694690926327573; l2 norm of weights: 0.42035722952364263\n","Iteration #5956  Loss: 0.48715465196028296; l2 norm of gradients: 0.011683287083606506; l2 norm of weights: 0.4203536006182684\n","Iteration #5957  Loss: 0.4871519256488263; l2 norm of gradients: 0.01167189622420735; l2 norm of weights: 0.420349973794103\n","Iteration #5958  Loss: 0.4871492004776976; l2 norm of gradients: 0.011660518334086665; l2 norm of weights: 0.4203463490487299\n","Iteration #5959  Loss: 0.48714647644606246; l2 norm of gradients: 0.011649153399216067; l2 norm of weights: 0.42034272637973624\n","Iteration #5960  Loss: 0.4871437535530877; l2 norm of gradients: 0.011637801405582283; l2 norm of weights: 0.4203391057847134\n","Iteration #5961  Loss: 0.48714103179794155; l2 norm of gradients: 0.011626462339187096; l2 norm of weights: 0.4203354872612567\n","Iteration #5962  Loss: 0.4871383111797932; l2 norm of gradients: 0.01161513618604738; l2 norm of weights: 0.4203318708069651\n","Iteration #5963  Loss: 0.4871355916978137; l2 norm of gradients: 0.011603822932195019; l2 norm of weights: 0.4203282564194419\n","Iteration #5964  Loss: 0.48713287335117456; l2 norm of gradients: 0.01159252256367687; l2 norm of weights: 0.420324644096294\n","Iteration #5965  Loss: 0.4871301561390493; l2 norm of gradients: 0.011581235066554877; l2 norm of weights: 0.4203210338351324\n","Iteration #5966  Loss: 0.4871274400606122; l2 norm of gradients: 0.011569960426905918; l2 norm of weights: 0.420317425633572\n","Iteration #5967  Loss: 0.4871247251150391; l2 norm of gradients: 0.011558698630821856; l2 norm of weights: 0.4203138194892315\n","Iteration #5968  Loss: 0.4871220113015069; l2 norm of gradients: 0.011547449664409503; l2 norm of weights: 0.42031021539973373\n","Iteration #5969  Loss: 0.48711929861919406; l2 norm of gradients: 0.011536213513790595; l2 norm of weights: 0.4203066133627052\n","Iteration #5970  Loss: 0.4871165870672798; l2 norm of gradients: 0.011524990165101771; l2 norm of weights: 0.4203030133757763\n","Iteration #5971  Loss: 0.48711387664494493; l2 norm of gradients: 0.011513779604494586; l2 norm of weights: 0.42029941543658145\n","Iteration #5972  Loss: 0.48711116735137155; l2 norm of gradients: 0.011502581818135491; l2 norm of weights: 0.4202958195427588\n","Iteration #5973  Loss: 0.4871084591857427; l2 norm of gradients: 0.011491396792205764; l2 norm of weights: 0.4202922256919505\n","Iteration #5974  Loss: 0.48710575214724294; l2 norm of gradients: 0.011480224512901574; l2 norm of weights: 0.42028863388180243\n","Iteration #5975  Loss: 0.4871030462350581; l2 norm of gradients: 0.011469064966433854; l2 norm of weights: 0.4202850441099645\n","Iteration #5976  Loss: 0.48710034144837494; l2 norm of gradients: 0.011457918139028413; l2 norm of weights: 0.42028145637409015\n","Iteration #5977  Loss: 0.48709763778638177; l2 norm of gradients: 0.011446784016925832; l2 norm of weights: 0.420277870671837\n","Iteration #5978  Loss: 0.4870949352482679; l2 norm of gradients: 0.011435662586381474; l2 norm of weights: 0.42027428700086616\n","Iteration #5979  Loss: 0.4870922338332241; l2 norm of gradients: 0.011424553833665417; l2 norm of weights: 0.4202707053588429\n","Iteration #5980  Loss: 0.48708953354044204; l2 norm of gradients: 0.011413457745062578; l2 norm of weights: 0.4202671257434362\n","Iteration #5981  Loss: 0.48708683436911493; l2 norm of gradients: 0.011402374306872515; l2 norm of weights: 0.42026354815231864\n","Iteration #5982  Loss: 0.4870841363184371; l2 norm of gradients: 0.011391303505409513; l2 norm of weights: 0.4202599725831669\n","Iteration #5983  Loss: 0.48708143938760406; l2 norm of gradients: 0.011380245327002585; l2 norm of weights: 0.42025639903366124\n","Iteration #5984  Loss: 0.4870787435758124; l2 norm of gradients: 0.011369199757995403; l2 norm of weights: 0.42025282750148574\n","Iteration #5985  Loss: 0.48707604888226036; l2 norm of gradients: 0.01135816678474627; l2 norm of weights: 0.42024925798432833\n","Iteration #5986  Loss: 0.4870733553061468; l2 norm of gradients: 0.011347146393628181; l2 norm of weights: 0.4202456904798807\n","Iteration #5987  Loss: 0.48707066284667233; l2 norm of gradients: 0.011336138571028703; l2 norm of weights: 0.4202421249858383\n","Iteration #5988  Loss: 0.4870679715030384; l2 norm of gradients: 0.011325143303350013; l2 norm of weights: 0.42023856149990035\n","Iteration #5989  Loss: 0.4870652812744479; l2 norm of gradients: 0.011314160577008944; l2 norm of weights: 0.42023500001976977\n","Iteration #5990  Loss: 0.4870625921601047; l2 norm of gradients: 0.011303190378436866; l2 norm of weights: 0.4202314405431532\n","Iteration #5991  Loss: 0.48705990415921413; l2 norm of gradients: 0.011292232694079649; l2 norm of weights: 0.42022788306776104\n","Iteration #5992  Loss: 0.4870572172709823; l2 norm of gradients: 0.011281287510397796; l2 norm of weights: 0.4202243275913077\n","Iteration #5993  Loss: 0.48705453149461714; l2 norm of gradients: 0.011270354813866281; l2 norm of weights: 0.4202207741115107\n","Iteration #5994  Loss: 0.4870518468293272; l2 norm of gradients: 0.011259434590974598; l2 norm of weights: 0.42021722262609196\n","Iteration #5995  Loss: 0.4870491632743224; l2 norm of gradients: 0.01124852682822672; l2 norm of weights: 0.4202136731327767\n","Iteration #5996  Loss: 0.48704648082881397; l2 norm of gradients: 0.011237631512141103; l2 norm of weights: 0.4202101256292938\n","Iteration #5997  Loss: 0.4870437994920144; l2 norm of gradients: 0.011226748629250657; l2 norm of weights: 0.42020658011337614\n","Iteration #5998  Loss: 0.48704111926313676; l2 norm of gradients: 0.01121587816610273; l2 norm of weights: 0.4202030365827599\n","Iteration #5999  Loss: 0.48703844014139613; l2 norm of gradients: 0.011205020109259105; l2 norm of weights: 0.42019949503518556\n","Iteration #6000  Loss: 0.48703576212600813; l2 norm of gradients: 0.011194174445295938; l2 norm of weights: 0.4201959554683965\n","Iteration #6001  Loss: 0.48703308521618993; l2 norm of gradients: 0.011183341160803825; l2 norm of weights: 0.4201924178801403\n","Iteration #6002  Loss: 0.4870304094111598; l2 norm of gradients: 0.011172520242387686; l2 norm of weights: 0.42018888226816814\n","Iteration #6003  Loss: 0.48702773471013683; l2 norm of gradients: 0.011161711676666833; l2 norm of weights: 0.42018534863023455\n","Iteration #6004  Loss: 0.4870250611123419; l2 norm of gradients: 0.011150915450274867; l2 norm of weights: 0.4201818169640981\n","Iteration #6005  Loss: 0.48702238861699654; l2 norm of gradients: 0.011140131549859808; l2 norm of weights: 0.4201782872675209\n","Iteration #6006  Loss: 0.4870197172233236; l2 norm of gradients: 0.011129359962083878; l2 norm of weights: 0.4201747595382684\n","Iteration #6007  Loss: 0.48701704693054726; l2 norm of gradients: 0.011118600673623613; l2 norm of weights: 0.42017123377411003\n","Iteration #6008  Loss: 0.48701437773789263; l2 norm of gradients: 0.011107853671169888; l2 norm of weights: 0.42016770997281866\n","Iteration #6009  Loss: 0.4870117096445859; l2 norm of gradients: 0.01109711894142775; l2 norm of weights: 0.42016418813217093\n","Iteration #6010  Loss: 0.4870090426498548; l2 norm of gradients: 0.01108639647111657; l2 norm of weights: 0.4201606682499469\n","Iteration #6011  Loss: 0.4870063767529278; l2 norm of gradients: 0.011075686246969831; l2 norm of weights: 0.4201571503239304\n","Iteration #6012  Loss: 0.4870037119530349; l2 norm of gradients: 0.011064988255735339; l2 norm of weights: 0.4201536343519087\n","Iteration #6013  Loss: 0.4870010482494068; l2 norm of gradients: 0.011054302484175004; l2 norm of weights: 0.4201501203316728\n","Iteration #6014  Loss: 0.48699838564127584; l2 norm of gradients: 0.01104362891906498; l2 norm of weights: 0.42014660826101724\n","Iteration #6015  Loss: 0.4869957241278751; l2 norm of gradients: 0.01103296754719551; l2 norm of weights: 0.42014309813774003\n","Iteration #6016  Loss: 0.4869930637084387; l2 norm of gradients: 0.011022318355371034; l2 norm of weights: 0.4201395899596428\n","Iteration #6017  Loss: 0.48699040438220265; l2 norm of gradients: 0.011011681330410097; l2 norm of weights: 0.4201360837245309\n","Iteration #6018  Loss: 0.48698774614840323; l2 norm of gradients: 0.011001056459145326; l2 norm of weights: 0.4201325794302131\n","Iteration #6019  Loss: 0.48698508900627835; l2 norm of gradients: 0.010990443728423498; l2 norm of weights: 0.4201290770745016\n","Iteration #6020  Loss: 0.4869824329550668; l2 norm of gradients: 0.010979843125105391; l2 norm of weights: 0.42012557665521244\n","Iteration #6021  Loss: 0.4869797779940088; l2 norm of gradients: 0.010969254636065913; l2 norm of weights: 0.4201220781701649\n","Iteration #6022  Loss: 0.4869771241223452; l2 norm of gradients: 0.010958678248193967; l2 norm of weights: 0.4201185816171821\n","Iteration #6023  Loss: 0.4869744713393185; l2 norm of gradients: 0.010948113948392516; l2 norm of weights: 0.42011508699409017\n","Iteration #6024  Loss: 0.48697181964417213; l2 norm of gradients: 0.01093756172357852; l2 norm of weights: 0.4201115942987194\n","Iteration #6025  Loss: 0.48696916903615034; l2 norm of gradients: 0.010927021560682917; l2 norm of weights: 0.4201081035289032\n","Iteration #6026  Loss: 0.48696651951449904; l2 norm of gradients: 0.010916493446650645; l2 norm of weights: 0.4201046146824785\n","Iteration #6027  Loss: 0.48696387107846484; l2 norm of gradients: 0.01090597736844061; l2 norm of weights: 0.42010112775728586\n","Iteration #6028  Loss: 0.48696122372729556; l2 norm of gradients: 0.010895473313025608; l2 norm of weights: 0.42009764275116923\n","Iteration #6029  Loss: 0.4869585774602401; l2 norm of gradients: 0.010884981267392436; l2 norm of weights: 0.42009415966197616\n","Iteration #6030  Loss: 0.4869559322765487; l2 norm of gradients: 0.010874501218541775; l2 norm of weights: 0.4200906784875573\n","Iteration #6031  Loss: 0.48695328817547245; l2 norm of gradients: 0.010864033153488195; l2 norm of weights: 0.42008719922576737\n","Iteration #6032  Loss: 0.48695064515626374; l2 norm of gradients: 0.01085357705926015; l2 norm of weights: 0.42008372187446424\n","Iteration #6033  Loss: 0.4869480032181756; l2 norm of gradients: 0.010843132922899975; l2 norm of weights: 0.42008024643150904\n","Iteration #6034  Loss: 0.4869453623604629; l2 norm of gradients: 0.010832700731463812; l2 norm of weights: 0.42007677289476675\n","Iteration #6035  Loss: 0.48694272258238097; l2 norm of gradients: 0.010822280472021707; l2 norm of weights: 0.42007330126210546\n","Iteration #6036  Loss: 0.48694008388318655; l2 norm of gradients: 0.010811872131657465; l2 norm of weights: 0.420069831531397\n","Iteration #6037  Loss: 0.4869374462621373; l2 norm of gradients: 0.010801475697468711; l2 norm of weights: 0.4200663637005163\n","Iteration #6038  Loss: 0.48693480971849223; l2 norm of gradients: 0.010791091156566816; l2 norm of weights: 0.4200628977673421\n","Iteration #6039  Loss: 0.4869321742515111; l2 norm of gradients: 0.010780718496076983; l2 norm of weights: 0.4200594337297563\n","Iteration #6040  Loss: 0.48692953986045506; l2 norm of gradients: 0.010770357703138137; l2 norm of weights: 0.42005597158564423\n","Iteration #6041  Loss: 0.48692690654458604; l2 norm of gradients: 0.01076000876490293; l2 norm of weights: 0.4200525113328946\n","Iteration #6042  Loss: 0.48692427430316726; l2 norm of gradients: 0.010749671668537743; l2 norm of weights: 0.42004905296939965\n","Iteration #6043  Loss: 0.48692164313546304; l2 norm of gradients: 0.010739346401222675; l2 norm of weights: 0.42004559649305495\n","Iteration #6044  Loss: 0.48691901304073876; l2 norm of gradients: 0.010729032950151506; l2 norm of weights: 0.4200421419017596\n","Iteration #6045  Loss: 0.48691638401826065; l2 norm of gradients: 0.010718731302531672; l2 norm of weights: 0.42003868919341575\n","Iteration #6046  Loss: 0.4869137560672963; l2 norm of gradients: 0.010708441445584265; l2 norm of weights: 0.42003523836592926\n","Iteration #6047  Loss: 0.4869111291871142; l2 norm of gradients: 0.010698163366544052; l2 norm of weights: 0.42003178941720914\n","Iteration #6048  Loss: 0.486908503376984; l2 norm of gradients: 0.0106878970526594; l2 norm of weights: 0.42002834234516795\n","Iteration #6049  Loss: 0.48690587863617635; l2 norm of gradients: 0.010677642491192286; l2 norm of weights: 0.42002489714772145\n","Iteration #6050  Loss: 0.4869032549639628; l2 norm of gradients: 0.010667399669418282; l2 norm of weights: 0.42002145382278877\n","Iteration #6051  Loss: 0.4869006323596165; l2 norm of gradients: 0.010657168574626534; l2 norm of weights: 0.4200180123682925\n","Iteration #6052  Loss: 0.486898010822411; l2 norm of gradients: 0.01064694919411975; l2 norm of weights: 0.4200145727821584\n","Iteration #6053  Loss: 0.4868953903516214; l2 norm of gradients: 0.010636741515214229; l2 norm of weights: 0.4200111350623158\n","Iteration #6054  Loss: 0.4868927709465235; l2 norm of gradients: 0.010626545525239717; l2 norm of weights: 0.42000769920669717\n","Iteration #6055  Loss: 0.48689015260639446; l2 norm of gradients: 0.010616361211539516; l2 norm of weights: 0.42000426521323825\n","Iteration #6056  Loss: 0.4868875353305122; l2 norm of gradients: 0.010606188561470458; l2 norm of weights: 0.4200008330798783\n","Iteration #6057  Loss: 0.486884919118156; l2 norm of gradients: 0.010596027562402799; l2 norm of weights: 0.4199974028045598\n","Iteration #6058  Loss: 0.48688230396860593; l2 norm of gradients: 0.010585878201720323; l2 norm of weights: 0.4199939743852285\n","Iteration #6059  Loss: 0.4868796898811431; l2 norm of gradients: 0.010575740466820195; l2 norm of weights: 0.4199905478198334\n","Iteration #6060  Loss: 0.4868770768550499; l2 norm of gradients: 0.010565614345113085; l2 norm of weights: 0.41998712310632685\n","Iteration #6061  Loss: 0.4868744648896095; l2 norm of gradients: 0.010555499824023047; l2 norm of weights: 0.4199837002426646\n","Iteration #6062  Loss: 0.48687185398410626; l2 norm of gradients: 0.01054539689098754; l2 norm of weights: 0.4199802792268056\n","Iteration #6063  Loss: 0.48686924413782545; l2 norm of gradients: 0.010535305533457445; l2 norm of weights: 0.41997686005671186\n","Iteration #6064  Loss: 0.4868666353500536; l2 norm of gradients: 0.01052522573889693; l2 norm of weights: 0.4199734427303489\n","Iteration #6065  Loss: 0.48686402762007813; l2 norm of gradients: 0.010515157494783628; l2 norm of weights: 0.41997002724568555\n","Iteration #6066  Loss: 0.48686142094718726; l2 norm of gradients: 0.010505100788608467; l2 norm of weights: 0.41996661360069365\n","Iteration #6067  Loss: 0.48685881533067077; l2 norm of gradients: 0.01049505560787569; l2 norm of weights: 0.4199632017933485\n","Iteration #6068  Loss: 0.48685621076981894; l2 norm of gradients: 0.01048502194010286; l2 norm of weights: 0.4199597918216286\n","Iteration #6069  Loss: 0.48685360726392335; l2 norm of gradients: 0.010474999772820888; l2 norm of weights: 0.4199563836835156\n","Iteration #6070  Loss: 0.4868510048122764; l2 norm of gradients: 0.010464989093573823; l2 norm of weights: 0.41995297737699444\n","Iteration #6071  Loss: 0.4868484034141719; l2 norm of gradients: 0.010454989889919154; l2 norm of weights: 0.41994957290005325\n","Iteration #6072  Loss: 0.48684580306890424; l2 norm of gradients: 0.010445002149427507; l2 norm of weights: 0.4199461702506835\n","Iteration #6073  Loss: 0.486843203775769; l2 norm of gradients: 0.010435025859682775; l2 norm of weights: 0.41994276942687964\n","Iteration #6074  Loss: 0.4868406055340629; l2 norm of gradients: 0.010425061008282091; l2 norm of weights: 0.41993937042663965\n","Iteration #6075  Loss: 0.4868380083430834; l2 norm of gradients: 0.010415107582835747; l2 norm of weights: 0.4199359732479644\n","Iteration #6076  Loss: 0.48683541220212917; l2 norm of gradients: 0.01040516557096723; l2 norm of weights: 0.4199325778888581\n","Iteration #6077  Loss: 0.48683281711049997; l2 norm of gradients: 0.010395234960313261; l2 norm of weights: 0.41992918434732823\n","Iteration #6078  Loss: 0.48683022306749624; l2 norm of gradients: 0.01038531573852363; l2 norm of weights: 0.4199257926213853\n","Iteration #6079  Loss: 0.4868276300724196; l2 norm of gradients: 0.010375407893261308; l2 norm of weights: 0.41992240270904296\n","Iteration #6080  Loss: 0.48682503812457273; l2 norm of gradients: 0.010365511412202455; l2 norm of weights: 0.4199190146083183\n","Iteration #6081  Loss: 0.48682244722325946; l2 norm of gradients: 0.01035562628303622; l2 norm of weights: 0.41991562831723145\n","Iteration #6082  Loss: 0.48681985736778394; l2 norm of gradients: 0.010345752493464918; l2 norm of weights: 0.4199122438338053\n","Iteration #6083  Loss: 0.4868172685574523; l2 norm of gradients: 0.010335890031203956; l2 norm of weights: 0.41990886115606674\n","Iteration #6084  Loss: 0.4868146807915707; l2 norm of gradients: 0.01032603888398178; l2 norm of weights: 0.419905480282045\n","Iteration #6085  Loss: 0.486812094069447; l2 norm of gradients: 0.010316199039539906; l2 norm of weights: 0.41990210120977284\n","Iteration #6086  Loss: 0.48680950839038967; l2 norm of gradients: 0.010306370485632854; l2 norm of weights: 0.4198987239372862\n","Iteration #6087  Loss: 0.48680692375370826; l2 norm of gradients: 0.010296553210028186; l2 norm of weights: 0.4198953484626239\n","Iteration #6088  Loss: 0.4868043401587135; l2 norm of gradients: 0.010286747200506473; l2 norm of weights: 0.41989197478382806\n","Iteration #6089  Loss: 0.4868017576047167; l2 norm of gradients: 0.010276952444861272; l2 norm of weights: 0.4198886028989439\n","Iteration #6090  Loss: 0.48679917609103057; l2 norm of gradients: 0.010267168930899106; l2 norm of weights: 0.41988523280602\n","Iteration #6091  Loss: 0.4867965956169685; l2 norm of gradients: 0.01025739664643944; l2 norm of weights: 0.41988186450310744\n","Iteration #6092  Loss: 0.48679401618184487; l2 norm of gradients: 0.010247635579314735; l2 norm of weights: 0.4198784979882609\n","Iteration #6093  Loss: 0.48679143778497524; l2 norm of gradients: 0.010237885717370348; l2 norm of weights: 0.41987513325953807\n","Iteration #6094  Loss: 0.486788860425676; l2 norm of gradients: 0.010228147048464517; l2 norm of weights: 0.41987177031499956\n","Iteration #6095  Loss: 0.4867862841032644; l2 norm of gradients: 0.010218419560468445; l2 norm of weights: 0.4198684091527093\n","Iteration #6096  Loss: 0.48678370881705907; l2 norm of gradients: 0.0102087032412662; l2 norm of weights: 0.4198650497707341\n","Iteration #6097  Loss: 0.48678113456637906; l2 norm of gradients: 0.01019899807875468; l2 norm of weights: 0.41986169216714403\n","Iteration #6098  Loss: 0.48677856135054476; l2 norm of gradients: 0.010189304060843675; l2 norm of weights: 0.4198583363400121\n","Iteration #6099  Loss: 0.4867759891688773; l2 norm of gradients: 0.010179621175455806; l2 norm of weights: 0.41985498228741425\n","Iteration #6100  Loss: 0.48677341802069907; l2 norm of gradients: 0.010169949410526501; l2 norm of weights: 0.4198516300074299\n","Iteration #6101  Loss: 0.4867708479053331; l2 norm of gradients: 0.010160288754004045; l2 norm of weights: 0.4198482794981412\n","Iteration #6102  Loss: 0.4867682788221035; l2 norm of gradients: 0.010150639193849438; l2 norm of weights: 0.41984493075763324\n","Iteration #6103  Loss: 0.4867657107703355; l2 norm of gradients: 0.010141000718036523; l2 norm of weights: 0.4198415837839946\n","Iteration #6104  Loss: 0.4867631437493549; l2 norm of gradients: 0.01013137331455188; l2 norm of weights: 0.41983823857531627\n","Iteration #6105  Loss: 0.4867605777584888; l2 norm of gradients: 0.010121756971394847; l2 norm of weights: 0.41983489512969296\n","Iteration #6106  Loss: 0.4867580127970651; l2 norm of gradients: 0.010112151676577513; l2 norm of weights: 0.4198315534452219\n","Iteration #6107  Loss: 0.4867554488644125; l2 norm of gradients: 0.01010255741812464; l2 norm of weights: 0.41982821352000355\n","Iteration #6108  Loss: 0.48675288595986116; l2 norm of gradients: 0.010092974184073717; l2 norm of weights: 0.4198248753521413\n","Iteration #6109  Loss: 0.48675032408274155; l2 norm of gradients: 0.010083401962474986; l2 norm of weights: 0.41982153893974167\n","Iteration #6110  Loss: 0.4867477632323855; l2 norm of gradients: 0.010073840741391251; l2 norm of weights: 0.41981820428091393\n","Iteration #6111  Loss: 0.48674520340812555; l2 norm of gradients: 0.01006429050889803; l2 norm of weights: 0.4198148713737707\n","Iteration #6112  Loss: 0.48674264460929534; l2 norm of gradients: 0.01005475125308351; l2 norm of weights: 0.41981154021642725\n","Iteration #6113  Loss: 0.48674008683522946; l2 norm of gradients: 0.010045222962048492; l2 norm of weights: 0.41980821080700226\n","Iteration #6114  Loss: 0.486737530085263; l2 norm of gradients: 0.010035705623906368; l2 norm of weights: 0.4198048831436168\n","Iteration #6115  Loss: 0.4867349743587327; l2 norm of gradients: 0.010026199226783153; l2 norm of weights: 0.4198015572243954\n","Iteration #6116  Loss: 0.48673241965497593; l2 norm of gradients: 0.01001670375881746; l2 norm of weights: 0.41979823304746544\n","Iteration #6117  Loss: 0.4867298659733305; l2 norm of gradients: 0.01000721920816043; l2 norm of weights: 0.4197949106109572\n","Iteration #6118  Loss: 0.48672731331313585; l2 norm of gradients: 0.009997745562975793; l2 norm of weights: 0.4197915899130038\n"]}],"source":["learning_rate = 0.001\n","lambda_ = 1.0\n","\n","model = fit(xtrain_normal, ytrain, learning_rate, lambda_, 10000, verbose=1) #keep the verbose on here for your submissions"]},{"cell_type":"code","execution_count":239,"metadata":{"id":"-AiarpzOhIvE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678162941896,"user_tz":360,"elapsed":550,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"cecfacb0-a350-447c-e59a-558523a4f706"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy:  0.8411538461538461\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-237-c2c25048c40d>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  return np.sum((prob(x, w, b)>0.5).astype(np.float) == y_true)  / y_true.shape[0]\n"]}],"source":["print(\"Train accuracy: \", accuracy(xtrain_normal, ytrain, model))"]},{"cell_type":"code","execution_count":240,"metadata":{"id":"nrU6Tr7mhIvE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678163147644,"user_tz":360,"elapsed":205754,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"dfa9cded-3e86-43c9-d79e-49797284a9e7"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-237-c2c25048c40d>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  return np.sum((prob(x, w, b)>0.5).astype(np.float) == y_true)  / y_true.shape[0]\n"]},{"output_type":"stream","name":"stdout","text":["0.01 5 0.6763236763236763\n","0.01 2 0.8351648351648352\n","0.01 1 0.8791208791208791\n","0.01 0.1 0.913086913086913\n","0.01 0.01 0.9240759240759241\n","0.001 5 0.6163836163836164\n","0.001 2 0.8571428571428571\n","0.001 1 0.8741258741258742\n","0.001 0.1 0.8761238761238761\n","0.001 0.01 0.8141858141858141\n","0.0001 5 0.38461538461538464\n","0.0001 2 0.7122877122877123\n","0.0001 1 0.6813186813186813\n","0.0001 0.1 0.5994005994005994\n","0.0001 0.01 0.5794205794205795\n","1e-05 5 0.5024975024975025\n","1e-05 2 0.5374625374625375\n","1e-05 1 0.3436563436563437\n","1e-05 0.1 0.5054945054945055\n","1e-05 0.01 0.6813186813186813\n"]}],"source":["#grid search for finding the best hyperparams and model\n","\n","best_model = None\n","best_val = -1\n","for lr in [0.01, 0.001, 0.0001, 0.00001]:\n","    for la in [5, 2, 1, 0.1, 0.01]:\n","        model = fit(xtrain_normal, ytrain, lr, la, 10000, verbose=0)\n","        val_acc = accuracy(xval_normal, yval, model)\n","        print(lr, la, val_acc)\n","        if val_acc > best_val:\n","            best_val = val_acc\n","            best_model = model\n","    "]},{"cell_type":"code","execution_count":241,"metadata":{"id":"n5zNZCNVhIvE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678163147651,"user_tz":360,"elapsed":53,"user":{"displayName":"Jordan Fanapour","userId":"05750580216150453241"}},"outputId":"e47c30f8-8fb6-4b78-807f-b23c7659f9fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy:  0.94\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-237-c2c25048c40d>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  return np.sum((prob(x, w, b)>0.5).astype(np.float) == y_true)  / y_true.shape[0]\n"]}],"source":["print(\"Test accuracy: \", accuracy(xtest_normal, ytest, best_model))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[{"file_id":"19De2-UAquIv1nljW5rvURtX4RLWzVW6U","timestamp":1678086663663}]}},"nbformat":4,"nbformat_minor":0}